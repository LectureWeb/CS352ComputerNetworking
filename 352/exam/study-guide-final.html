<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title> CS 352 Final Exam Study Guide </title>

<link href="../../css/layout.css" rel="stylesheet" type="text/css" />
<link href="../../css/main.css" rel="stylesheet" type="text/css" />
<link href="../../css/print.css" rel="stylesheet" type="text/css" media="print" />
<link href="../../css/main-print.css" rel="stylesheet" type="text/css" media="print" />
<style type="text/css">
.rqbox {
	text-align: center;
	margin-left: auto;
	margin-right: auto;
        position: relative;
	width: 15em;
        background-color: #FDF5B6;
        border-style: double; border-width: 3px;
        padding: 0.5em 0.5em 0.5em 0.5em;
}
.nofinal {
        text-decoration:line-through;
}

</style>
</head>

<body id="s_ru352">
<div id="wrapper">
<!-- _______________________________________ BANNER _______________________________________ -->
<div id="banner">
  <div id="logo">
  <img src="../../css/images/pk-org-pencil.png" alt="pk.org" name="logo" width="122" height="45"/>
  </div>
  <div id="title"> Internet Technology </div>
  <div id="search">
  <form method="get" action="http://www.google.com/search">
	<div style="border:none ;padding:2px;width:25em;">
	<input type="text" name="q" size="25" maxlength="255" value="" />
	<input type="submit" value="Search" />
	<input type="hidden"  name="sitesearch" value="www.pk.org" checked />
	</div>
  </form>
  </div>
  <ul>
    <li class="separator"><a href="../../about/index.html">About</a></li>
    <li class="separator"><a href="../../about/contact.html">Contact</a></li>
    <li><a href="../../sitemap.html">Site map</a></li>
  </ul>
</div>

<!-- _______________________________________ MAIN NAV _______________________________________ -->
<div id="navbar">
	<ul>
	<li class="homelink"><a href="../../index.html">Home</a></li>
<!--
	<li class="aboutlink"><a href="../../about/index.html">About</a></li>
-->
	<li class="ru"><a href="../../rutgers/index.html">Rutgers</a></li>
	<li class="ru352"><a href="../../352/index.html">Internet Technology [352]</a></li>
	<li class="ru416"><a href="../../416/index.html">Operating Systems [416]</a></li>
	<li class="ru417"><a href="../../417/index.html">Distributed Systems [417]</a></li>
	<li class="cslink"><a href="../../cs/index.html">Computing</a></li>
	<li class="photolink"><a href="../../photo/index.html">Photography</a></li>
<!--
	<li class="funlink"><a href="#">Coming</a></li>
	<li class="funlink"><a href="#">Soon</a></li>
-->
	</ul>
</div>

<div id="subnav">
<P>
You are in: 
</p>
<ul>
	<li class="first"> <a href="index.html"> Home </a>  </li>
 	<li> <a href="../../index.html"> Rutgers </a>  </li>
 	<li> <a href="../index.html"> CS 352 </a>  </li>
 	<li> <a href="../exam/index.html"> Exam info </a>  </li>
 	<li> <a href="../exam/study-guide-final.html"> Final Exam study guide </a>  </li>
</ul>
</div>
<div id="content-wrapper">
<div id="main">
<div id="headline">
<h1> Final exam study guide </h1>
<h2> The three-hour study guide for the final exam </h2>
<p class="author"> Paul Krzyzanowski </p>
<p class="date"> Latest update: Fri May  6 15:41:35 EDT 2016
 </p>
</div>

<p class="first">

Disclaimer: 
This study guide attempts to touch upon the most
important topics that may be covered on the exam but does not claim to
necessarily cover everything that one needs to know for the exam. Finally,
don't take the <i>three hour</i> time window in the title literally.</p>

<h1 id="terminologyfromthehistoryofdatacommunication">Terminology from the history of data communication</h1>

<p>Data communication, the process of conveying information from one place to another, existed
long before computers and dates back to earliest recorded history. As various pre-computer
techniques for conveying data reliably over distances were invented, so too were a number
of underlying mechanisms that proved to be useful in digital networks.</p>

<p>The most basic form of message delivery is moving a message from one entity (person) to another.
This is known as <strong>point-to-point</strong>, or <strong>unicast</strong> delivery. Transmitting
a single message so that everyone gets it is known as a <strong>broadcast</strong>. It&#8217;s the
difference between a messenger and a smoke signal.
Two crucial categories of data are <strong>control</strong> and <strong>message</strong> data.
Message data is the information that needs to be delivered.
Control data comprises the information that is used to manage the delivery of the message. It
includes things such as acknowledgements, retransmission requests, and rate controls.
In some cases, control data is sent via a separate communication channel than message data. In
other cases, it is intermixed with some syntax for identifying which data is control and which is message.</p>

<p><strong>Synchronization</strong>, in the context of messaging, is the coordination of activity between
the sender and receiver of messages. It includes controls such as <em>ready to send</em>, <em>ready
to receive</em>, <em>transmission complete</em>.</p>

<p><strong>Congestion</strong> is the inability of a network element
to receive or transmit messages at the desired rate, leading to a
buildup or possibly a loss of messages and a deterioration in the
quality of service.
<strong>Flow control</strong>
is the aspect of synchronization that deals with the rate of transmission, controlling
when the sender transmits messages to keep the receiver or network from getting overwhelmed with
data traffic.
Rate control is the aspect of flow control that controls just the speed of
transmission.
Other elements of flow control may be directives to delay for a specific time or wait until
some feedback is received. By reducing the flow of traffic, congestion can be alleviated.</p>

<p>An <strong>acknowledgement</strong> (also known as a <strong>positive acknowledgement</strong>)
is a control message that states that a message was received.
A <strong>negative acknowledgement</strong> is a control message that states that a message was not delivered
to the destination or that it was received with errors. It is a form of <strong>error notification</strong>.</p>

<p>Packets may be lost or corrupted during transmission.
With <strong>best-effort</strong> message delivery the network does the best it can
do in providing service but
makes no guarantee that data will be delivered to its destination
or the time it will take for it to be delivered.
<strong>Reliable delivery</strong> ensures that data does arrive reliably. Reliable
delivery may be implemented as a layer of software on top of
a network that provides best-effort delivery. With reliable
delivery, if a message does not arrive or arrive correctly,
the transmitter will be notified and will attempt to resend the message.
Delivery time of the message can vary (since there is an extra delay to detect
lost data dnt to retranmit it). With flow control mechanisms to relieve
congestion as well as reliable delivery, there is generally no guarantee on
the resulting bit rate between two communicating parties.</p>

<p>A key part of any communication is the <strong>encoding</strong> of data. This ranges from how data
is represented in the medium (the component that carries data, whether it is radio frequency, a wire,
or a piece of paper) to the meaning of the messages themselves. To expedite messaging in the past,
table-based encoding was sometimes used to have a letter or a number represent an entire message
instead of transmitting each character in the message. </p>

<p>A <strong>repeater</strong>, also known as an <strong>extender</strong>,
is a device that regenerates a
signal to cover longer distances. Pre-networking, this would be a relay station where one would switch horses
or runners. These days, it is a often a device such as an ethernet extender that
regenerates incoming signals to full strength, allowing longer runs of ethernet cable.</p>

<h1 id="originsoftheinternet">Origins of the Internet</h1>

<p>The precursor to the Internet was ARPANET, a research project funded by DARPA, the Defense
Advanced Research Projects Agency. This was an experiment in using packet switched networking
to share computing resources over long distances. ARPANET got its start in late 1968 and was
inspired by three developments. First, Leonard Kleinrock&#8217;s concept of <strong>packet switching</strong>:
breaking messages into small chunks called <strong>packets</strong> and have them compete
with packets from other messages on the same shared communication link. Second, J.C.R. Licklider&#8217;s
vision of an &#8220;Intergalactic Network&#8221;, a globally connected set of computers where you can
access data from or run programs on remote machines. Third, the demonstration of the viability
of a wide-area network accomplished by connecting a computer in Massachusetts to one in California
over a telephone line via a dial-up connection.</p>

<p>The crucial component in the early ARPANET was the <strong>Interface
Message Processor</strong>, or <strong>IMP</strong>. This was the device that served
as the interface between one or more connected host computers and
the external packet-based ARPANET, processed the flow of packets,
and sent them to other IMPs or to a connected host . It was the
predecessor to the <strong>router</strong>. The ARPANET went live in 1969 with
two computers and two more by the end of the year. In the early
ARPANET, all the protocols for delivering packets were implemented
in the IMPs. The software that ran on the computer and interfaced
with the IMP was the <strong>Network Control Program</strong>, or <strong>NCP</strong>. This
allowed applications to interface with the network and provided
them with the abstraction of having a dedicated communication stream.
It also handled flow control to the IMP and retransmission. As the
ARPANET evolved, NCP became TCP, which handled reliable
application-to-application communication.</p>

<h2 id="coreprinciples">Core principles</h2>

<ol>
<li><p>ARPANET was designed to support the <strong>interconnection</strong> of networks (<strong>inter-networking</strong>).
It is a <strong>network of networks</strong> rather than a single network to which computers connect.
For an organization to join the ARPANET, there was no requirement for it to have a specific internal network.
This layer of networking would be a logical layer on top of any underlying physical network.</p></li>
<li><p>Because there was no assumption on the design of any physical network, the ARPANET assumed that
communication is not guaranteed to be reliable. Instead, it would provide <strong>best effort</strong> packet delivery.
Software, originally NCP and later TCP, provided the ability to detect errors or lost packets and request retransmission.</p></li>
<li><p><strong>Routers</strong> connect the various individual networks and links together that make up the Internet.
While routers are crucial in determining where to send packets, they were designed to not store
information about the flow of packets. Any packet can be routed on its own with no <i>a priori</i> connection setup.</p></li>
<li><p>Finally, the entire network is decentralized. There is no central administration or control point. This
aspect not only makes the network scalable (no single point of congestion) but aids in making it a fault
tolerant network. If a router is not working, it is likely that there may be alternate paths to the destination.</p></li>
</ol>

<p>The ARPANET clearly demonstrated the value of wide-area, hardware agnostic networking but access to it
was restricted to organizations working on U.S. Department of Defense projects. Other networks were
created to cater to non-defense communities and some of these, such as NSFNET and CSNET, also chose to use the
IP platform.</p>

<p>NSFNET was a government funded (by the NSF, the National Science Foundation) network that
was created in 1985 to connect NSF-funded supercomputing centers.
It initially did permit commercial traffic on its networks.</p>

<p>By the late 1980s, the NSF was looking for commercial partners who
would provide wide area networking services. A collection of these
partners removed any need for government funding. In 1990, the
ARPANET was decommisssioned and in 1995, the NFSNET backbone was
transitioned to commercial networks, leading to the Internet of
today.</p>

<h1 id="lanandinternetstructure">LAN and Internet structure</h1>

<p>The Internet comprises the network edge and the network core. The
network <strong>core</strong> is the set of interconnected networks that provide
wide area connectivity to the customers of the network. The network
<strong>edge</strong> is the set of devices and local networks that connect to
the core network. Your computers, TV sets, thermostats, and local
network constitute a network edge. Your Internet service provider
and its Internet service provider are components of the network
core.</p>

<h2 id="localareanetworks">Local area networks</h2>

<p>A <strong>local area network</strong>, or <strong>LAN</strong>, is a data communication network
that covers a relatively small area, typically a building. It uses
a the same network access protocol and usually the
same transmission medium (e.g., Ethernet), allowing
message delivery without the need to route messages through
different networks. Devices that send
and receive messages on the network are called <strong>hosts</strong> or <strong>nodes</strong>.
These devices are <strong>peers</strong>, meaning that no device has more control
or coordination of the network than any other device. Any host can
initiate a data transfer with any other host on the LAN. LANs usually
exhibit very low latency and a high data rate: typically 10s to a gigabit per
second (Gbps) for wireless networks and a 1 gigabit per
second (Gbps) or more for wired connections (although speeds as high as 10
and 100 Gbps are available).</p>

<p>Nodes connect to a local area
network with an <strong>adapter</strong>. These are usually integrated onto the
main circuit board but may be separate components, such as a USB
ethernet adapter. Another term for these is <strong>NIC</strong>,
which stands for <strong>Network Interface Controller</strong>.</p>

<p>The physical data communication links that the adapter uses to send and
receive data are called <strong>media</strong>. Common examples are <strong>unshielded
twisted pair</strong> (UTP) copper wire (e.g., ethernet cable), radio
frequency (e.g., the 5 GHz frequency bands used by
802.11ac), coaxial cable (e.g., used by cable TV in the home and the
MoCA standard, multimedia over coax), and optical fiber (which is not commonly
used in the home).</p>

<p>The other end of the media terminates at a <strong>switch</strong> or <strong>hub</strong>.
A <strong>hub</strong> is a device that acts as a central point for multiple LAN
cables. It takes any data that comes in one port and sends it to
all the other ports. A <strong>switch</strong> is similar to a hub but smarter.
It looks at incoming data and determines the port or ports on which
to transmit it. Switches have largely replaced hubs. They provide
<strong>scalable bandwidth</strong> in that they do not introduce more network
congestion as you add more hosts onto your LAN. Switches and hubs
are <strong>link-layer</strong> devices (more on that later). That is, they move ethernet packets
to their destination as opposed to relaying data between networks.
They are responsible for creating the physical network. For wireless
networks, a wireless access point serves as link-layer switch.</p>

<h2 id="accessnetwork">Access Network</h2>

<p>The connection between the LAN and the Internet (via the Internet
Service Provider) is called the <strong>access network</strong>. A <strong>residential
gateway</strong> (a type of router) or <strong>access router</strong> connects a home
or office LAN to the Internet. A <strong>modem</strong>, which stands for
<strong>modulator/demodulator</strong> converts data between various analog
formats as needed by the underlying media. Think of a modem as
back-to-back NICs, each converting data to their type of media.
Modems are generally built into access routers. Examples of access
links are:</p>
<dl>
<dt>DSL (digital subscriber line)</dt>
<dd><strong>DSL</strong> uses existing copper telephone wiring between your home
and the phone company&#8217;s central office. Since voice uses only the
0 - 4 kHz range of frequencies, there is a lot of untapped bandwidth
in a phone wire. DSL uses the 4 kHz through 50 kHz frequency band
for upstream data (data you send) and the 50 kHz through 1 MHz band
for downstream data (data you receive). A DSL modem serves as an
access router and modem. At the phone company&#8217;s central office, the
access link terminates at a DSLAM (digital subscriber line multiplexor).
From there, the data signals are sent to the data network (Internet)
and the voice signals are sent to the phone network.</dd>

<dt>Cable</dt>
<dd>Internet service provided by a TV cable company uses the same
coax cable that provides TV signals. With cable TV, hundreds of
channels are transmitted at once, each occupying a different frequency
band (high definition channels occupy a 6 MHz band and standard
definition digital channels typically occupy a 1.5 GHz band). This
type of transmission is called <strong>broadband</strong>. A certain number of
channels are not used for TV services but instead are used for
Internet access. The customer has an access router/modem that
conforms to the <strong>DOCSIS</strong> (Data Over Cable Service Interface
Specification) standard. Some number of channels are devoted to
downstream communication (data you receive). Each channel provides
38 Mbps service. Another set of channels are devoted to upstream
service, with each channel providing 27 Mbps service. The cable
terminates at the cable company&#8217;s headend on a device called the
CMTS, the cable modem termination system. Here, the Internet service
frequencies are filtered to separate out the data and send it to
the cable company&#8217;s Internet link. A key distinction between DSL
and cable service is that the phone wire between the DSL modem and
the phone company&#8217;s central office is a dedicated, non-shared line.
The coax cable of cable TV service is shared among a neighborhood.
However, even though the channel is shared, the capacity of coax
media is greater than that of a phone line and its signal attenuation
over distance is far lower.</dd>

<dt>FTTH (Fiber to the Home), FTTN (fiber to the Neighborhood)</dt>
<dd>Fiber offers even greater bandwidth than cable and can propagate
signals longer distances without degradation. Fiber to the Home
(FTTH) is an optical fiber link that connects a home to a central
office and delivers a combination of TV, telephone, and Internet
service. Verizon&#8217;s FiOS service is an example of a service that
delivers fiber to the home. Access links in this architecture often
use optical splitters to allow a single fiber from the central
office to fan out to several homes (typically 16&#8211;128). FTTH requires
an end-to-end fiber infrastructure, which is costly to deploy
(Verizon spent $23 billion through 2010 deploying FiOS at a cost
of over $800 per home). An alternative architecture that is designed
to be more cost effective is Fiber to the Neighborhood (FTTN).
AT&amp;T&#8217;s U-verse service is an example of this. A fiber runs from the
provider&#8217;s central office to the neighborhood (within a half mile
of the customers&#8217; homes). There, it goes to a mini headend that
uses copper wiring to connect to each customer&#8217;s home via VDSL (very
high bitrate DSL).</dd>
</dl>


<h2 id="isps">ISPs</h2>

<p>The organization that provides Internet service to a customer is called an
<strong>Internet Service Provider</strong> (<strong>ISP</strong>). One ISP does not have
access links to every Internet user in the world so there are thousands of ISPs: approximately 12,700
worldwide; 7,000 in the U.S. (about 100 or so larger-sized ones and lots of tiny regional ones).
Most smaller ISPs that serve end users purchase Internet connectivity from other ISPs.
ISP networsk are categorized by <strong>tiers</strong>. There isn&#8217;t a formal definition
of what constitutes a tier but there are generally accepted conventions.</p>

<p>At the very top of the hierarchy, <strong>Tier 1</strong> ISPs own the infrastructure that forms
the backbone of the Internet. Each Tier 1 ISP has a <strong>peering</strong> agreement with every other
Tier 1 ISP. This means that they agree to forward and receive traffic from each other without charging
the other ISP for it. Tier 1 ISPs have access to the <strong>Internet routing table</strong>,
also known as the <strong>global routing table</strong>. What this means that they know the
top-level ISP to which any IP address should be sent. They also know which of their lower-tier ISPs
receive any given IP address. As such, there is no concept of a &#8220;default route&#8221; at this level. With
lower-tier ISPs, a router can give up if it does not know where a certain packet should go and
just send it to a higher-level ISP. Tier 1 ISPs do not pay for data transit.
 Examples of Tier 1 ISPs include AT&amp;T, Verizon, CenturyLink, Level 3, Telef√≥nica, and NTT. </p>

<p><strong>Tier 2</strong> ISPs purchase data transit from Tier 1 ISPs and other Tier 2 ISPs but may also peer with other networks for
direct connectivity and cost saving. They may then resell Internet access.
Examples of Tier 2 ISPs include
Comcast, British Telecom, Vodafone, and Sprint Communications. </p>

<p>Tier 3 ISPs occupy the lowest level and solely purchase Internet
transit from one or more Tier 1 and Tier 2 IPSs. A Tier 3 ISP will
typically provide coverage in a limited region of a country. Examples
of these are Powernet Global, and Excel.Net. They concentrate on
the retail and consumer markets.</p>

<p>A packet will often pass through several networks en route to its destination, both within and between
ISPs. Each link terminates at a <strong>router</strong> which makes a decision on where the packet
should be delivered. Each transmission of a packet is called a <strong>hop</strong>.</p>

<p>Within ISPs, <strong>edge routers</strong> are placed at the edge of an ISP&#8217;s network and
communicate with other networks. For larger organizations, an edge router may sit at the
edge of a customer&#8217;s network and connect to one or more ISPs.
A <strong>core router</strong> is a router that connects routes on the Internet backbone.
A core router may also be used in dispersed organizations to interconnect routers from
multiple locations.</p>

<h1 id="networkingoverview">Networking overview</h1>

<p>A network is inherently a shared resource. Lots of devices send and receive data on it.
How do we share the network without clobbering each other&#8217;s data?</p>

<p>The most basic approach is to establish a dedicated connection from the source to the destination.
This is a <strong>physical circuit</strong> and is what was used in the early days of the phone
system: your phone line went to the central office where it connected to a patch cord that was
in turn connected to the wire of the person with whom you were speaking. The same stream of electrons
flowed from one end to the other. This is not a viable use of network resources. Moreover, we will
likely have multiple applications running on a machine and using the network. We need to find
a way to share network links.</p>

<p>One way can share the medium is to have each party communicate on
different frequencies. This ability to transmit different data
simultaneously is called <strong>broadband</strong> communication.
Each transmission is assigned a frequency band: a portion of the
total bandwidth. Broadband communications uses
<strong>Frequency Division Multiplexing</strong> (<strong>FDM</strong>).
Note that bidirectional communication
on one channel is not possible: each sender-receiver set needs its
own frequency band. Cable TV is an example of broadband. </p>

<p>An alternate method is to have everyone take turns in accessing the
medium. This is called <strong>baseband</strong> communication. Each device is allowed
full access to the medium&#8217;s bandwidth but only for a portion of time.
Each communication session is assigned a specific set of short, fixed-length time slots during which it
can transmit. This is called <strong>Time Division Multiplexing</strong> (<strong>TDM</strong>).
Time Division Multiplexing is an example of <strong>circuit switching</strong>.</p>

<p>Both FDM and TDM are examples of <strong>circuit switching</strong>.
Circuit switching sets up a dedicated communication channel, similar to
a physical circuit.
The key difference from a physical circuit is that the effective bandwidth is lower than
the capacity of the medium since it is shared.
With Frequency Division Multiplexing, the available bandwidth is sliced into frequency ranges.
With Time Division Multiplexing, the available bandwidth is sliced into time slots.</p>

<p>An althernate way of sharing a medium is to use
variable-size time slots with no scheduling on the transmitter&#8217;s
part. This is called <strong>packet switching</strong>.</p>

<h2 id="circuitswitching">Circuit switching</h2>

<p>Circuit switching requires a <strong>connection setup</strong> (or <strong>circuit setup</strong>).
A control message is sent from the source to establish
a path (route) from the source to the destination. Every
switching element in the path agrees to the setup of the path and allocates the
appropriate time slots (and other resources, such as memory buffers). The originating
node is then informed that the connection is established and communication
can take place. The path and all the switching resources (e.g., time slices or frequency bands)
remain allocated to the communication session whether data is being sent or not.
All data travels along this predetermined path from the source to the destination.
When the communication is complete, the sender &#8220;releases&#8221; the circuit. This results
in the sending of another control message through the path informing routers to
de-allocate the resources they were using for the session.</p>

<p>Benefits of circuit switching are that it offers constant latency and guaranteed bandwidth.
Another benefit is that data routing decisions do not have to be made for each message
that is transmitted. They
are made once at the start of the communication session. Data can flow through a router
without having to be stored first until a decision is made where and when to transmit it.
The downside of circuit switching is that each connection ties up bandwidth and switching resources whether
any data is being transmitted or not. Conversely, if a connection needs to transfer a
larger amount of data, it still needs to spread it over bandwidth given to it even
if the rest of the network is not being used at the moment.
In short, circuit switching does not use network resources efficiently. Each circuit is
allocated a fixed bandwidth whether it is used or not.</p>

<h2 id="packetswitching">Packet switching</h2>

<p>With packet switching, a communication stream is broken into chunks of data called <strong>packets</strong>.
Each packet must contain a destination address in its message header.
The packets travel from the source node to their final destination via <strong>packet switches</strong>.
Routers and ethernet switches are examples of packet switches.
Routers are used to transmit packets between different
networks and switches are used to transmit packets within a local area network. Each packet switch
decides on the disposition of the packet (to what port it should be transmitted) based on the packet&#8217;s destination
address. There is no need to retain memory of a predefined route for a stream of packets that represents
a communication stream. In fact, there is no real concept of a communication stream because no
routes have to be set up and no resources need to be reserved ahead of time.
Because packet switching is designed for baseband networks, each packet has full use of the
network link. If there are no other packets transmitted on the network, a node may see its available
bandwidth approach the maximum capacity of the network link.</p>

<p>Packet switched traffic is known as <strong>datagram service</strong> in contrast to circuit switched
virtual circuit service. Think of a datagram as a telegram or letter, where each message has to be
addressed individually and may take a different path through the network. Think of a virtual circuit
as a telephone call where the call is first set up but then gets an established route constant bandwidth
for its duration.</p>

<p>Packet switching employs <strong>statistical multiplexing</strong>. Multiplexing means dividing
a communication channel among multiple data streams. With TDM&#8217;s circuit switching,
a communication channel was divided into fixed time slots per data stream. With packet switching,
we are still sharing the network but now using variable time slots. What this means is that
if a node has a lot of data to transmit and others do not then it can transmit large packets
(or a lot of smaller packets) and use more of the network. If a node has little to transmit, it
will use less of the network and more network capacity will be available for others.
Of course, there might be times when a node may have to wait longer for the network to be free.
If a lot of nodes have a lot of data to transmit, they will collectively have to wait longer to use
the network, some more than others.
Similarly, routers may end up queuing packets for a particular outbound port. This leads to
variable latency. Packet switching is characterized by variable bandwidth and variable latency.
With packet switching, an entire packet needs to be received by a router before it is
transmitted on an outgoing link. This is called <strong>store and forward</strong> delivery
and also contributes to network latency as we will see in our discusion on delay and throughput.</p>

<p>Despite its variable bandwidth and variable latency, packet switching allows for far more
efficient use of the network than circuit switching and has no limit on the number of concurrent
communication sessions. Because, on average, applications do not use the network non-stop,
switching and link resources are wasted whenever data is not flowing on an established connection.
With packet switching, there is no such reservation of these resources and more streams can be accommodated
while providing the same bandwidth to applications.
Packet switching is the dominant means of data communication. The Internet is built around
packet switching.</p>

<h1 id="units">Units</h1>

<p>Throughout our discussions on networking, we will bring up units of measure.
The three crucial ones for us are the size of data, the speed at which it moves,
and the time that it takes for it to get somewhere.</p>
<dl>
<dt>Size</dt>
<dd>The fundamental unit of size is a <strong>bit</strong> (b).
Eight bits make a <strong>byte</strong> (B).
Packet sizes are generally measured in bytes (watch out for the factor of eight when working
with bits per second!).
Networks tend to use base&#8211;10 units, so a <strong>kilobyte</strong> (KB)
is 1,000 bytes rather than the 1,024 bytes we are used to in programming.
A <strong>kilobit</strong> (Kb) is 1,000 bits.
A <strong>megabit</strong> (Mb) is 10<sup>6</sup> bits and a <strong>gigabit</strong> (Gb) is 10<sup>9</sup> bits.
A <strong>megabyte</strong> (1 MB) is 1,000 KB or 10<sup>6</sup> bytes or 8&times;10<sup>6</sup> bits.</dd>

<dt>Time</dt>
<dd>Time is measured in <strong>seconds</strong> (s).
One second (1 s) = 1,000 ms (<strong>milliseconds</strong>) = 10<sup>6</sup> &mu;s (<strong>microseconds</strong>) =
10<sup>9</sup> ns (<strong>nanoseconds</strong>).</dd>

<dt>Rate</dt>
<dd>Rate is measured in <strong>bits per second</strong> (b/s or bps).
Moving a megabit (1 Mb) of data over a 10 Mbps (megabit per second) network will take
(1&times;10<sup>6</sup> b &divide; 1&times;10<sup>6</sup>; bps) = 0.1 s, or 100 ms.
Transmitting 1 kilobit on a 1 Gbps link will take (kilo &divide; giga)
= (10<sup>3</sup> &divide; 10<sup>9</sup>) = 10<sup>&#8211;6</sup> = 1 &mu;s, or one millionth of a second.</dd>
</dl>


<h1 id="delayandthroughputinnetworks">Delay and throughput in networks</h1>

<p>As a packet flows from its source to its ultimate destination, it goes through
multiple routers. Each router introduces a delay as does the transit of the packet over
the communication link.</p>

<p>With packet switching, a packet must be fully received by a router before it can be
sent out. This is <strong>store and forward</strong> packet delivery.
To see how this contributes to overall delay, let us consider each link.
If data is transmitted at <em>R</em> bits per second and a packet is <em>L</em> bits
long, it takes <em>L/R</em> seconds to transmit a packet from one link to the next.
Since transmission on the next link will not start until the packet is received,
each link adds a delay of <em>L/R</em> seconds. With <em>N</em> links (there are <em>N&#8211;1</em> routers or transmitters
but we also count the delay of the initial transmission),
we have a total delay of <em>N(L/R)</em> seconds.</p>

<p>Network delay is due to four factors:</p>

<ol>
<li><p><strong>Processing delay</strong>.
The processing delay is the computation that a router has to do to
examine the header, check for packet errors, figure out the outbound port (route), and
move data around. It is usually not a significant contributor to the overall delay
and consumes a few microseconds.</p></li>
<li><p><strong>Transmission delay</strong>.
The transmission delay is the time that it takes to get a complete packet out onto the network.
This is a function of the speed of the link (e.g., 1 Gbps) and the number of bits in the packet:
<em>(packet size &divide; transmission speed)</em>. If the packet size is <em>L</em> and the
transmission rate is <em>R</em>, the transmission delay is <strong><em>L/R</em></strong>.</p></li>
<li><p><strong>Propagation delay</strong>.
The propagation delay is the time it actually takes the signal to move from one end of the medium to
the other. While we might transmit the bits onto the network at, say, 100 megabits per second, there
is a delay between the time that the signal is sent and the signal is received. This is the speed
of signal propagation in the medium. For electrical signals in unshielded twisted pair or
for light pulses in fiber optics, this value is approximately 2&times;10<sup>8</sup> m/s (about
67% of the speed of light in a vacuum). An
electrical signal propagates in air on a wireless network at approximately 3&times;10<sup>8</sup>
m/s. Depending on the distance the packet needs to travel, the delay may be from a few nanoseconds
to a few tens of milliseconds. It might be considerably longer for satellite transmission due to the longer
distance covered.</p></li>
<li><p><strong>Queuing delay</strong>.
With packet based networks, we can only transmit one packet onto a link at a time. Any other packets
that need to go out on that link will need to wait in a queue. The queuing delay is a function
of the amount of bits that are ahead of the packet (number of packets &times; the size of each packet)
and the transmission rate of the outbound link. Queuing delay can vary a lot depending on how much
data traffic is flowing over any particular link. It is dependent on how much traffic arrives at
a router at approximately the same time that needs to go out on the same link and on how quickly the
router can transmit the data out (see <em>transmission delay</em>).</p></li>
</ol>

<p>One useful measure for estimating the likelihood of queuing delays is
<strong>traffic intensity</strong>.
Traffic intensity is the average packet transmission delay
(<em>L/R</em>; see <em>transmission delay</em>) multiplied by the average rate of packet arrival.
If the average rate of packet arrival is <em>a</em>, <strong>traffic intensity</strong> is <strong><em>La/R</em></strong>.
It is technically a unitless quantity since packets/second &times; bits/packet &divide; bits/second cancel out.
[Trivia - not on the exam: the unit of this measure is called an <em>erlang</em> and refers to the load
on a network.]</p>

<p>If the traffic intensity is greater than one, that means that, on average, packets arrive faster than
they can be transmitted and the queue will keep growing without bound.
This assures us that the queue will eventually overflow
and packets will have to be <strong>dropped</strong>, leading to <strong>packet loss</strong>.</p>

<p>If the traffic intensity is less than or equal to one, packets are arriving slower or at the
same speed that they are being transmitted. This does not mean that packets will never get queued up.
A number of packets may occasionally arrive in rapid succession &#8212; a <strong>burst</strong> &#8212; and will have to be queued.
As traffic intensity approaches one, the probability that there will be bursts of packets that need to
be enqueued increases drastically. Hence, as traffic intensity approaches 1, queuing delay starts
to increase dramatically. In some cases, this will lead to lost packets due to limited queue sizes (routers
have only a fixed amount of memory to devote to queues).</p>

<p>The total delay for a node is the sum of the four delays we just mentioned: processing + queue +
transmission + propagation. The total delay for <em>N</em> links in a store-and-forward
network is simply <em>N</em> times that amount.</p>

<h1 id="protocols">Protocols</h1>

<p>Data networking is generally implemented as a stack of
several <strong>protocols</strong> &ndash; each responsible for a specific aspect of
networking. The <strong>OSI reference model</strong> defines seven <strong>layers</strong>
 of network protocols.</p>
<dl>
<dt>1. Physical</dt>
<dd>Deals with hardware, connectors, voltage levels, frequencies, etc.
This layer does not care about contents but defines what constitutes a 1 or a 0.
Examples of this layer are USB and Bluetooth.</dd>

<dt>2. Data link</dt>
<dd>Sends and receives packets
on the physical network. It may detect and possibly correct errors but only
on this link (for example, queue overflow at a router may still cause packet loss).
Ethernet packet transmission is an example of this layer.</dd>

<dt>3. Network</dt>
<dd>Relays and routes data to its destination. This is where networking gets interesting
because we are no longer confined to a single physical network but can route traffic
between networks. IP, the Internet Protocol, is an example of this layer.</dd>

<dt>4. Transport</dt>
<dd>Provides a software endpoint for networking. Now we can communicate application-to-application
instead of machine-to-machine. Each application can create one or more distinct streams
of data. TCP/IP and UDP/IP are examples of this layer.</dd>

<dt>5. Session</dt>
<dd>Manages multiple logical connections over a single communication link. Examples
are SSL (Secure Sockets Layer) tunnels and remote procedure call connection management.</dd>

<dt>6. Presentation</dt>
<dd>Converts data between machine-specific data representations.
Examples are data representation
formats such as MIME (for media encoding on email),
XML, XDR (for ONC remote procedure calls), NDR (for Microsoft COM+ remote procedure calls),
and ASN.1 (used for encoding cryptographic keys and digital certificates).</dd>

<dt>7. Application</dt>
<dd>This is a catch-all layer that includes every application-specific communication
protocol. For example, SMTP (sending email), IMAP (receiving email), FTP (file transfer),
HTTP (getting web pages).</dd>
</dl>


<p>The OSI reference model gives us a terminology to discuss and compare different
networks. Any specific network may not necessarily implement all these layers.
The Internet protocol stack relies on layers 1 through 4 (physical through transport)
but it is up to applications to implement and use session, presentation, and,
of course, application layers.</p>

<p>A key aspect of this layering approach is that each layer only has to interact with the
corresponding layer on the other side. For example, an application talks to
another application. TCP on one system deals with issues of retransmission and
message acknowledgement by talking to the TCP layer on the remote system.
A layer also does not need to be aware of the implementation of layers above it or
below it: they are just data sources and data sinks.</p>

<h2 id="protocolencapsulation">Protocol encapsulation</h2>

<p>if we want to send an IP packet (layer 3) out on an Ethernet network
(layers 1 and 2), we need to send out an Ethernet packet (an Ethernet
NIC or transceiver knows nothing about IP). The
entire IP packet becomes the payload (data) of an Ethernet packet.
Similarly, TCP and UDP, layers above IP, have their own headers,
distinct from IP headers (they need a port number, for example).
A TCP or UDP packet is likewise treated simply as data by the IP layer.
This wrapping process is known as <strong>protocol encapsulation</strong>. Each
layer of the networking stack can ignore the headers outside of its
layer and treat anything from higher layers simply as part of the
payload that needs to be sent.</p>

<h1 id="theapplicationlayer">The Application Layer</h1>

<h2 id="applicationarchitectures">Application architectures</h2>

<p>There are two ways that network applications are structured: <strong>client-server</strong> and <strong>peer-to-peer</strong>.</p>
<dl>
<dt>Client-server</dt>
<dd>This is the dominant model of interaction. One application, called the <strong>client</strong> (and usually run by the end user), requests
something from another application, called a <strong>server</strong>. The server provides a <strong>service</strong>.
Examples of this are a web browser (client) requesting a web page from a web server, a
mail application (client) accessing a mail server to get mailbox contents, or a print server being given
content to print. In this model, clients communicate with the server
and not with other clients.</dd>

<dt>Peer-to-peer</dt>
<dd>A peer-to-peer architecture employs a collection of applications, any of which can talk to any other.
These applications are <strong>peers</strong> and are generally run by a collection of end users
rather than some service provider. The name <strong>peer</strong> implies that there is no leader: applications all have equal
capabilities. An appealing aspect of a peer to peer design is <strong>self-scalability</strong>. As more
and more nodes join the collection of peers, the system has more peers to do the work and can hence
handle a large workload. Examples of peer-to-peer architectures are BitTorrent and Skype.</dd>

<dt>Hybrid</dt>
<dd>A difficulty with peer-to-peer architectures is that one often needs to do things such as keep track
of peers, identify which system can take on work or has specific content, and handle user
lookup and authentication. This led to a variation of the peer-to-peer model where
a <strong>coordinator</strong>, a central server, is in place to deal with these centralized
needs. However, the peers still handle all the bandwidth-intensive or compute-intensive work.</dd>
</dl>


<p>No matter what architecture is used, there is still a fundamental <em>client-server</em>
relationship. One system (a client) will send some request to another (a server).</p>

<h2 id="networkapplicationapi">Network Application API</h2>

<p>When we write network-aware applications, they need to use the network to communicate with
each other. These applications are no different than any other processes on the computer. Any
process can access the network and it is up to the operating system to coordinate this
access. When writing applications, the programmer will use a
set of interfaces referred to as a
<strong>Network API</strong>
(Application Programming Interface) to interact with the network and not worry about the lower
layers of the network.
For example, a programmer does not need to know about ethernet or IP
to communicate with another program but needs to have available the abstraction of being able to send data
from one logical port on an application to one on another application.</p>

<p>The communication session is the conducted by the <strong>application layer protocol</strong>.
This is a definition of the valid sequence of requests, responses, and their respective message formats
for a particular network service. The protocol needs to be well-defined for applications to
be able to communicate with each other.</p>

<p>Given a well-defined protocol, any application should be able to follow the rules of the protocol
and create messages that the other side can understand, regardless of the implementation language
or operating system. For instance, an iPad running an Swift should be able to talk to
a mail server written in Java running on a Windows platform.</p>

<p>The Network API will have core services, such as those related to sending and
receiving data, that are provided by the operating system. These are augmented with libraries
to handle other functions, such as looking up names, converting data, and
simplifying certain operating-system interfaces.</p>

<p>As programmers writing network-aware applications, we obviously need functions
for sending and receiving data but we may want to be able to specify something
about the behavior of that data over the network. For example:</p>
<dl>
<dt>Do we need reliable data transfer?</dt>
<dd>Is it important to the application that data arrives reliably and in order at the
destination? It seems like the answer should always be <em>yes</em>, but we have to
realize that ensuring reliability entails the detection, request for, and retransmission of
lost packets. This adds a considerable delay to the delivery of that lost packet.
For streaming media applications, such as telephony, that packet may arrive too late.
In this case, it is useless to the application and the application could just as
easily unsed best-effort service.
Moreover, some applications may choose to handle retransmission requests themselves in a
different manner.
Applications that can handle unreliable media streams are called <strong>loss-tolerant applications</strong>.</dd>

<dt>Throughput</dt>
<dd>An application may have specific bandwidth requirements. For example, video and voice telephony
applications may have minimum bandwidth needs. Applications with such needs are <strong>bandwidth
sensitive applications</strong>. Applications that can adapt to whatever bandwidth is available
(for example, switch to a lower bandwidth codec) are known as <strong>elastic applications</strong>.</dd>

<dt>Delay and Jitter</dt>
<dd>Interactive applications, such as voice and video telephony may want to ensure minimal
network delay to reduce the delay to the receiver. <strong>Jitter</strong> is the
variation in delay and these applications would also like to see low jitter.</dd>

<dt>Security</dt>
<dd>Applications may need to ensure that they are truly communicating with the
proper computer and a legitimate application on that computer. They may be concerned
about the integrity of the data that is being transmitted and want to ensure
that it cannot be modified or read by outside parties.</dd>
</dl>


<p>These are all legitimate desires. Unfortunately, IP gives us no control over
throughput, delay, jitter, and security. We can handle security at the application layer
and we will later examine mechanisms that were added to IP to support some degree of control
over packet delivery.</p>

<h2 id="iptransportlayers">IP Transport Layers</h2>

<p>Applications interact with IP&#8217;s transport layer.
There are two dominant transport-layer protocols on top of IP (IP is the network layer):
TCP and UDP (there are a few others, such as the SCTP, but these two dominate).</p>

<p>TCP, the <strong>Transmission Control Protocol</strong>, provides
<strong>connection-oriented</strong> service.
This does not imply that an actual network connection is being set up as one would
for a circuit-switched network.
We are strictly talking about transport-layer services here. For layer 2 and layer 3
protocols, a connection refers to setting up a pre-defined route (circuit) and
providing that connection with guaranteed bandwidth. At the transport layer (4),
we still strive strive to provide the illusion of a reliable bidirectional
communication channel but it is all done in software on top of unreliable
datagrams (IP). At the transport layer, the software does not have any control
of the route that packets take or the bandwidth that is available to the connection.</p>

<p>The TCP layer of software ensures that
packets are delivered in order to the application (buffering them in memory in the
operating system if any arrive out of order) and that lost or corrupt packets are
retransmitted. TCP keeps track of the destination so that the
application can have the illusion of a connected data stream (just keep
feeding data to the stream and don&#8217;t worry about addressing it). TCP provides
a <strong>full-duplex connection</strong>, meaning that both sides can send and
receive messages over the same link. TCP is <strong>stream oriented</strong>, meaning that
data is received as a continuous stream of bytes and there is no preservation
of message boundaries.</p>

<p><strong>UDP</strong>, the <strong>User Datagram Protocol</strong> is designed as a very thin
transport layer over IP. It provides <strong>connectionless</strong> service,
also known as <strong>datagram</strong> service. While UDP drops packets with
corrupt data, it does not ensure in-order delivery or reliable
delivery. UDP&#8217;s datagram service preserves message boundaries. If you send
<em>n</em> messages, you will receive <em>n</em> messages; they will not be combined into
one message.</p>

<p><strong>Port numbers</strong> in both TCP and UDP are used
to allow the operating system to direct the data to the appropriate application
or, more precisely, to the <strong>socket</strong> that is associated with the
communication stream on the application. A port number is just a 16-bit number
that is present in both TCP and UDP headers to identify a specific endpoint on a node.</p>

<h1 id="sockets">Sockets</h1>

<p><strong>Sockets</strong> are an interface to the network provided to
applications by the operating system. They were created at the University of
California at Berkeley for 4.2BSD (a derivative of UNIX) in 1983
and most operating systems now support this interface.
The purpose of sockets is to provide a protocol-independent interface for applications
to communicate with each other. The underlying network does not have to by IP.
Once set up, the socket looks like a file descriptor for an open file. With connection-oriented
(e.g., TCP) sockets, you can use the regular file system <em>read</em> and <em>write</em> system
calls to receive and send data.</p>

<p>Sockets are the mechanism that the operating system exposes to the user for accessing
the network.
A socket is created with the <strong>socket</strong>
system call and assigned a local address and port number with the <strong>bind</strong> system
call. The OS can fill in defaults if you do not want to specify a specific address and port.
(Note that you specify an address because your system might have multiple IP addresses; one for
each of its network interfaces). The <em>socket</em> also requires that the programmer identify
the <strong>address family</strong> for the socket (e.g., the protocol stack: IP, IP version 6, Bluetooth, local)
as well as the mode of communication (e.g., connection-oriented or datagrams).</p>

<h2 id="socketsforconnection-orientedprotocolsstreams">Sockets for connection-oriented protocols (streams)</h2>

<p>For connection-oriented protocols (TCP), a socket on a server can be set to
listen for connections with the <strong>listen</strong> system call. This turns it
into a listening socket. Its only purpose will now be to receive incoming connections. </p>

<p>The <strong>accept</strong>
call waits for a connection on a listening socket.
It blocks until a connection is received, at which point the server receives
a <em>new</em> socket that is dedicated to that connection.</p>

<p>A client establishes a connection with
the <strong>connect</strong> system call. After the connection is accepted by the server,
both sides now have a socket on which they can communicate.</p>

<p>Sending and receiving data is
compatible with file operations: the same <strong>read/write</strong> system calls can be
used. Data communication is <strong>stream-oriented</strong>. A sender can transmit an
arbitrary number of bytes and there is no preservation of message boundaries.</p>

<p>When communication is complete, the socket can be closed with the <strong>shutdown</strong>
or <strong>close</strong> system calls.</p>

<h2 id="socketsforconnectionlessprotocolsdatagrams">Sockets for connectionless protocols (datagrams)</h2>

<p>With connectionless protocols, there is no need to establish a connection or to close one.
Hence, there is no need for the <em>connect</em>, <em>listen</em>, or <em>shutdown</em>
system calls.</p>

<p>Unlike connection oriented sockets, data communication
is <strong>message-oriented</strong>.
A sender transmits a message. The size of the message is limited to the maximum
size allowable by the underlying network (the MTU, maximum transfer unit).</p>

<p>Because you need to specify the destination as the operating system does not keep
state of a &#8220;connection&#8221;, new system calls were created for sending and receiving
messages.
The <strong>sendto</strong> and <strong>recvfrom</strong> system
calls are used to send and receive datagrams. <em>sendto</em> allows you to
send a datagram and specify its destination. <em>recvfrom</em> allows you to
receive a datagram and identify who sent it.</p>

<h2 id="thejavainterfacetosockets">The Java interface to sockets</h2>

<p>Java provides many methods to deal with sockets and some commonly-used ones
consolidate the sequence of steps that need to be take place on the operating system.
The constructor for the <strong>ServerSocket</strong> class creates a socket for the TCP/IP protocol,
binds it to a specified port and host (using defaults if desired), and sets
that socket to the listening state.
A client&#8217;s <strong>Socket</strong> class constructor
creates a socket for the TCP/IP protocol, binds it to any available local port and host,
and connects to a specified host and port. It returns when the server accepts the connection.
The connected socket object allows you to acquire an InputStream and OutputStream for
communication.</p>

<h1 id="threadsconcurrencyandsynchronization">Threads, concurrency, and synchronization</h1>

<p>A process normally has one thread of execution, or flow of control.
A process may be <strong>multithreaded</strong>, where
the same program has multiple concurrent threads of execution.</p>

<p>In a multi-threaded process, all of the process&#8217; threads share the
same memory and open files. Within this shared memory, each thread
gets its own stack, which is where return addresses from functions are
placed and where local variables get allocated.
Each thread also has its own instruction pointer and registers.
Since memory is shared, it is important to note that there is
no memory protection among the threads in a process. Global variables
are freely accessible by all threads.
In particular, the <strong>heap</strong>, the pool of memory
that is used for dynamic memory allocation is shared and freely accessible
to all threads in the process. </p>

<h2 id="advantagesofthreads">Advantages of threads</h2>

<p>There are several benefits in using threads. Threads are more efficient than
processes.
The operating system does not need to create and manage a new memory map
for a new thread (as it does for a process). It also does not need to
allocate new structures to keep track of the state of open files and
increment reference counts on open file descriptors. Threading maps nicely
to multicore architectures and allows for the effective use of
multiple process cores.</p>

<p>Threading also makes certain types of programming easy. While it&#8217;s true
that there is a potential for bugs because memory is shared among threads,
shared memory makes it trivial to share data among threads. The same
global and static variables can be read and written among all threads
in a process. For a network server process, threading is appealing because it
becomes easy to write code that handles multiple client requests at
the same time. A common programming model is to have one <strong>master thread</strong>
that waits for client connections and then dispatches a <strong>worker thread</strong>
to handle the request.</p>

<p>While thread usage differs slightly among languages,
there always needs to be a mechanism to create a thread and to
wait for threads to exit.
When a thread is created, a specific method (function) is called
in that new execution flow. The original execution flow (the one thread
that started when the process began) continues normally.
When that thread eventually returns from that method, the thread terminates.
If a thread needs to wait for another thread, it can choose to block until the
other thread terminates. This is called a <strong>join</strong>. </p>

<h2 id="mutualexclusion:avoidingsteppingoneachother">Mutual exclusion: avoiding stepping on each other</h2>

<p>Because threads within a process share the same memory and hence share all global data (static variables,
global variables, and memory that is dynamically-allocated via <em>malloc</em> or <em>new</em>),
there is an opportunity for bugs to arise where multiple threads are reading and writing the same data
at the same time. A <strong>race condition</strong> is a bug
where the outcome of concurrent threads is unexpectedly dependent on
a specific sequence of thread scheduling.
<strong>Thread synchronization</strong> provides a way to ensure
<strong>mutual exclusion</strong>, where we can have regions of code that only
one thread can execute at a time.
Any other thread that tries to run in that region of code will go to sleep (be blocked) until the lock is
released when the current thread in that region leaves it.</p>

<p>Java allows a <strong>synchronized</strong> keyword to be added to a method to ensure
that no more than one thread will be allowed to run in that method. If that degree of
control is too coarse, Java also allows the programmer to use the <code>synchronized</code>
keyword to define a region of code that will be locked by a variable called a
<strong>monitor object</strong>. Any other thread that tries to enter any region
of code that is synchronized by the same monitor object will be blocked. This region
of code that provides mutual exclusion is called a <strong>synchronized block</strong>.</p>

<h1 id="domainnamesystem">Domain Name System</h1>

<p>A node on the Internet is identified by its IP address.
For IP version 4, the most common version deployed today, an IP address is a 32-bit value
that is expressed as a set of four bytes, each as a decimal number and separated
by dots. For instance, the IP address of the Rutgers web server is 199.83.128.67.
[IP version 6, which is rapidly expanding as we are out of IPv4 addresses in some areas,
is a 128-bit value and is expressed as a set of 8 groups of four hexadecimal digits.]
As humans, however, we prefer to identify endpoints by name rather than by a number.
For example, we think of the Rutgers web server by its name, www.rutgers.edu.
We will now explore the management of IP domain names and IP addresses and converting between them.</p>

<h2 id="howareipaddressesassigned">How are IP addresses assigned</h2>

<p>IP addresses are distributed hierarchically. At the very top level, an organization
called the <strong>IANA</strong> (<strong>Internet Assigned Numbers Authority</strong>)
is responsible for the entire set of IP addresses. It allocates blocks of addresses
to <strong>Regional Internet Registries</strong> (<strong>RIR</strong>).
There are five RIRs, each responsible for a part of the world&#8217;s geography. For instance,
the U.S. and Canada get addresses from <a href="http://www.arin.net">ARIN</a>,
the American Registry for Internet Numbers. Countries in Europe and the mid-East get addresses
from the RIPE Network Coordination Centre. These RIRs in turn allocate blocks of IP
addresses to ISPs within their region. Since ISPs are tiered, an ISP may allocate
a smaller block of addresses to a lower-tier ISP as well as to a company that subscribes
to its services. </p>

<h2 id="howarenamesassigned">How are names assigned</h2>

<p>In the early days of the ARPANET, each machine had to have a globally unique name.
The Network Information Center (<strong>NIC</strong>) at the Stanford Research Institute (SRI)
kept the master list of machine names and their corresponding IP addresses.
This solution does not scale. As the number of hosts on the Internet grew larger,
a <strong>domain hierarchy</strong> was imposed on the name space. This created a
a tree-structured name space with name management delegated to the various nodes of the tree, where
each node is responsible for the names underneath it.
Rutgers, for example, can name a new machine on the Internet
anything it wants as long as the name is unique within Rutgers and is suffixed with rutgers.edu.
The textual representation of <strong>Internet domain names</strong> is a set of strings delimited
by periods with each set representing a level in the naming hierarchy. The rightmost string is
the highest level in the hierarchy. </p>

<p>The hierarchy of Internet domain names has a single root under which are
<strong>top-level domains</strong> (<strong>TLDs</strong>). These are the .com, .edu, .org suffixes
that you are familiar with. Currently, there are 1,239 top-level domains.
They are divided into two categories: generic top-level domains and country-code top-level domains.</p>

<p><strong>Generic TLDs</strong> (<strong>gTLD</strong>) include the .com, .edu, .gov, .net, etc. domains.
Many of them date back to the first proposal of creating a domain hierarchy
<a href="http://tools.ietf.org/rfc/rfc920.txt">RFC 920</a>.
Each of these domain names is three or more characters long.
As the Internet became international, country-specific domains were created. These <strong>Country-code TLDs</strong>
(<strong>ccTLDs</strong>) are two-letter
<a href="http://en.wikipedia.org/wiki/ISO_3166">ISO 3166</a>
country codes (e.g., .ad for Andorra, .dk for Denmark, .es for Spain, .us for the U.S.A.).
The root of the domain hierarchy initially allowed only US-ASCII (Latin) characters.
This rule changed in 2009 and a
new set of <strong>Internationalized Domain Names</strong> for country code top-level domains
(<strong>IDN ccTLD</strong>) became available.
Examples of these domains are
ÿßŸÑÿ≥ÿπŸàÿØŸäÿ©. for Saudi Arabia, .—Ä—Ñ for Russia, and .‰∏≠Âúã for mainland China.
In 2011, internationalized domain names were approved for generic top-level domains (IDN gTLD),
giving us domains such as .„Åø„Çì„Å™ (&#8220;everyone&#8221; in Japanese), .ÁßªÂä® (&#8220;mobile&#8221; in Chinese), and
.–¥–µ—Ç–∏ (&#8220;kids&#8221; in Russian).</p>

<p>Each top-level domain has one administrator assigned to it. The IANA keeps track of the organizations
that manage the various top-level domains. Until 1999, for example, a company called
Network Solutions Inc. operated the .com, .org, and .net registries. Until that time,
Network Solutions maintained the registry of names and processed registration requests
from customers. Since then, the process has been decentralized to support <strong>shared registration</strong>.
This allows multiple companies to provide domain registration services.
One company is still assigned by the IANA to be the keeper of the master list for a specific
top-level domain. This list of registered domain names for a particular TLD
is called the <strong>domain name registry</strong>.
The company that maintains this registry is called the
<strong>domain name registry operator</strong>, also known as the <strong>network information center</strong> (<strong>NIC</strong>).
The IANA keeps track of all these organizations.
A <strong>domain name registrar</strong> is a company that provides <strong>domain registration services</strong>
to customers, allowing
them to register domain names for a fee. There are approximately 2,124 of these companies.
Examples of these are GoDaddy (with over 60 million domains), Namecheap, eNom, and Tucows.</p>

<p>When you pay GoDaddy, the <strong>registrar</strong>, $11.99 to register poobybrain.com, it consults the .com domain
name <strong>registry</strong> at Verisign, which is the <strong>registry operator</strong> for the .com domain.
If the domain name is available, GoDaddy becomes the <strong>designated registrar</strong> for that domain.
This means that Verisign knows that Go Daddy has information on the owner of poopybrain.com and that changes and
requests to transfer ownership or the registrar of your domain will have to come from that registrar.
Of the $11.99 that you paid GoDaddy, $7.85 went to Verisign as a registry fee (different TLDs have different
fees; .net registration costs $7.46). A $0.18 yearly fee that goes to ICANN to manage
the registry.</p>

<h2 id="associatingnameswithaddresses">Associating names with addresses</h2>

<p>We now saw how IP addresses are allocated and how domain names are registered. There is no
correlation between the two of them. A domain name does not imply a specific address and
adjacent IP address numbers may belong to completely different domain names.
We need a way to look up www.rutgers.edu and find out that its address is 199.83.128.67
since the IP layer knows absolutely nothing about domain names. Since the network
core has no interest in domain names, name-to-address resolution is handled at the
network edge, in the application before it establishes a socket connection. The
process of looking up a name is an <strong>application-layer protocol</strong>.</p>

<p>In the past, Stanford Research Institute&#8217;s Network Information Center maintained the entire
list of hosts on the internet (in a hosts.txt file; /etc/hosts on Unix systems). This file
would be periodically downloaded by every system on the Internet.
Clearly, this solution was not sustainable. We already saw that it made managing unique names
problematic. Moreover, with millions of hosts on the Internet, there was a lot of churn
in this database. Downloading a new copy of every host on the Internet constantly just doesn&#8217;t make sense. </p>

<p>The system that was put in place was a database of <strong>DNS servers</strong>
(Domain Name System servers). Like domain names themselves, DNS is a distributed,
hierarchical database. </p>

<p>A DNS server is responsible for a managing a sub-tree in the domain name hierarchy. For example,
a server might be responsible for everything under rutgers.edu or even just the machines under
under cs.rutgers.edu. This sub-tree of a group of managed nodes is called a <strong>zone</strong>.
Each <strong>authoritative name server</strong> is responsible for answering
queries about its <strong>zone</strong>. The authoritative name server for
rutgers.edu is therefore responsible for the rutgers.edu zone. The question now is,
how do you find it?</p>

<p>A DNS server accepts queries from clients (called <em>questions</em>) and provides
responses (called <em>answers</em>). By default, interactions with DNS servers use UDP for
improved performance, although TCP is almost always supported as well.</p>

<p>Any DNS server can be found by starting at the top of the name hierarchy.
There are 13 <strong>root name servers</strong> that can provide a list of
authoritative name servers for all the top-level domains (you can
download the list of root name servers
<a href="http://www.internic.net/domain/named.root">here</a>.
By contacting any one of these servers, you can find out the address
of a name server responsible for a specific top-level domain (such as .edu).
Then, by querying a name server for that domain (e.g., the .edu name server),
you can find a name server responsible for a name within that domain (such as rutgers.edu).
The process can continue until you find a name server that is responsible for the
zone that contains the host you need.</p>

<p>There are two basic approaches for name resolution when dealing with a hierarchy of
name servers: iterative or recursive queries.
An <strong>iterative</strong> query is a single query to a name server. That server will return the
best answer it can with the knowledge it has. This can be the the information
configured for its zone (e.g., the domain names for which it is responsible) or
cached results. If it does not have an answer to the query, it may return a <strong>referral</strong>.
A referral is the name server for the next lower layer, taking you closer to your destination.
For example, the root server can return a referral to tell you how to get to the .edu name server.
A query to the .edu name server can return a referral to tell you how to get to the rutgers.edu name server.
The advantage of this approach is that each name server can be completely stateless.
It either knows the answer or it does not.</p>

<p>With <strong>recursive resolution</strong>, the DNS server takes
on the responsibility of performing the set of iterative queries to other DNS servers
on behalf of the requestor and sends back a single answer.
With recursive resolution, a DNS server
may first send a query for the full domain name to the root name server.
The root name server will return a referral to, for example, the .edu name server.
A query to that server will then return a referral to the rutgers.edu name server.
In reality, the recursive server will cache past lookups so it will likely know
the addressses of recently-used top-level domains.</p>

<p>The advantage of recursive resultion is that it
incurs less communication at the client, simplifies the client&#8217;s protocol, and
allows for caching of results at all the intermediate servers. The
disadvantage is that a recursive server has to keep state about the client&#8217;s request until
it has completed all processing and is ready to send a response back to the client.</p>

<p>A DNS server is not obligated to support recursion.
Most top-level DNS servers, such as root servers, do not support recursive queries.</p>

<h2 id="howdoesadnsquerywork">How does a DNS query work?</h2>

<p>The client interaction with DNS is via a <strong>DNS resolver</strong>. This is a
a DNS server that is not necessarily part of the DNS hierarchy (that is, it does
not have to be a server responsible for a zone). However, it is capable of taking
a recursive request from a client and performing a set of iterative queries,
going to the root servers if necessary, to get the result. Resolvers could be
hosted on the client, within the organization, or by third parties such
as Google Public DNS, or OpenDNS. Most ISPs provide a DNS resolver service.
Many systems (such as Windows and Linux platforms)
support extremely limited local DNS resolvers that are incapable of iterative queries
and simply talk to another DNS server (e.g., a resolver hosted by the customer&#8217;s ISP). These
limited DNS resolvers are called <strong>stub resolvers</strong>.</p>

<p>DNS resolvers maintain a local cache of frequently used lookups
to avoid the overhead of repeated lookups for the same name and
to avoid the overhead of iterative queries. For example, it does not make sense to look up
the name server responsible for .com over and over for each query (it&#8217;s 192.5.6.30, by the way).</p>

<p>Let us look at the sequence of operations that a query for www.cs.rutgers.edu might take from
an application. We assume that the client machine is configured to use OpenDNS
as a DNS resolver service.</p>

<p>Rutgers registers its domain with educause.edu, the domain registrar for names in the edu TLD.
It provides Educause with a list of DNS servers that can answer queries for names underneath
rutgers.edu. Educause.edu, in turn, registers its DNS servers with ICANN, who is responsible
for the data in the root name servers.</p>

<ol>
<li><p>The client application contacts a local DNS <strong>stub resolver</strong>. This checks its
cache to see if it already has the answer. It also checks a local hosts file to
see if the answer is hard-coded in the configuration file. Giving up, it contacts
a real DNS resolver (e.g., OpenDNS at 208.67.222.222) and sends it a query for &#8220;www.rutgers.edu&#8221;.</p></li>
<li><p>The OpenDNS resolver checks its cache and doesn&#8217;t know either, so it
contacts one of the root name servers (let&#8217;s assume the resolver&#8217;s cache is
completely empty). It sends a query of &#8220;www.cs.rutgers.edu&#8221; to the root server
198.41.0.4 (a.root_servers.net).</p></li>
<li><p>The root server doesn&#8217;t have the answer but it knows the DNS server responsible
for the edu domain, so it sends a <strong>referral</strong> back to the OpenDNS
resolver giving it a list of name servers responsible for edu.</p></li>
<li><p>The resolver now sends a query of &#8220;www.cs.rutgers.edu&#8221; to 192.41.162.32, one
of the edu name servers (a.edu-servers.net at 192.5.6.30). It does not know the answer either
but it does know the name servers for rutgers.edu, so it sends back
a <strong>referral</strong> with a list of those servers.</p></li>
<li><p>The resolver now sends a query of &#8220;www.cs.rutgers.edu&#8221; to 192.230.122.7,
one of the rutgers.edu name servers (ns8.a1.incapsecuredns.net). This happens to be
an authoritative name server for the rutgers.edu zone and it
returns back the address, 128.6.4.24. If cs.rutgers.edu was defined as a separate zone,
the rutgers.edu DNS server would send a referral to yet another name server.</p></li>
<li><p>The query is now complete and the OpenDNS resolver sends the result back
to the stub resolver that requested the query, which sends it back to the
client application.</p></li>
</ol>

<h2 id="insideadnsserver">Inside a DNS server</h2>

<p>DNS servers store various information about domain names. Each datum is called
a <strong>resource record</strong>. A resource record contains a name, value, type of
record, and a time to live value.
Common records include:</p>

<ul>
<li><p><strong>Address</strong> (A record): identifies the IP address for a given host name.</p></li>
<li><p><strong>Canonical name</strong> (CNAME record): identifies the real host name for an alias.
For example, www.cs.rutgers.edu is really a CNAME (alias) to
www3.srv.lcsr.rutgers.edu.</p></li>
<li><p><strong>Name server</strong> (NS record): identifies the authoritative name servers for the domain.</p></li>
<li><p><strong>Mail exchanger</strong> (MX record): identifies the mail server for a given host name.</p></li>
</ul>

<p>DNS uses a simple request-response protocol. Each query message from a client has
a corresponding response message from the server. The exact same binary message structure
is used for all DNS messages. A flag field identifies whether the message is a query
or a response and whether recursion is desired.
A variable-length set of fields after the fixed-length message header contains
questions (e.g., that you are looking for the A record of www.rutgers.edu) and
answers (the responses to the questions).</p>

<h2 id="caching">Caching</h2>

<p>As we mentioned earlier, DNS resolvers rely on caching to avoid performing the
same queries over and over. Every DNS zone contains a time to live
value, which is an estimate of how long it is safe for a resolver to keep the
results for that zone cached. For example, systems under rutgers.edu have a TTL of 3600 seconds
(1 hour) while systems under google.com have a TTL of 900 seconds (15 minutes).</p>

<h2 id="reversedns">Reverse DNS</h2>

<p>DNS servers are also able to take an IP address as a query and resolve a domain name for
the address.
Doing this requires a different query path: the edu server has no idea what range of IP
addresses were allocated to Rutgers; it just knows the name servers for Rutgers.</p>

<p>A special domain, <code>in-addr.arpa</code> is created for reverse lookups
(<em>arpa</em> stands for <em>Address &amp; Routing Parameter Area</em>).
The IP address to be queried is written in reverse order, with the first byte last, to construct a name
that looks like 24.4.6.128.in-addr.arpa for the address 128.6.4.24.</p>

<p>An organization has a range, or several ranges, of IP addresses assigned to it.
It sets up a local DNS
server with PTR (pointer) records that map IP addresses to names.
It then tells its ISP what DNS servers are responsible for reverse DNS lookups.
The ISP knows what range of addresses belong to the organization. If it gets a query for
an address in that range, it now knows which name servers to send on a referral reply.
A reverse query that starts at the root will contact the root name servers.
These servers, in addition to knowing the name servers of TLDs, also know the
name servers for the five RIRs (ARIN, RIPE NCC, etc.) - the entities that hand out IP addresses.
The root server
may return a referral for the ARIN server (responsible for IP addresses in North America).
The ARIN server knows the blocks of IP addresses that were allocated to various ISPs
and will send a referral to the name server for the appropriate ISP. That ISP, when
queried, will then respond with a referral to the name server for the organization that
owns that address.</p>

<h2 id="dnstermsglossary">DNS Terms Glossary</h2>

<ul>
<li><strong>IANA</strong>: Internet Assigned Numbers Authority, the organization in charge of keeping track of IP addresses, port numbers, and other number-related aspects of the Internet</li>
<li><strong>ICANN</strong>: Internet Corporation for Assigned Names and Numbers, the non-profit company that currently runs the IANA.</li>
<li><strong>RIR</strong>: Regional Internet Registry, assigns IP addresses to ISPs within a geographic region.</li>
<li><strong>TLDs</strong>: top-level domains.</li>
<li><strong>gTLD</strong>: generic top-level domain (.com, .edu, .net, &#8230;).</li>
<li><strong>ccTLD</strong>: country code top-level domain (.ac, .ae, .ie, .nl, .us).</li>
<li><strong>IDN</strong>: Internationalized Domain Names.</li>
<li><strong>Domain name registry</strong>: the database of registered domain names for a top-level domain.</li>
<li><strong>Domain name registry operator</strong>: the company that keeps the database of domain names under a TLD.</li>
<li><strong>NIC</strong>: network information center, another name for a domain name registry operator.</li>
<li><strong>Domain name registrar</strong>: a company that lets you register a domain name.</li>
<li><strong>DNS</strong>: Domain Name System.</li>
<li><strong>Canonical name</strong>: a name that is an alias for a another domain name.</li>
<li><strong>Authoritative name server</strong>: a name server that stores, and is responsible for, specific DNS records (as opposed to storing a cached copy)</li>
<li><strong>Zone</strong>: a portion of the domain name space (a sub-tree) that is managed by a specific entity. E.g., rutgers.edu is a zone that manages all domains within rutgers.edu.</li>
<li><strong>A (address) record</strong>: a DNS record that stores the IP address corresponding to a specific host name.</li>
<li><strong>MX (mail exchanger) record</strong>: a DNS record that stores the name of the host that handles email for the domain.</li>
<li><strong>DNS resolver</strong>: the client side program that is responsible for contacting DNS servers to complete a DNS query.</li>
<li><strong>Reverse DNS</strong>: querying IP addresses to find the corresponding domain names.</li>
</ul>

<h1 id="http">HTTP</h1>

<p><strong>HTTP</strong> stands for <strong>Hypertext Transfer Protocol</strong>
and is the web&#8217;s application-layer protocol for interacting between web browsers
and web servers.
It is a TCP, line-oriented, text-based protocol that consists of requests to the
server followed by responses from the servers.
The protocol is <strong>stateless</strong>. This means that the server does not
store any state from previous requests. This simplifies the design of the protocol,
simplifies recovery from crashes, and makes load balancing easier. Note that
web application that use HTTP may impose their own state but it is not a part
of the HTTP protocol.</p>

<h2 id="persistentvs.non-persistentconnections">Persistent vs. non-persistent connections</h2>

<p>HTTP was originally designed to support <strong>non-persistent connections</strong>.
This meant that the connection was alive for only a single request-response interaction.
For each new request, the client had to re-establish a connection. That may have been fine in
the earliest days of the web but a request for a page is now typically accompanied
by multiple successive requests to download supporting files (stylesheet files and images). The overhead
of the round-trip time in setting up a connection for each piece of content adds up.
HTTP was enhanced to support <strong>persistent connections</strong>, where a client
and server can exchange multiple request-response interactions on the same connection.</p>

<h2 id="requestsandresponses">Requests and responses</h2>

<p>The main function of HTTP is to request objects (content). These are identified in a
browser
by a <strong>URL</strong> (<strong>Uniform Resource Locator</strong>). A URL takes the format:</p>

<pre><code>protocol://domain_name:port_number/path/to/object/the_object
</code></pre>

<p>Browsers support various protocols, not just HTTP. Common ones include HTTP, HTTP (HTTP
that is made secure via SSL), FTP (file transfer protocol), and &#8220;file&#8221; (local files).
If the protocol is &#8220;http&#8221; or &#8220;https&#8221;, the browser process it via its HTTP protocol
module.</p>

<p>The HTTP protocol comprises requests and responses. Each of these messages
is structured as a set of text headers, one per line, followed by a blank line,
and optionally followed by content. The first line of a request contains a command.
The three main HTTP requests are:</p>

<ul>
<li><p><strong>GET</strong>: request an object</p></li>
<li><p><strong>HEAD</strong>: like GET but download only the headers for the object.</p></li>
<li><p><strong>POST</strong>: upload a sequence of name/value pairs to the server. The
data is present in the body of the message and is often the response to a form.
An alternate way of uploading user data as a set of name/value pairs is
to use the <strong>GET</strong> command and place the data as a set of
parameters at the end of the URL. For example,
<code>http://www.pk.org/test?name=paul&amp;id=12345&amp;ref=zzz</code></p></li>
</ul>

<p>Each HTTP response contains multiple headers, the first of which contains a status code and corresponding
message.</p>

<h2 id="cookies">Cookies</h2>

<p>While the HTTP protocol itself does not require keeping state, HTTP provides
a way for web servers to store state about past sessions from the browser.
It does this through <strong>cookies</strong>.
A cookie is a small amount of data that is associated with the web site.
The data is created by the server when it gets an HTTP request from the client.
It then sends that data back in a <code>Set-Cookie</code> line in the header of the HTTP response.
Future HTTP requests to the same server will contain a <code>Cookie</code> line in the header
and contain the data that is associated with the cookie.</p>

<p>This simple mechanism allows a web server to create a database entry indexed by
a the cookie data to keep track of a user&#8217;s session. That database entry can include things such as
shopping cart contents, authentication state, pages visited, time spent on a page, etc.
The actual cookie itself does not need to store any of this; it just serves as a unique
key into the database table.</p>

<p>Because a web page may contain content from other web sites (hence, other servers),
it is possible that requests to those sites will result in the generation of cookies.
A <strong>first-party cookie</strong> is one that comes from the web server that
is serving your page request.
A <strong>third-party cookie</strong> is one that comes from another web server that
serves some content that is present on the page you originally requested.
There has been concern over third party cookies in that they allow these parties,
usually advertisers, to track your visits to specific web sites. Most web browsers
block third-party cookies by default.</p>

<h2 id="caching">Caching</h2>

<p>Caching avoids the need to request the same content over and over from the server.
However, the challenge is to find out whether the content that is in the cache is
still valid. HTTP provides a <strong>conditional GET</strong> mechanism that
is triggered by two lines in the GET header.</p>

<p>When a browser requests content from a server via an HTTP GET message, the
response headers include two lines. One is a <code>Last-Modified</code>
header that contains the timestamp of the last modification time of that
content. The second is a <code>ETag</code> header that contains a hash
of the content. The client stores both of these values along with the
copy of the content in its cache.</p>

<p>When the content is requested again, the client issues an HTTP GET
request to the server but includes two lines in the headers. One
is an <code>If-Modified-Since</code> line that contains the last modification
time from the cache and the other is an <code>If-None-Match</code>
line that contains the value from the ETag. This allows the
server to check whether the content has changed since the
version that the client cached. If it did not, the server responds
with a <code>Not Modified</code> message and no content. If it did,
the server responds just as it would with a regular GET request.</p>

<h2 id="cachingproxies">Caching proxies</h2>

<p>Caching does not need to be handled only by the web browser. A <strong>caching proxy</strong> is
an HTTP server that is hosted within a company&#8217;s LAN. All HTTP requests go to
that server instead of to the actual destinations (browsers allow you to define
a proxy server for some or all domains). The proxy server, in turn, passes the
request out to the requested server. This seems like an extra layer of overhead
(the client connects to the proxy and sends its request; the proxy then does the same).
However, the proxy can cache content. If you request content that somebody else
previously requested, it is likely to be cached and the organization incurs less
traffic on its outside Internet connection. Even pages that change frequently will
usually have significant portions that can be cached, such as CSS files, JavaScript files,
and supporting graphics.</p>

<h2 id="optimizations">Optimizations</h2>

<p>A web browser connects to a web server, issues an HTTP request for a web page (an
html file), and then parses it to see what additional objects need to be requested
to render the page. This typically includes multiple images, CSS files, and possibly
additional content such as JavaScript files. Requests are then issued for these objects.
One problem with issuing HTTP requests to a server one at a time is that one large
response (or a slow one) can hold up all other requests that the client will make.
This is called <strong>head-of-line blocking</strong>.</p>

<h3 id="parallelconnections">Parallel connections</h3>

<p>One way to avoid head of line blocking is to have the browser open a separate TCP connection
for each HTTP request. There are several downsides to this. Many web pages have many dozens or
even hundreds of objects (think of a photo thumbnails gallery, for instance). Opening a large
number of connections can take substantial time. Moreover, it can consumer substantial resources
at the server since each connection requires kernel and application memory resources as well as CPU time.
Because of this, browsers support parallel connections but usually limit them to a small number
(typically four). Once you limit the number of connections, you again have the risk of head-of-line blocking.
Another problem with pipelining is that there&#8217;s no assurance that it will work
if a proxy is present. Just because your browser establishes several TCP connections to
the proxy does not mean that the proxy will, in turn, establish those connections to the server.</p>

<h3 id="pipelining">Pipelining</h3>

<p>Another performance optimization was <strong>HTTP pipelining</strong>. With pipelining, instead of waiting
for each response, multiple HTTP requests can be dispatched one after another over one connection.
However, the server is still obligated to issue responses in the order that the requests were
received and head-of-line blocking is still an issue since one delayed or long response can
hold up the responses behind it. Most browsers as well as proxies have disabled pipelining or do not implement it.</p>

<h3 id="http2multiplexing">HTTP/2 Multiplexing</h3>

<p>HTTP/2, the next major update to the HTTP protocol, which came out in 2015, supports the same commands
as its predecessor, HTTP/1.1. However, it adds a number of optimizations.</p>

<p>HTTP/2 supports <strong>multiplexing</strong>. This allows multiple messages to be interleaved on one connection. It
is a form of a session layer (implemented in the application, of course). A large response may be broken up
into multiple chunks with other responses interleaved among it. The browser keeps track of the pieces
and reassembles all the objects.</p>

<h3 id="http2serverpush">HTTP/2 Server Push</h3>

<p>The HTTP/2 protocol adds a <strong>server push</strong> capability that allows the server to send objects to the
client proactively. The client, upon receiving them, can add them to its cache. This is useful for
objects such as stylesheets that are used by an HTML page. Normally, the browser would have to
first receive the HTML page so it can parse it before issuing requests for objects that the page
needs. If this information is given to the server, it can start sending these objects before the
server requests them.</p>

<h3 id="http2headercompression">HTTP/2 Header compression</h3>

<p>HTTP request and response headers tend to be verbose and are text-based. Their size often requires
several round trips just to get the headers for a page out to the server. Compressing headers
can make requests and responses shorter and speed up page loads. </p>

<h1 id="ftp">FTP</h1>

<p><strong>FTP</strong>, the <strong>file transport protocol</strong>,
is one of the earliest Internet protocols and was designed to transfer files between computers.
The protocol uses TCP and is based on commands and responses.
A command is a single line of ASCII text. A response is also a single line
of text and contains a status code along with a message.</p>

<p>To communicate, a client establishes a TCP connection from some available port <em>N</em>
to port 21 on the server. Commands and responses are sent over this communication
channel. Some basic commands are <strong>USER</strong> to identify a user name,
<strong>PASS</strong> to specify the password, <strong>GET</strong> to download
a file, <strong>PUT</strong> to upload a file, and <strong>DIR</strong> to get
a directory listing.</p>

<p>If the command is a request for data transfer (such as putting a file, getting
a file, or getting a directory listing), the server initiates a TCP connection
back to the client on port <em>N+1</em>. Data is then transferred over this channel
(either from client to server or server to client, depending on the request) and the connection
is then closed. FTP is unique compared with most other protocols in that it separates
<strong>control</strong> and <strong>data channels</strong>. Control information is sent
<strong>out of band</strong>, on a different channel than the data.</p>

<h2 id="passivemode">Passive mode</h2>

<p>Because having a server connect to a client proved problematic in some environments,
FTP supports an alternate mechanism, called <strong>passive mode</strong>, where the
client connects to the server to set up the data channel. This is now the more popular
mode of operation and some FTP clients, such as web browsers, only support this mode.</p>

<h1 id="smtp">SMTP</h1>

<p>The <strong>Simple Mail Transfer Protocol</strong> (<strong>SMTP</strong>)
is designed for delivering mail to a server that hosts the recipient&#8217;s mailbox.
It is a TCP-based protocol that is line-based and uses ASCII text for all interactions.
An SMTP server acts as a client and a server.
Typically a mail application uses SMTP to send a message to a user&#8217;s SMTP server (e.g.,
smtp.gmail.com). This server is often hosted by the organization that provide&#8217;s the
sender&#8217;s email service (e.g., Google, Comcast, Rutgers). This SMTP server then queues
the message for delivery. To deliver the message, it acts like a client. The SMTP
server looks up the DNS MX (mail exchanger) record for the destination domain, connects
to that SMTP server, and delivers the message. The receiving server places the message
in the user&#8217;s mailbox. If the user has an account on that machine and runs the mail
client locally, the mail client can access the mailbox and read the message.
More often, the user is on a different system and needs to fetch messages. For that,
mail retrieval protocols, such as POP or IMAP, must be used.</p>

<p>The SMTP protocol consists of server identification (<code>HELO</code>),
specifying who the mail is from (<code>MAIL FROM:</code>), and then specifying
one or more recipients, one per line (<code>RCPT TO:</code>). Finally, the
message is send with the <code>DATA</code> command. The message is multiple lines
of ASCII text and typically starts with the mail headers that you see in your email.
It is useful to note that all those mail headers are of no value to SMTP; it just
treats them as the message data. You can have a completely different list of names
in the <code>To:</code> header than you specified in the SMTP <code>RCPT TO</code>
commands and the mail will only be delivered to the recipients you listed
with the <code>RCPT TO</code> commands.</p>

<p>SMTP is an example of a <strong>push protocol</strong>. The client takes content
and sends it to the server. HTTP, on the other hand, is a <strong>pull protocol</strong>.
The client connects to it and asks it for content.</p>

<p>Because SMTP was designed to handle only text-based interaction, sending mail containing
binary data, such as a jpeg file, was problematic. To remedy this, an encoding format
called <strong>MIME</strong> (<strong>Multipurpose Internet Mail Extensions</strong>)
was created. This defines formats for encoding content in a suitable format for
message delivery. A MIME header in the body of the email identifies the content type
and encoding used. To support mail attachments and the encoding of multiple objects,
<strong>multipart</strong> MIME headers in the message body allow one to identify
multiple chunks of content. MIME has nothing to do with SMTP but is designed to
cope with the restrictions that SMTP placed on the structure of a message
(7-bit ASCII text with line breaks). It is up to mail clients to create and
parse MIME encodings.</p>

<h1 id="pop3">POP3</h1>

<p>SMTP dealt only with mail delivery.
POP3 is a TCP-based protocol to allow a user to connect to a remote mailbox, download, and delete messages.
The entire protocol is text-based. A user authenticates with <code>user</code> and <code>pass</code>
commands and then sends commands to list messages, retrieve a specific message, or delete a message.</p>

<p>POP3 supports two interaction models.
The <strong>download-and-delete</strong> model has a client connect to a mail server, download messages
to the client&#8217;s local mailbox, and then delete them from the server. With this model, the server is just
a temporary repository for mail until the client gets around to downloading it. The problem with this
model is that it does not work if you access mail from multiple devices. Once a message is deleted
from the server, other devices cannot get it.</p>

<p>The <strong>download-and-keep</strong> model has the client connect to a mail server, download
messages to the client&#8217;s local mailbox, but does not delete them from the server. They only get
deleted when a user deletes them locally and the mail client connects back to the server and
issues a POP3 delete command for those messages. With this behavior, a user can access messages
from multiple devices.</p>

<p>The downside of POP3 is that it does not keep state across sessions. It does not know, for example,
if a user marked several messages for deletion during a previous connection session.</p>

<h1 id="imap">IMAP</h1>

<p><strong>IMAP</strong>, the <strong>Internet Message Access Protocol</strong>
was designed to operate on a mailbox remotely rather than POP&#8217;s approach
of retrieving the contents of a mailbox onto a client. It can handle the
case where multiple clients are accessing the same mailbox and can
keep operations synchronized since state is maintained on the server.</p>

<p>IMAP also supports the ability to move messages into folders,
search for specific messages on the server, mark messages for deletion
prior to actually deleting them, and fetch headers or full messages.
It allows the same offline convenience that POP does, where all content
can be downloaded onto a client, but also offers full state tracking
on the server.</p>

<p>Like POP, SMTP, and HTTP, IMAP commands are also sent as lines of ASCII text.
Unlike those protocols, requests and responses can be handled asynchronously;
a client can send multiple requests without first waiting for responses.</p>

<h1 id="peertopeerprotocols">Peer to Peer Protocols</h1>

<p>Traditional, and still the most common, network-based applications
are those that follow a client-server model. A client needs a
service (access to a file&#8217;s contents, for example) and contacts
a server that can provide that service.
A peer-to-peer model is an alternative application architecture
that removes the need for dedicated servers and enables each
host to participate in providing the service. Because all machines
can both access as well as provide the service, they are called <strong>peers</strong>.</p>

<p>A true peer-to-peer architecture has no reliance on a central
server. In practice, some peer-to-peer architectures are really
<strong>hybrid architectures</strong>, where a central server may provide key
authentication or location services. Desirable (but not necessary)
characteristics of peer-to-peer application architectures are
robustness and self-scalability. <strong>Robustness</strong> refers to the ability
of the overall service to run even if some systems may be down.
<strong>Self-scalability</strong> refers to the ability of the system to handle
greater workloads as more peers are introduced into the system.</p>

<p>In our discussions, we focused on just one application domain: peer-to-peer
file distribution.</p>

<p>For file distribution, there are four key operations (primitives):
(1) how a peer joins and leaves a peer-to-peer system;
(2) how peers register files and their metadata (names, attributes);
(3) how search is handled;
and (4) how files are downloaded.</p>

<p>The systems that we examine may or may not tackle all of these areas.</p>

<h2 id="napster">Napster</h2>

<p>Napster is the earliest of peer-to-peer systems and is the
system that put peer-to-peer file sharing on the map. It was
built for sharing MP3 files. Napster is not a pure peer-to-peer
architecture since it relies on a single server to keep track
of which peer has which content. </p>

<p>A peer contacts the central server and publishes a list of files
that it wants to share. Anyone who wants to find a file contacts
the central server to get a list of peers that have the
file. The peer then connects to any of the peers in that list and
downloads the file. The download is either via a direct TCP
connection to the server or, if the system is inaccessible because
it is behind a firewall, it contacts the central server to send
a message to the desired peer requesting that it connect and upload to the
requestor.</p>

<p>The advantage of Napster is that it is a simple design.
The use of a central server, while deviating from a true
peer-to-peer model, establishes a single point of control
and maintains all the information on the locations of content.</p>

<p>The downside is that the server can become a bottleneck
with high query volumes. The failure of the central server
causes the entire system to cease to operate.</p>

<h2 id="gnutella">Gnutella</h2>

<p>After Napster was shut down by shutting down its central server,
Gnutella set out to create an architecture that offers truly
distributed file sharing. Unlike Napster, Gnutella can not be shut
down since there is no central server.</p>

<p>Gnutella&#8217;s approach to finding content is based on <strong>query flooding</strong>.
When a peer joins the system, it needs to contact at least one other
Gnutella node and ask it for a list of nodes it knows about (its
&#8220;friends&#8221;). This list of peers becomes its list of <strong>connected
nodes</strong>. This builds an <strong>overlay network</strong>. An overlay network
is a logical network that is formed by peer connections. Each peer
knows of a limited set of other peers. These become its <strong>neighbors</strong>,
and do not need to be physical neighbors. A peer is capable of
communicating with any other peer; it is just the lack of knowing
that the other peer exists that stops it.</p>

<p>To search for content, a peer sends a query message to its connected
nodes. Each node that receives a query will respond if it has
the content. Otherwise, it forwards the content to its connected
nodes. This is the process of <strong>flooding</strong>.
Once the content is found, the requesting peer downloads the
content from the peer hosting the content via HTTP.</p>

<p>A facet of the original design of Gnutella was anonymity. Replies were
sent replies through the
same path that the queries took. A peer receiving a query would
not know if it came from the requestor or from a peer just
forwarding the request.</p>

<p>Gnutella has a significant architectural advantage over Napster.
Its design is fully decentralized. There is no central directory
and hence the service cannot be shut down. On the other hand,
flooding-based search is inefficient compared to maintaining a single database.
Search may require contacting a large number of systems and going
through multiple hops. Well-known nodes (e.g., those that may be
configured in default installations) may become overly congested.</p>

<p>A few optimizations were later added to Gnutella. </p>

<ul>
<li><p>The process of routing replies through the query path was
changed to sending responses directly to the requester to reduce response times.</p></li>
<li><p>If connecting
to a peer that serves the content is not possible because of firewall
restrictions at the peer, the requesting node can send a <strong>push request</strong>,
asking the serving peer to send it the file.</p></li>
<li><p>Much of the Gnutella network was composed of end user&#8217;s
personal machines and these had varying levels of uptime and connectivity.
As such, not all peers are equal. With this in mind, Gnutella
divided its peers into two categories: <strong>leaf nodes</strong> and
<strong>ultrapeers</strong>. Leaf nodes are normal peers. They know of a small
number of ultrapeers and may not have fast connections. Ultrapeers
are peers that have a high degree of connectivity (32 or more connections
to other ultrapeers) and can hence flood queries with more hops.</p></li>
</ul>

<h2 id="kazaa">Kazaa</h2>

<p>Kazaa was created a year after Gnutella with the core premise that
not all nodes have equivalent capabilities as far as network connectivity
and uptime are concerned. They introduced the concept of <strong>supernodes</strong>.
These nodes have high uptime, fast connectivity, faster processors, and
potentially more storage than regular nodes. They also know other
supernodes. This is the same concept as Gnutella&#8217;s later enhancement
with its addition of <strong>ultrapeers</strong>. A client (peer) needs to know
of one supernode to join the system. It sends that supernode a list
of all the files that it is hosting. Only supernodes are involved in
the search process. Search is a flood over the overlay network as in Gnutella.
Once a query reaches a supernode that has the requested content
in its list, it sends a reply directly to the peer that initiated
the query. The querying peer will then download the content from the peer
that hosts the content.</p>

<h2 id="bittorrent">BitTorrent</h2>

<p>The design of BitTorrent was motivated by the <strong>flash crowd</strong> problem. How
do you design a file sharing service that will scale as a huge
number of users want to download a specific file? Systems such
as Napster, Gnutella, and Kazaa all serve their content from
the peer that hosts it. If a large number of users try to download
a popular file, all of them will have to share the bandwidth that
is available to the peer hosting that content.</p>

<p>The idea behind BitTorrent is to turn a peer that is downloading
content into a server of that content. The more peers are downloading
content, the more servers there will be for it. BitTorrent only
focuses on the download problem and does not handle the mechanism
for locating the content.</p>

<p>To offer content, the content owner creates a <strong>.torrent file</strong>.
This file contains metadata, or information, about the file,
such as the name, creation time, and size of the file. It also
contains a list of hashes of blocks of the content.
The content is logically divided into fixed-size blocks and the
list of hashes in the .torrent file
allows a downloading peer to validate that any downloaded blocks has been
downloaded correctly. Finally, the .torrent file contains a
list of <strong>trackers</strong>. </p>

<p>The <strong>tracker</strong> is a server running a process that manages
downloads for a set of .torrent files. When a downloading peer
opens a .torrent file, it contacts a tracker that
is specified in that file. The tracker is responsible for keeping
track of which peers have which have the content. There could be
many trackers, each responsible for different torrents.</p>

<p>A <strong>seeder</strong> is a peer that has the entire file available for download
by other peers. Seeders register themselves with trackers so
that trackers can direct downloading peers to them. An <strong>initial seeder</strong>
is the initial version of the file.</p>

<p>A <strong>leecher</strong> is a peer that is downloading files. To start the
download, the leecher must have a .torrent file. That identifies the
tracker for the contents. It contacts the tracker, which keeps track of the seed nodes for
that file as well as other leechers, some of whom may
have already downloaded some blocks of the file. A leecher contacts
seeders and other leechers to download random blocks of the file.
As it gets these blocks, it can make them available to other leechers.
This is what allows download bandwidth to scale: every downloader
increases overall download capacity. Once a file is fully downloaded,
the leecher has the option of turning itself into a seeder and continue to
offer serving the file.</p>

<p>BitTorrent scales very well. The more participants there are, the
greater the aggregate bandwidth is. Peers may be given an incentive to
share since BitTorrent software may choose to block downloads if
you don&#8217;t offer uploads. The downside of BitTorrent is that unpopular
files will not have leechers and will not offer this benefit of scale.
Block sizes tend to be large (the default is often 256 KB with a maximum
size of 4 MB). This makes the architecture not suitable for small files
as the distributed download aspect won&#8217;t come into play unless a large
number of leechers choose to act as future seeders. Finally, search is
not a part of the protocol. A user needs to turn to some other mechanism
to actually get the .torrent file.</p>

<h2 id="distributedhashtables">Distributed Hash Tables</h2>

<p>The systems we covered use one of three approaches for
locating content:</p>

<ol>
<li>Central server (Napster)</li>
<li>Flood (Gnutella, Kazaa)</li>
<li>Nothing (BitTorrent). Search is out
of scope for BitTorrent and it relies on
separate solutions to allow users to locate a
.torrent file for the desired content.</li>
</ol>

<p>Flooding can be an inefficient and indeterminate
procedure for finding content. Some nodes may
be slower than others and some may have fewer
connections than others, resulting in more hops
to query the same number of machines. Gnutella
and Kazaa tried to ameliorate this somewhat by
creating ultrapeers (supernodes) but the mechanism
of the flood still exists.</p>

<p>In standalone systems, <strong>hash tables</strong> are attractive
solutions for high-speed lookup tables.
A <strong>hash function</strong> is applied to a search <strong>key</strong>.
That result becomes an index into a table.
Hash tables result in O(1) lookup performance
versus the O(log N) time for a binary tree or
search through a sorted table. Since there is a
chance that multiple keys hash to the same value
(known as a <strong>collision</strong>), each table entry,
called a <strong>slot</strong> (or <strong>bucket</strong>),
may contain a linked list or additional hash table.</p>

<p>A <strong>distributed hash table</strong>, or <strong>DHT</strong>, is
a peer-to-peer version of a hash table: a distributed
<em>key, value</em> database. The interface we want for
a DHT is that a client will query a DHT server with a key
to get the corresponding value. This DHT server may be
a separate collection of peer-to-peer systems, all
acting as one server from the client&#8217;s point of view or the
querying client may also be a peer. The DHT software
finds the host that holds the <em>key, value</em> pair
and returns the corresponding value to the querying host.
This should be done without the inefficiency
of a flood. The specific implementation of a DHT that
we examine is called <strong>Chord</strong> and it creates an overlay
network that is a logical ring of peers.</p>

<p>Chord takes a large hash of a key (e.g., a 160-bit SHA&#8211;1 hash).
Each node in the system is assigned a position in the ring
by hashing its IP address. Because the vast majority of bucket positions will be empty,
<em>key, value</em> data is stored either at the node to which the key hashes (if, by some
chance, the key hashes to the same value that the node&#8217;s IP address hashed)
or on a <strong>successor</strong> node, the next node that would be encountered as the
ring is traversed clockwise. For a simple example, let us suppose that we have a 4-bit
hash (0..15) and nodes occupying positions 2 and 7. If a key hashes to 4, the <em>successor</em>
node is 7 and hence the machine at node 7 will be responsible for storing all data
for keys that hash to 4. It is also responsible for storing all data to keys that hash
to 3, 5, 6, and 7.</p>

<p>If a node only knows of its clockwise neighbor node, then any query that a node cannot
handle will be forwarded to a neighboring node. This results in an unremarkable <em>O(n)</em>
lookup time for a system with <em>n</em> nodes. An alternate, faster, approach is to have
each node keep a list of all the other nodes in the group. This way, any node will be
able to find out out which node is responsible for the data on a key simply by hashing
the key and traversing the list to find the first node &ge; the hash of the key. This
gives us an impressive <em>O(1)</em> performance at the cost of having to maintain
a full table of all the nodes in the system on each node.
A compromise approach to have a bounded table size is to use <strong>finger tables</strong>.
A finger table is a partial list of nodes with each node in the table being a factor of two
away from the current node. Element 0 of the table is the next node (2<sup>0</sup> = 1 away),
element 1 of the table is the node after that (2<sup>1</sup> = 2 away),
element 2 of the table four nodes removed (2<sup>2</sup>),
element 3 of the table eight nodes removed (2<sup>3</sup>), and so on. With finger tables,
<em>O(log n)</em> nodes need to be contacted to find the owner of a key.</p>


<h1 id="transportlayer">Transport Layer</h1>

<p>The network layer (layer 3 of the OSI stack) is responsible for machine-to-machine
communication. The transport layer, one layer higher (layer 4), provides logical
communication channels between applications. An application can create an
arbitrary number of these channels, each of which has another endpoint on some process
running on some host. Writing data onto this channel delivers it to the
application that is reading data on the other end of this channel.
The transport layer is responsible for implementing this abstraction.
Routers in the network are unaware of this concept since they only provide
network layer (machine-to-machine) services.</p>

<p>There are multiple transport protocols available on top of IP, including TCP,
UDP, and SCTP. TCP and UDP are by far the most popular of these.
Two responsibilities of the transport layer are multiplexing and demultiplexing
communication channels on the network and, in some cases, implementing
reliable data transfer.</p>

<p>Incidentally, a packet at the transport layer is called a <strong>segment</strong>; it
is called a <strong>datagram</strong> at the network layer and a <strong>frame</strong> at the datalink layer.
We send Ethernet frames, which contain datagrams that are routed by routers. These
datagrams, in turn, contain segments that the transport layer of the operating
system&#8217;s network stack processes.</p>

<h2 id="transportlayermultiplexinganddemultiplexing">Transport layer multiplexing and demultiplexing</h2>

<p><strong>Multiplexing</strong> and <strong>demultiplexing</strong> are the software mechanisms in place
to combine data from multiple logical communication channels on a machine into
a single stream of packets on the network
and then separate a stream of incoming datagrams into the appropriate
communication channels. This is important since communication on
multiple sockets shares the same network connection. We can have multiple
distinct streams at the transport layer that appear as a single stream of data
to the network layer.</p>

<p><strong>Multiplexing</strong> is the process of taking data from multiple
communication channels (sockets) and sending it out of the machine as
a stream of datagrams. <strong>Demultiplexing</strong> is the opposite process:
separating the incoming stream of datagrams into the individual messages for the
individual sockets to which each segment is targeted. </p>

<p>The key to IP transport layer multiplexing and demultiplexing is the
use of port numbers. Each transport layer segment contains source and destination
port numbers. A <strong>port number</strong> is a 16-bit number that has a unique association
to a socket (a communication endpoint) on each host.
Naming a socket, also known as <strong>binding</strong>, is the process of associating
a socket with a specific port number and address. The address is the local host&#8217;s
IP address, of course. In the case where a host has several network
interfaces, it will have that many IP addresses and it is possible to make the
socket available on only one of these interfaces. More commonly, though,
a special address, INADDR_ANY, is used to associate a socket with all available
network interfaces. Port numbers are usually specified explicitly for server
programs since clients will need to know where to contact them. For example,
an SMTP mail server will typically listen for client connections on TCP port 25.
A client, on the other hand, will generally not care what port it uses and
specifying port 0 is a request for the operating system to pick any available unused port number.</p>

<h2 id="udpmultiplexinganddemultiplexing">UDP multiplexing and demultiplexing</h2>

<p>UDP is an extremely lightweight transport layer protocol on top of IP.
Unlike TCP, it does not offer reliable message delivery and it does not guarantee
that messages will be received in the order that they were sent.</p>

<p>An incoming frame (e.g., ethernet packet) contains a protocol identifier
that identifies the payload (the data part of the frame) as IP data.
When that payload is passed to the
IP layer, a field in the header of the IP datagram identifies the
higher layer protocol as UDP. The UDP layer reads the destination port
field in the UDP header and delivers the segment to the socket that
is associated with that port number. The kernel maintains a hash table
of socket structures that is indexed by a key that is created from the
UDP destination port.
With UDP, any segments addressed to
a specific port number will be delivered to the socket that is identified
with that port. We will see that this is different from TCP, which takes
performs full demultiplexing based on the source <em>and</em> destination.</p>

<p>While UDP does not have the reliability and in-order delivery
advantages of TCP (or, as we shall see, rate adjustment to deal with congestion),
there are several reasons that make it attractive for certain applications:</p>

<ul>
<li><p><strong>Segments are sent immediately</strong>. When a user writes data to a UDP socket,
it immediately goes down the layers of the network stack and is transmitted
onto the network. TCP may wait for an acknowledgement or
for sufficient data in its transmit buffer instead of transmitting immediately.</p></li>
<li><p><strong>Message boundaries are preserved</strong>. TCP treats a communication stream
as a sequence of bytes. The number of writes to a socket does not necessarily
correspond to the number of messages that will be received at the other end.</p></li>
<li><p><strong>No connection setup overhead</strong>. With UDP, the first segment that is
sent on the network can contain application data. With TCP, we first need to
establish a connection with a three-way handshake, which requires an overhead of
sending a segment and receiving an acknowledgement before we can send a segment
with data.</p></li>
<li><p><strong>UDP is stateless</strong>. The kernel has to keep track of sockets, of course, but
does there is no need to keep track of sequence numbers, buffer for
out-of-order data, acknowledgements, etc. This uses less kernel memory and
makes error recovery and load balancing easier: requests can be redirected to
other hosts spontaneously.</p></li>
<li><p><strong>Smaller headers</strong>. UDP has an eight byte header compared to TCP&#8217;s 20-byte
header. This leads to smaller packets on the network.</p></li>
</ul>

<h2 id="udpheaderandchecksum">UDP header and checksum</h2>

<figure>
<img src="images/UDP-sm.png" alt="Figure 1. UDP header with IP pseudo header" id="udp_fig" title="UDP header with IP pseudo header" style="width:350px;" />
<figcaption>Figure 1. UDP header with IP pseudo header</figcaption></figure>



<p>The UDP header (Figure 1) is eight bytes long. It contains the source and destination
ports, segment length and a checksum. The <strong>checksum</strong> is a simple error-detecting code
that allows the UDP layer to check
for segment corruption. If the received segment contains an error, it is dropped and not
delivered to the socket. The checksum is computed over the UDP header, application
data, and a <strong>pseudo IP header</strong>. The pseudo IP header contains a subset of fields
from the IP header (source address, destination address, transport protocol ID,
and UDP segment length). It is included in the checksum computation to ensure
that the UDP layer will not get any misrouted segments (this is a safeguard but
the IP header has its own checksum, which is computed in the same way). </p>

<p>The checksum is a 16-bit value. If the data being summed does not contain
an even number of bytes (i.e., it does not have an integral multiple of 16-bit values),
it is padded with a zero byte. The same <strong>ones&#8217; complement</strong> algorithm is used to compute checksums
for IP headers, UDP headers, and TCP headers.
The value of the checksum field is set to zero during
the computation. To compute the checksum, all 16-bit chunks of data are added
together. Each time the addition of two numbers results in an overflow, a one is added to the
result.
Finally, the bits of the final result are inverted.</p>

<p>To validate the checksum, the receiver performs the same arithmetic,
generating a checksum for the segment and pseudo IP header. Since the
segment checksum is included in the header for this computation, the result for an
error-free packet will be all ones (0xffff). This computation reliably
detects single bit errors in the segment.</p>

<h2 id="tcpmultiplexinganddemultiplexing">TCP multiplexing and demultiplexing</h2>

<p>UDP offers only limited demultiplexing. Segments from multiple sockets
(sources) that are directed to the same host address and port number
are received by the socket on that host that is associated with
that port number.</p>

<p>With TCP, a connected socket is associated is associated with four values:</p>

<ol>
<li>the sender&#8217;s source address</li>
<li>recipient&#8217;s (destination) address</li>
<li>source port</li>
<li>destination port</li>
</ol>

<p>Recall that with TCP sockets, a server first creates a socket
whose sole purpose is listening for and accepting incoming
connections. It does so with the <strong>listen</strong> system call.
This socket is said to be in the LISTEN state.
It will never be used for data transfer. Its only purpose is
to accept incoming connections.</p>

<p>When an incoming TCP connection request arrives at the host, the
kernel searches for a socket in the LISTEN state where the packet&#8217;s
destination address and port match those of the socket (the address
can be &#8220;any&#8221; - a wildcard). The kernel then creates a <em>new</em> socket.
The remote address and port are copied from the TCP segment header onto the
new socket structure.
Once the connection is set up, this new socket is in the ESTABLISHED state,
indicating to the kernel that it has a connection to another
socket. Any incoming TCP data segment will go to the socket that
is associated with the source and destination addresses and ports in the
segment header.</p>

<h2 id="principlesofreliabledatatransfer">Principles of reliable data transfer</h2>

<p>Given that the underlying IP network does not guarantee packet
delivery, if a transport layer protocol wants to provide
reliable data delivery, it has to implement it
via software. We will first look at evolving
<strong>reliable data transfer</strong> (<strong>RDT</strong>) software in general before
turning our attention to how it is implemented in TCP specifically.</p>

<p>If the underlying networking layer was indeed reliable,
there would, of course, be no need for RDT. A sender would
send a segment and a receiver would receive it and immediately
<strong>deliver</strong> it to the application. </p>

<h3 id="stop-and-waitprotocol">Stop-and-wait protocol</h3>

<p>Let us now assume that <em>all</em> segments are received (this will
not be a valid assumption in the real world of IP) but that
some of them might have corrupted data. In this case, we may
need to request retransmission of a segment because it
arrived with errors. <strong>Automatic Repeat Request</strong> (<strong>ARQ</strong>)
refers to a family of protocols that acknowledge
received packets and request <strong>retransmission</strong> for bad packets. </p>

<p>An <strong>acknowledgement</strong> (<strong>ACK</strong>), also known as a <strong>positive acknowledgement</strong>,
is a <strong>receiver feedback</strong> message that confirms the successful receipt of a message. A
<strong>negative acknowledgement</strong> (<strong>NAK</strong>) is a feedback message
that tells the sender that a message was <em>not</em> successfully received.</p>

<p>A simple protocol for providing RDT over a channel that always
delivers segments but may introduce errors into them is to transmit
one segment and wait for an ACK or NAK. A receiver sends an ACK if
the segment was received without errors. Upon receipt of the ACK,
the sender can transmit the next segment. A receiver sends a NAK
if the segment was received with errors. Upon receipt of a NAK,
the sender <strong>retransmits</strong> the same segment and again waits for an ACK or NAK.
This form of ARQ protocol, where a segment will not be sent until the
previously sent segment has been acknowledged, is called a <strong>stop-and-wait</strong> protocol.</p>

<p>The protocol we just outlined fails in that it recognizes that data
can be corrupted in transit but does not take that possible corruption into account for
ACK/NAK messages. We can modify the protocol by adding a checksum for ACK/NAK
segments and, upon receipt, detect if those segments are corrupted. If corrupted,
we will treat the message as a NAK and retransmit the segment. This can
result in the receiver getting <strong>duplicate packets</strong>. If a receiver
gets a duplicate packet, it will need to ignore the data but still send an ACK in return.</p>

<p>Now we need to distinguish
new data from a retransmission. A <strong>sequence number</strong> allows us to do that.
In the case of a stop-and-wait protocol, a one-bit sequence number suffices since
we only need to distinguish between the current packet we&#8217;re waiting for
and the retransmission of a correctly-received previous packet. This stop-and-wait
protocol using a single-bit sequence number is called an <strong>alternating bit protocol</strong>.</p>

<h3 id="removingnaks">Removing NAKs</h3>

<p>We just saw two cases where a recipient gets a packet that it does not want:
receipt of a duplicate packet (in which case it sends an ACK) and receipt
of a corrupted packet (in which case it sends a NAK and awaits retransmission).
We can remove the need for a NAK by using an ACK and adding a sequence number to it.
The ACK will acknowledge the last packet that was correctly received. If
the sender receives an ACK for a different number than the packet it
most recently sent, it will treat that as a NAK and retransmit the packet.</p>

<h3 id="rdtoveralossychannel">RDT over a lossy channel</h3>

<p>So far, we only considered the case where packet data might be corrupted but
the packets were always delivered to their destination. Now we need to account
for the fact that packets may be lost. This can be due to overflow of a queue
at a router or to data corruption in the packet header that prevents the packet from being
routed to its destination.</p>

<p>We place the burden of detecting a lost packet on the sender. The sender will
not get an acknowledgement from a receiver in the case that a packet was never
delivered or in the case that it was delivered but the acknowledgement message was lost.
To detect this lack of acknowledgement, the sender will use a <strong>countdown timer</strong>.
The timer is initialized when the packet is sent. If it times out before
an acknowledgement is received, the sender retransmits the packet and
reinitializes the timer. The timer should be set to some value that is longer
than the average round-trip time so that the timeout will indicate a likely loss. Setting the timer
to a too-short value will result in excess duplicate packets. Although the
protocol can deal with them, we&#8217;d like to avoid an excessive amount of unnecessary retransmissions.</p>

<h3 id="pipelining">Pipelining</h3>

<p>A stop-and-wait protocol will not transmit a packet until the previous
packet has been successfully sent and acknowledged. Having to wait a
round-trip delay before sending the next packet yields horrible network
utilization, often far less than 1%.
Recall that <strong>network utilization</strong> is the ratio of the
actual traffic on the network to the traffic that the network can
support. </p>

<p>A way to improve network utilization dramatically is to send successive packets without
first waiting for acknowledgements of earlier packets. This technique is
called <strong>pipelining</strong> and we will look at two approaches to pipelining:
<em>Go-Back-N</em> and <em>Selective Repeat</em>. In order to send multiple packets
without first waiting for an acknowledgement from each one, we will
need to increase the range of sequence numbers so that we can identify
packets and match an acknowledgement to a specific transmitted packet.
We also need to save packets on the transmitter until they have been
acknowledged by the receiver in case we need to re-send them. If the receiver gets
out-of-sequence packets, it cannot deliver them to the application and
may need to consider storing them in a receive buffer or else requesting
those same packets from the sender again in the future.</p>

<h3 id="go-back-ngbnprotocol">Go-Back-N (GBN) Protocol</h3>

<figure>
<img src="images/GBN-sm.png" alt="Figure 2. Go-Back-N sliding window" id="gbn_fig" title="Go-Back-N sliding window" style="width:400px;" />
<figcaption>Figure 2. Go-Back-N sliding window</figcaption></figure>



<p>The Go-Back-N protocol allows a sender to send multiple packets
without waiting for an acknowledgement. Each successive packet
has a monotonically increasing sequence number.
A <strong>window size</strong> defines
the maximum number of packets that could be transmitted before
waiting for acknowledgements. The <strong>base</strong> of the window is
the earliest packet that has been sent but not yet acknowledged.
When an acknowledgement is received for a sequence number that
corresponds to that packet, it can be discarded (the sender
will never need to retransmit it) and the window advances,
or <strong>slides</strong> to the next unacknowledged packet and the new
packet that entered the window can now be transmitted.
This is why Go-Back-N is called a <strong>sliding window protocol</strong>.</p>

<p>The sender sends all packets that fall within the current
window and starts a timer.
The receiver expects to receive packets
in the correct sequence number order but it may not get that. Whenever
a packet is received correctly and is in the proper sequence,
that packet is acknowledged with its sequence number and delivered
to the application. The expected sequence number is incremented
and the receiver waits for the next packet.</p>

<p>If the receiver gets a packet that has a different sequence number,
it discards the packet and
sends back a duplicate acknowledgement (that is, the
acknowledgement for the previous sequence number it received).
An acknowledgement number <em>n</em>
indicates that the receiver has correctly received all packets up to
and including packet <em>n</em>.
This form of acknowledgement is called a <strong>cumulative acknowledgement</strong>.
The receiver only needs to keep track of the next sequence number it needs and
only stores one packet at a time.</p>

<p>When the sender receives an acknowledgement <em>n</em>, it advances
the base of its window to <em>n</em>. Packets less than or equal
to <em>n</em> can be discarded.
Note that there is no harm in losing acknowledgements less than
<em>n</em>; this acknowledgement indicates that <em>all</em> prior packets were
received as well.
If the acknowledgement number corresponds to the last packet that was
sent, the sender has all outstanding acknowledgements and can
stop the timer. Otherwise, the timer is restarted to wait for
additional acknowledgments. If the timer expires, that means that
some all transmitted packets have not been acknowledged; one
or more packets have been lost (or the final acknowledgement
has been lost). Upon timer expiration, the sender sends
<em>all</em> packets that are in the current window. </p>

<h3 id="selectiverepeatsrprotocol">Selective Repeat (SR) Protocol</h3>

<p>With the Go-Back-N protocol, many packets can be in the
pipeline: sent but not yet acknowledged. A single error
in one of these packets will result in a timeout at the sender
and hence a retransmission of <em>all</em> packets in the sender&#8217;s
window. This can result in a lot of unnecessary retransmissions
since the receiver will be getting packets that it has previously
received correctly but discarded.</p>

<p><strong>Selective Repeat</strong> (<strong>SR</strong>).
Selective repeat, like Go-Back-N, is a sliding window protocol but
allows the receiver to store and acknowledge out-of-order
packets so that they do not need to be retransmitted.</p>

<p>Instead of cumulative acknowledgements, the receiver sends an
acknowledgement for the specific packet that was received.
The sender&#8217;s window slides when the earliest packet in the
window is acknowledged and always starts at the first
unacknowledged packet. The window itself may contain a mix
of acknowledged and unacknowledged packets.</p>

<p>The receiver must also maintain a window since it may receive
packets out of sequence and needs to buffer them. Whenever
a packet is received in sequence, it can be delivered to the
application. The receive window slides to the slot for the
first non-received packet. </p>

<p>Every transmitted packet has a separate timer associated with it.
If an acknowledgement is not received successfully within that
time, that specific packet is retransmitted. The receiver will
acknowledge the packet if it fits in the window or if it
is sequenced before the window. This latter case means that the
packet is a duplicate of a packet that was already received
and delivered to the application. If the packet is beyond
the receiver&#8217;s window, it has no room to accept the packet
and will ignore it.</p>

<h1 id="transportlayer:tcp">Transport Layer: TCP</h1>

<p><strong>TCP</strong>, the <strong>Transmission Control Protocol</strong>, is the dominant transport protocol on the Internet.
Where UDP was a thin layer over IP that provided us with multiplexing and a limited
demultiplexing service (the source host was not factored into the demultiplexing - that&#8217;s up
the the application to process),
TCP provides applications with a <em>reliable</em>, <em>bidirectional</em> communication channel.
In addition, TCP attempts to be a good network citizen and manage the <strong>flow control</strong> of
data to ensure that the receiver&#8217;s buffer does not overflow and to avoid network <strong>congestion</strong>.</p>

<p>TCP is a <strong>connection-oriented</strong> protocol. This does <em>not</em> mean that a virtual circuit
is established in the network. Indeed, routers are not aware of TCP any more than they are
of UDP or other transport layer protocols.
TCP is a form of protocol design called <strong>end-to-end control</strong>, where only the endpoints
are responsible for the integrity of the connection. Routers do not
guarantee not to drop packets, ensure the are sequenced correctly, or correct errors.
The &#8220;connection&#8221; is managed in software at both
end systems. Because the hosts need to keep track of the state of the communication channel,
TCP has an initial <strong>connection setup</strong> step that comprises a three-way handshake where parameters
such as port numbers, sequence numbers, and buffers are established. Once the connection
is established, all messages are acknowledged and retransmitted if lost. When the session is
complete, TCP enters a <strong>teardown</strong> phase to ensure both sides are informed that the connection
is no longer needed and that they can free up resources that were used for the connection.</p>

<p>TCP&#8217;s communication is <strong>full duplex</strong>. This means that if a process <em>A</em> established a TCP
connection to process <em>B</em> then process <em>B</em> can use the same connection to send data to
process <em>A</em>. </p>

<h2 id="tcpsegments">TCP Segments</h2>

<p>TCP sends <strong>segments</strong> between the sender and receiver. A <strong>segment</strong> is simply the
transport layer term for a packet. The sending process writes data to a socket. This
data is copied to the operating system kernel and ends up in TCP&#8217;s <strong>send buffer</strong>,
a pool of memory devoted to that specific connection. TCP reads chunks of data from this
send buffer, creates TCP segments, and sends them down to the IP layer for transmission
onto the network (the IP layer will, in turn, send the IP datagram to the data link layer,
which will actually get it to the network). When an IP datagram arrives at the
destination machine, a <strong>protocol</strong> field in the IP header identifies the message as a TCP
message and IP forwards it up to the TCP driver. TCP then examines the source address,
source port, destination address, and destination port for find the appropriate socket
for this segment. The data is placed in that socket&#8217;s <strong>receive buffer</strong>, the counterpart
to the send buffer that is a pool of memory used to hold received data that did not
yet make it up to the application.</p>

<p>Note that, unlike in UDP, TCP views the data coming from the application as
a stream of bytes rather than individual messages that need to be sent out.
There is no assurance that writing, say, 20 bytes to a socket will result in
20 bytes of data being transmitted in a TCP segment. If there was other
data in the send buffer that was ready to transmit, it could be combined with
this new data. Similarly, the 20 bytes may not be transmitted immediately but
combined with additional data that is written onto that socket.</p>

<h3 id="maximumsegmentsize">Maximum Segment Size</h3>

<p>When the TCP driver reads bytes from data in the send buffer to create outgoing segments, it makes
sure that the number of bytes is grabs is
less than the <strong>maximum segment size</strong> (<strong>MSS</strong>) to make sure it does not try to send a segment
larger than the underlying network can transmit.
Data link layers have a <strong>Maximum Transmission Unit</strong> (<strong>MTU</strong>), the largest payload
that they can carry. For a TCP segment to fit into a data link frame, the IP header,
TCP header, and application data must be no larger than the MTU.
Ethernet supports an MTU of 1500 bytes (with support for an MTU of 9,000 bytes for
jumbo frames in gigabit ethernet) while 802.11 (Wi-Fi) supports a 7981-byte MTU.
Since IP and TCP headers are each 20 bytes long, the MSS is typically the MTU minus 40 bytes.
Hence, a common MSS on an ethernet network is 1460 bytes (1500&#8211;40).</p>

<h3 id="pathmtudiscovery">Path MTU Discovery</h3>

<p>It is easy enough for the TCP layer to find out the MTU of the local link layer, subtract 40,
and compute a value for the MSS. While this is fine for communication within the LAN, this
does not ensure that some link that the packet traverses to the destination will not have
a smaller MTU, resulting in the need to fragment the packet (which is undesirable).
The <strong>path MTU</strong> is the minimum MTU of all the hops along the path the destination.
Unfortunately, there is no foolproof way of determining this value.
IP networks are required to support an MTU of 576 bytes (512 bytes of data plus up to 64 bytes for headers).
However, most network links can support larger MTUs (for example, and MTU of 1500 bytes works on
practically all routers in the Internet but support is not guaranteed).</p>

<p><strong>Path MTU Discovery</strong>
(<a href="http://www.ietf.org/rfc/rfc1181.txt">RFC 1181</a> and
<a href="http://www.ietf.org/rfc/rfc1191.txt">RFC 1191</a>) defines a method for a host to discover
the path MTU and hence set its MSS to the maximum possible value.</p>

<p>It works by initially assuming that the path MTU is the MTU of the first hop
(this is local to the host and easy to find). All initial datagrams are
sent to the destination with the <strong>&#8220;don&#8217;t fragment&#8221;</strong> (DF) bit
set in the IP header. If a router needs to route the datagram
to a link with a smaller MTU, the router will discard the datagram
and send back an ICMP<a href="#fn:1" id="fnref:1" title="see footnote" class="footnote">[1]</a> datagram containing an
<em>ICMP Destination Unreachable</em> message with a code indicating
<em>fragmentation needed</em>.
The MTU of the outbound link is placed in the ICMP message.
Upon getting this response, the sending host reduces its MTU to the returned value and
tries again. Since routes may change periodically, the Path MTU process is repeated
periodically (every 10 minutes on Linux and Windows systems by default).</p>

<h2 id="tcpstructure">TCP structure</h2>

<figure>
<img src="images/TCP-header-sm.png" alt="TCP header" id="tcpheader" title="TCP header" style="width:350px;" />
<figcaption>TCP header</figcaption></figure>



<p>A TCP segment comprises at least 20 bytes of a TCP header followed by a variable number of
bytes of application data. The TCP header is prefixed by an IP header that, in turn,
is encapsulated in a link layer header.</p>

<p>Some of the key parts of the TCP header are:</p>
<dl>
<dt>Source port number</dt>
<dd>Identifies the port number of the sender&#8217;s socket.</dd>

<dt>Destination port number</dt>
<dd>Identifies the port number of the receiver&#8217;s socket.</dd>

<dt>Checksum</dt>
<dd>Like UDP, this is a 16-bit 1s complement sum of the TCP header, application data,
and <strong>IP pseudo header</strong>. The IP pseudo header is a subset of the fields in the
IP header: source address, destination address, protocol ID, and the length of the
TCP header and data. All the data is added in 16-bit chunks. Any overflow carries result
in a 1 added to the result. The final result is complemented (bits inverted).
When the receiver performs the same operation and includes the checksum field,
the result for an uncorrupted packet is all 1s (0xffff).</dd>

<dt>Sequence number</dt>
<dd>Every byte that is transmitted is counted starting from some <strong>initial sequence number</strong>.
The 32-bit sequence number of the TCP segment is the number of the first byte in the data.
The sequence number is an essential part of TCP&#8217;s reliable data transfer service.</dd>

<dt>Acknowledgement (ACK) number</dt>
<dd>A receiver sends back a 32-bit acknowledgement number that is the sequence number
of the next byte that it expects to receive. This too is an
essential part of TCP&#8217;s reliable data transfer service.</dd>

<dt>Receive window</dt>
<dd>The receiver tells the sender the maximum number of bytes that it is capable of
receiving. This 16-bit value takes priority over the MSS (maximum segment size).</dd>

<dt>Header length</dt>
<dd>The header length indicates the number of 32-bit words in the TCP header. Note
that the IP header contains the total datagram length, which can be used to determine
how much data is in the message.
The basic TCP header is 20 bytes and the minimum value of the header length is hence 5 (20 bytes =
5 32-bit words). TCP supports optional data in its header and this may make the header
larger.</dd>

<dt>TCP Options</dt>
<dd>If the header length is greater than 5, that means there are more than 20 bytes in the header
and TCP options are present. TCP options are located at the end of the normal 20-byte
header.
While approximately 32 different options are defined, some are obsolete and many
are never used. The most commonly used options include:</dd>
</dl>


<ul>
<li><strong>Maximum Segment Size</strong>: defines the maximum segment size that will be used during
a connection between two hosts.</li>
<li><strong>Window Scaling</strong>: extends the number of bits for the receive window to allow the TCP
window size to be specified as a 30-bit number instead of a 16-bit number.</li>
<li><strong>Selective Acknowledgements</strong>: allow the receiver to inform the sender if it received
any out-of-order segments.</li>
<li><strong>Timestamps</strong>: an extension to allow more accurate mechanism to measure
segment delivery time, including retransmissions.</li>
</ul>
<dl>
<dt>Flags</dt>
<dd>A number of 1-bit flags are present in the header. These are:</dd>
</dl>


<ul>
<li><p><strong>ACK</strong>: informs the recipient of the segment that the acknowledgement field contains a valid sequence number.</p></li>
<li><p><strong>RST</strong>, <strong>SYN</strong>, <strong>FIN</strong>: These are used to set up and tear down the TCP connection.
The SYN (&#8220;Synchronize&#8221;) flag is used in the handshake used to set up a connection.
The FIN (&#8220;Finish&#8221;) flag is used to close a connection.
The RST (&#8220;Reset&#8221;) flag indicates that a segment was received for a closed or nonexistent socket.</p></li>
<li><p>PSH (&#8220;Push&#8221;): Tells the receiver to pass the received data to the application layer immediately.
This flag is not used in practice.</p></li>
<li><p>URG (&#8220;Urgent&#8221;): Tells the receiver that the application data contains a region of &#8220;urgent&#8221;
data, possibly along with &#8220;non-urgent&#8221; data. The 16-bit <strong>urgent data pointer</strong> is an index to the
last byte of this data. As with PSH, the concept of urgent data is not used.</p></li>
<li><p>NS (&#8220;Nonce Sum&#8221;), CWR (&#8220;Congestion Window Reduced&#8221;), and ECE (&#8220;Explicit Congestion Expected&#8221;)
are all part of an Explicit Congestion Notification protocol, which is an extension to IP.
Not all routers support this and we will not cover this extension.</p></li>
</ul>

<h2 id="tcpsequencenumbers">TCP Sequence Numbers</h2>

<figure>
<img src="images/TCP-seq-sm.png" alt="TCP sequence numbers" id="tcpsequencenumbers" title="TCP sequence numbers" style="width:250px;" />
<figcaption>TCP sequence numbers</figcaption></figure>



<p>TCP views application data as a sequence of bytes and each byte in the sequence
is assigned a <strong>sequence number</strong>. The sequence number in each TCP segment
is the sequence number of the first byte in that segment.
For example, if the current sequence number is 500 and we transmit
a segment containing 1460 bytes, the segment will contain a sequence number
of 500. If the following segment contains 1000 bytes, the sequence
number of the segment will be 1960, which is 500 (the last sequence number)
plus 1460 (the number of bytes that was sent with the last segment). Sequence
numbers are 32-bit values and do not have to start with 0. The value may
wrap around the 32-bit boundary, so all sequencing is done modulo 2<sup>32</sup>
(mod 4,294,967,296).</p>

<h2 id="tcpacknowledgementnumbers">TCP Acknowledgement numbers</h2>

<p>Received TCP segments are acknowledged by the receiver. TCP uses <strong>pipelining</strong>
and permits several segments to be sent at once prior to waiting for
an acknowledgement (more on this later).</p>

<p>An <strong>acknowledgement number</strong> is present in a received segment and is a
32-bit number that indicates the sequence number of the next byte that
the remote host is expecting to receive next. For example,
in the above example we sent 1460 bytes with a sequence number of 500.
Upon receipt of this segment, the return segment will contain an acknowledgement
number of 1960, the sequence number of the next byte that the receiver needs.
The message it just received contained bytes numbered 500 through 1959.
When the receiver receives the next segment, 1000 bytes with a sequence number
of 1960, it will return an acknowledgement number of 2960.</p>

<p>It would be a waste of network resources to send back a TCP segment containing
nothing but an acknowledgement number. While this is inevitable in some cases,
if the receiver happens to have data to transmit back to the sender, the acknowledgement
number is simply set in the TCP header of the transmitted segment, completely
avoiding the need to send a separate acknowledgement. Using an outgoing
data segment to transmit an acknowledgement is known as a <strong>piggybacked acknowledgement</strong>.</p>

<p>TCP uses <strong>cumulative acknowledgements</strong>.
The acknowledgement number that a receiver sends inside a TCP header
is always the sequence number that the receiver
wants to see next. Going back to the earlier example, if a receiver receives
1460 bytes with a sequence number of 500, it sends back an acknowledgement for
the next byte it wants: 1960. Suppose that the next segment it receives
contains 500 bytes with the sequence number 2960. This means that the desired segment was either lost
or will be arriving out of sequence. Upon receiving the segment with sequence
number 2960, the receiver generates an acknowledgement (all segments get acknowledged)
but the acknowledgement
number is the number of the earliest sequence it does not yet have: 1960. Hence,
the sender will get back <strong>duplicate acknowledgements</strong> for 1960. </p>

<p>To avoid sending too many data-free acknowledgement segments, a receiver is allowed to
wait up to 500 ms (half a second) before sending an acknowledgement. If another
segment comes in during that interval, a cumulative acknowledgement must be sent.
For a steady stream of messages, therefore, cumulative ACKs need to be sent for every other
packet. However, any out-of-order segments must be acknowledged upon receipt.</p>

<p>A receiver does not have to store segments that were received out of sequence but
it is more efficient to do so and practically every implementation of the TCP
protocol does this. Continuing our example, suppose that the receiver
does get the segment containing sequence number 1960 after receiving
the segment withe the sequence number 2960; the segment really did arrive out of
order and was not dropped.
Now the receiver can fill in its hole in the receive buffer. It just
got bytes 1960&#8230;2959 and it already had bytes 2960&#8230;3459 from the earlier
receipt of the segment with sequence number 2060. The acknowledgement it sends
now will be the cumulative acknowledgement &#8211; the sequence number of the next
byte it needs: 3460.
The sender will see acknowledgements of 1960, 1960, and 3460.
Had the transmitted segments arrived in order, the acknowledgements
would have been 1960, 2960, and 3460.</p>

<h2 id="tcpconnectionsetupandteardown">TCP Connection setup and teardown</h2>

<h3 id="connectionsetup">Connection setup</h3>

<p>TCP employs a <strong>three-way handshake</strong> to set up a connection: a process of SYN, SYN-ACK, and ACK.
The client initiates the process; it creates a random initial sequence number (client_isn) and
sends it to the server in a TCP segment with the <strong>SYN</strong> flag set.
The server receives this and allocates send and receiver buffers as well
as variables. This set of data, containing all the information about a connection,
is called the <strong>transmission control block</strong> (<strong>TCB</strong>). The server then
creates a <strong>SYN-ACK</strong> segment that acknowledges the
received sequence number and contains a random sequence number from
the receiver. This segment also has the SYN bit set.
Upon receiving this, the client acknowledges the server&#8217;s sequence
number by sending the final <strong>ACK</strong> segment to the server and
allocates its own TCP buffers and variables for the connection.</p>

<h3 id="synflooding">SYN flooding</h3>

<p>Kernel memory is finite and the operating system will not allocate an unlimited amount
of memory for managing TCP connections. A <strong>denial-of-service attack</strong> called
<strong>SYN flooding</strong> sends a large number of SYN segments to a machine but
usually uses an unreachable return address to
never complete the handshake to set up a legitimate connection. The
recipient normally allocates memory for each SYN segment that it receives, expecting
each to become a legitimate connection. Eventually, kernel memory is exhausted
and the operating system will not allow any more incoming TCP connection requests,
including, of course, legitimate ones. The operating system will continue
to refuse incoming connections until those incomplete ones time out. The connection
setup timeout
is an administrator-configured value and can range from half a minute to several
minutes.</p>

<p>Several approaches have been proposed to deal with SYN flooding.
One of these is <strong>SYN cookies</strong>.
The key realization is that the kernel allocates memory (the TCB)
before the connection is fully set up. With the technique of SYN cookies,
<strong>no state is saved</strong> (no memory allocated) upon the receipt of a connection
request. Instead, any needed information is encoded into
the initial sequence number. Since that sequence number (+1) will be
sent back in the final ACK from the client, the server will be able
to validate that the ACK, and hence the requesting client, is legitimate.
The initial sequence number that the server creates is a hash of
the source and destination IP addresses, ports, and some secret value
known only to the server. A client will not be able to spoof this value
since it does not know the secret but a server can easily validate it
from the acknowledgement number in the ACK message from a legitimate client.</p>

<h3 id="mssannouncement">MSS announcement</h3>

<p>TCP provides an option during connection setup to tell the other side
its maximum segment size (MSS); that is, the largest size segment that
it is willing to accept.
If both machines are on the same LAN, the MSS is likely to be the MTU of the
network interface minus the size of the protocol headers (20 bytes for IP and 20
more bytes for TCP), although it can differ even here if, for example, one
device supports jumbo (9000-byte) Ethernet packets and the other does not.
The Internet requirement is that all IP routers support an MSS of at least 536
bytes.</p>

<h3 id="invalidmessages">Invalid messages</h3>

<p>If a host receives a TCP segment where the port numbers or source address
to not match any connection (e.g., the socket is closed or there is
no listener on that address), it will send back a <strong>reset segment</strong>, a
TCP segment with the <strong>RST</strong> flag set. In the case of UDP, an attempt
to send a message to a port that does not have any process listening
on it will result in the generation of an ICMP message back to the sender.</p>

<h3 id="connectionteardown">Connection teardown</h3>

<p>Either the sender or the receiver can decide to terminate a TCP connection.
Termination involves telling the other side that you are finished sending
data, getting an acknowledgement, and then freeing allocated resources for
that connection.</p>

<p>This is a two step process between the two hosts (let&#8217;s call them A and B).
The the host that initiates the teardown, host A,
sends a <em>finished</em> message: a TCP segment with the <strong>FIN</strong> flag set. It
then enters the FIN_WAIT_1 state and waits for an acknowledgement from host B.
This FIN message is a promise that host A will not send any more data on the connection.
Host B sends the acknowledgement and enters the CLOSE_WAIT state. It may
still have data to send, however. Upon receiving the acknowledgement, host A enters the
FIN_WAIT_2 state and waits for host B to terminate its side of the connection.
Host B does the same thing that host A did: once it has no more data to send, it sends
a TCP segment with the
<strong>FIN</strong> flag set and enters the LAST_ACK state, which means it is waiting for the
final acknowledgement from host A. When host A receives the FIN message,
it knows that it will receive no more messages from host B. It sends the final
ACK to host B and enters the TIME_WAIT state, which is a timeout period to ensure
that host B receives the final ACK and there are no stray packets for that connection in
the network.</p>

<h2 id="timeouts">Timeouts</h2>

<p>Since TCP does not know about the congestion or performance of the underlying network and
the network does not provide any guarantees for packet delivery, TCP relies on a time limit
to determine if any transmitted segments were lost. If an acknowledgement is not
received within a specific time limit then the sender may assume that the segment
was lost. Since IP provides no guarantees on the timeliness of delivery, this
<strong>retransmission timeout</strong> (<strong>RTO</strong>)
can only be a best guess. We need a value that is long enough to avoid excessive
timeouts and retransmissions but is short enough to avoid excessive waits. </p>

<p>TCP keeps track of the average round-trip time of segments. Since these may fluctuate
over time, TCP uses an <strong>exponentially weighted moving average</strong> (<strong>EWMA</strong>), which places
approximately 10&#8211;20% of the weight on the most recent RTT measurement.
This average of the RTT is called the <strong>smoothed round trip time</strong>, or <strong>SRTT</strong>.</p>

<p>The second thing that TCP keeps track of is how much the recently measured
round-trip time deviated from the SRTT. This too is is an EWMA function,
placing approximately 25% of the weight on the most recent deviation.
The average delay is called the <strong>round-trip time variation</strong>, or <strong>RTTVAR</strong>.
It is an estimate of how much the RTT typically deviates from the average RTT
and is an approximation to the standard deviation, which would be slower
to compute.</p>

<p>The retransmission timeout (<strong>RTO</strong>) value is set to a function of the SRTT and RTTVAR:</p>

<pre><code>timeout interval = SRTT + 4 * RTTVAR
</code></pre>

<p>However, whenever a timeout occurs, this value is doubled. The timeout value
is doubled for each timed-out retransmission up to a value of 64 seconds. This doubling
of the timeout is called an <strong>exponential backoff</strong>. Whenever an acknowledgement is received
without a timeout, the timeout interval is reset to its base value.</p>

<h2 id="tcpreliabledatatransfer">TCP Reliable Data Transfer</h2>

<p>When an application writes data to a socket, the data is copied over to the <strong>send buffer</strong>
for that socket&#8217;s TCP connection. Some time later (the time is not specified in the standards),
TCP will grab data sitting in the send buffer and create and transmit one or more TCP
segments, each with the appropriate sequence number.
The first of these segments will cause the <strong>RTO timer</strong> to be set. Whenever
any segment containing an acknowledgement from the receiver arrives, the timer is reset.
If the acknowledgement number is greater than the sequence number of the base of the
send buffer, the sender can remove all bytes prior to that of the acknowledgement number
from the send buffer since it knows that the receiver has them. If a timeout
occurs, TCP will retransmit only one segment, the non-acknowledged segment with the smallest
sequence number, double the timeout interval, and restart the timer.</p>

<p>Let us consider a few cases to illustrate how this works.</p>

<h3 id="lostack">Lost ACK</h3>

<p>If an acknowledgement is lost and the sender times out (meaning that there were no
additional segments sent), the sender will retransmit that segment. The receiver,
upon getting it, will see that it does not need that sequence number (it is a
duplicate segment) and will discard the segment but will send an acknowledgement back to the sender.</p>

<p>Another case is when the sender sent two segments but the acknowledgement to the first one
was lost. Because acknowledgements are cumulative and the sender gets the second, cumulative
acknowledgement, it knows that the sender has received all the bytes and a retransmission is
not necessary.</p>

<h3 id="delayedacks">Delayed ACKs</h3>

<p>If the network round-trip time suddenly became much longer than expected, acknowledgements
to a sequence of segments might arrive after a timeout. When the timeout occurs, the
sender will retransmit only the earliest unacknowledged segment and restart the timer.
Suppose now that acknowledgements for the previously transmitted segments arrive.
TCP will process them and adjust the base of its send buffer if those bytes are no longer
needed. When the receiver gets the duplicate packet, it will discard it but send an
acknowledgement. Some time later, the sender will receive this acknowledgement but
see that it is a duplicate and hence discard it.</p>

<h3 id="tcpfastretransmit">TCP Fast Retransmit</h3>

<p>As we have seen, TCP uses pipelining and can send multiple segments before waiting
for acknowledgements. If a receiver detects a missing sequence number, it means one of
two things: a segment was lost (either discarded due to queue overflows or due to
data corruption) or that a segment is delivered out of sequence. TCP does not use
negative acknowledgements but sends an acknowledgement for every received out-of-sequence
segment with the sequence number of the next byte it needs. In the case where a segment has
indeed been lost, every segment after that will be acknowledged with the same sequence
number. Hence, the receiver will see a stream of duplicate acknowledgements.</p>

<p>If TCP receives three segments with duplicate
acknowledgement numbers, it assumes a segment was lost (the assumption is that it is
unlikely that one segment was routed or delayed such that three others arrived first).
Instead of waiting for an RTO to occur, TCP will transmit that missing segment
(the one with the sequence number of the ACK). This technique is
called a <strong>fast retransmit</strong> because the protocol retransmits the segment
without waiting for the RTO to occur.</p>

<h3 id="selectiveacknowledgements">Selective Acknowledgements</h3>

<p>TCP behaves in a similar manner, but not quite the same, as the Go-Back-N protocol.
The sender only keeps track of the
earliest sequence number that has been transmitted but not acknowledged. A distinction
is that, in the case of a timeout, Go-Back-N will retransmit <em>all</em> segments in its
window while TCP will only retransmit the lowest (earliest) segment. </p>

<p>With this behavior, having the receiver store out-of-order segments is
optional. The receiver may store them. If it does, and the receipt of
a segment fills in a hole (missing segment), then TCP will send back
a cumulative ACK, ensuring that the sender will not have to retransmit those
segments. However, the sender does not know if this will happen and has
to hold on to all unacknowledged segments in its send buffer just
in case any or all of those segments have to be resent.</p>

<p>An optional enhancement to the TCP protocol uses the <em>options</em> field in the
header to allow the receiver to send over a list of {start byte, end byte}
values to identify the specific range of bytes that have been received.
These are called <strong>Selective Acknowledgements</strong> and make the protocol behave like
a Selective Repeat protocol.
The acknowledgement number in the TCP header is unchanged but the list of
received byte ranges allows the sender to
remove those enumerated segments from its send buffer since it knows that
they will never have to be retransmitted. </p>

<h2 id="receiver-regulatedflowcontrol">Receiver-regulated flow control</h2>

<p>It would be pointless to keep sending data to the receiver if the
receiver&#8217;s application isn&#8217;t keeping up with reading it.
In addition to slowing down transmission due to congestion (which
we will look at next), TCP allows the receiver to tell the sender
to tell it how much space it has in the receive window (the free
space in the receive buffer).
It does this by placing the size of the receive window into
the <strong>receive window</strong> field in the TCP header in each segment
that is sent to the sender.</p>

<p>One problem that can arise is if the receive window size is zero,
the sender would stop transmitting and not get feedback from
the receiver once the window size became bigger when the application
consumed some data. The feedback is absent because the receiver
already sent acknowledgements for all received segments.
To remedy this, the sender uses <strong>probing</strong>:
if the receive window is zero, the sender will periodically send
a message with one byte of data. The receiver does not have to
accept this data if the window size is truly zero (the sender will
retransmit) but this gives the receiver a chance to send an
acknowledgement segment with a non-zero receive window once
there is room in the buffer.</p>

<h2 id="tcpcongestioncontrol">TCP Congestion Control</h2>

<p>TCP tries to be a good network citizen and decrease the rate at which a sender sends
traffic based on its perception of <strong>congestion</strong> in the network.
This is in addition to TCP&#8217;s flow control where a recipient can tell
a sender to send less data by giving it a smaller window.</p>

<p>TCP uses a <strong>sliding window</strong> approach to flow control.
The way that TCP controls the rate at which data is sent on a network is
by adjusting the size of its sending window. Recall that a window size
regulates the number of bytes that can be sent before waiting for acknowledgements
from the recipient. We have seen that a window size of one MSS gives us a
stop-and-wait protocol where we can send out only one segment before waiting
for an acknowledgement for the receipt of that segment. In general, the
transmission rate is the <strong>window size divided by the round-trip time</strong>.
TCP&#8217;s use of the sliding window algorithm makes the protocol <strong>self-clocking</strong>:
packet transmission is timed by the receipt of acknowledgements; each
acknowledgement slides the window.</p>

<p>A TCP connection receives a maximum size that the receiver can accept in
the <strong>receive window</strong> field of the TCP header (abbreviated as <strong>rwnd</strong>).
In addition to that, the
transmitter will dynamically adjust its transmit window size based on
its guess of network congestion. This window size is called the
<strong>congestion window</strong>, abbreviated as <strong>cwnd</strong>. At any point in time,
the maximum window size will be the smaller of these two windows: rwnd and cwnd.</p>

<p>IP provides no guarantees on the accuracy or timeliness of end-to-end
packet delivery through the network, nor does it provide any data on the
levels of traffic in the network or queue sizes at routers.
TCP, therefore, relies on making assumptions on congestion based on observed behavior.
If an RTO occurs or three duplicate acknowledgements
are received, the protocol assumes a segment was lost. Segment loss often
implies congestion, so TCP will decrease its transmission rate by reducing
the size of its congestion window, cwnd.
On the other hand, if TCP gets acknowledgements for sent packets then there
is no packet loss and therefore no congestion. In this case, TCP will increase
its transmission rate by increasing the size of its cwnd. This continuous
approach of increasing the transmission rate until packet loss occurs and
then decreasing it is called <strong>bandwidth probing</strong>. The connection tries to
see if it can transmit faster, backing down when problems arise, and then
trying again.</p>

<h3 id="aimd">AIMD</h3>

<p>TCP&#8217;s congestion control is called <strong>Additive Increase / Multiplicative Decrease</strong> (<strong>AIMD</strong>).
While a connection is experiencing no packet loss, it will increase
its congestion window by one MSS every round-trip time (RTT), hence increasing its
transmission rate. This is an <strong>additive increase</strong> (linear increase).
If cwnd was 15 segments and all 15 segments
were sent and acknowledged, the window is then increased to 16 segments.</p>

<p>In practice, we don‚Äôt wait for all the segments in the window to be sent out before
increasing the windows size. We increase the window size (cwnd) fractionally for each arriving ACK.
The number of segments that fit into a window is <strong>cwnd/MSS</strong>.
After that many segments have been delivered, we want to increase cwnd by one segment (MSS bytes).
That means for each ACK (each received segment),
we will increase cwnd by this fractional amount: MSS divided by segments_per_cwnd, or MSS / cwnd/MSS.
The assumption is that we always have data to transmit so every segment will be of size MSS.</p>

<p>If TCP feels that it has congestion (because of lost segments), the
congestion window is halved. This is a <strong>multiplicative decrease</strong>.
AIMD is a necessary condition for TCP congestion control to be stable in the
system as a whole and ensure that some connections do not end up monopolizing
the network link.</p>

<h3 id="statesoftcpcongestioncontrol">States of TCP congestion control</h3>

<p>TCP operates in one of three states: Slow Start, Congestion Avoidance, and Fast Recovery.</p>

<p>If we start with a congestion window of one MSS and increase it linearly, it can take
a long time before we reach an effective transmission rate. TCP <strong>Slow Start</strong>
<em>prevents</em> this slow ramp at startup by increasing the cwnd size exponentially.
The congestion window starts at one MSS and increases by one MSS with
each received ACK, causing it to double every RTT. Slow Start starts off
slowly but speeds up quickly. It continues to increase until cwnd reaches
a threshold level, called <strong>ssthresh</strong> (<strong>slow start threshold</strong>). Then the protocol switches to <em>Congestion Avoidance</em>.
Initially, ssthresh is effectively not set (set to a maximum value), so the rate
of transmission continues to increase exponentially until a transmission times out
waiting for an ACK. At this time, the protocol sets the threshold, ssthresh, to
<em>one half</em> of the window size that resulted in the RTO and restarts the Slow Start
process. This second time, it will ramp up to the threshold and then
switch to the Congestion Avoidance state (unless packet loss occurs).</p>

<p><strong>Congestion Avoidance</strong> is the normal state for TCP. The state is entered
after Slow Start reaches its threshold, which is one half the last window size that experienced
an RTO due to packet loss. Congestion avoidance continues to increase the
window size (cwnd) but does so linearly: one MSS per RTT. This continues until one
of two events occur. If an RTO occurs, then ssthresh is set to half
of the cwnd (half of the current window size when the packet loss occurred), cwnd is
set to one MSS, and the protocol moves to the Slow Start state.
If three duplicate ACKs are received then instead of
going all the way back to cwnd=1 and a Slow Start, the protocol switches to Fast Recovery.</p>

<p><strong>Fast Recovery</strong> is entered only when three duplicate ACKs are received.
The receipt of three or more duplicate ACKs is a sign that we lost a segment
but data is still flowing between the sender and receiver since each of those ACKs was generated
when a segment was received.
They are duplicates because one needed segment was not received.
Fast Recovery assumes that cwnd is the estimated system capacity.
Instead of reducing data flow abruptly by going into Slow Start, ssthresh
and the congestion window are cut to half of their current size (this is a
<strong>multiplicative decrease</strong>).
Fast Recovery loops, picking up every duplicate acknowledgement it
receives and increases cwnd by 1 MSS each time it does so. This includes
the three duplicates that caused TCP to enter this state.
Once a non-duplicate acknowledgement is received, cwnd is set back
to the threshold, ssthresh, and the state is switched to Congestion Avoidance (<strong>additive increase</strong>).
Should a retransmission timeout occur, the same thing happens as anywhere else that an
RTO occurs: ssthresh is set to half the window size, cwnd is set to one MSS,
and TCP switches to the Slow Start state.</p>

<h3 id="summaryoftcpcongestioncontrol">Summary of TCP congestion control</h3>

<p>TCP congestion control is always operating in one of three states.</p>

<ol>
<li><p>Whenever the congestion window, cwnd, is below the slow start threshold, ssthresh,
 the protocol is in the Slow Start state and the window increases exponentially.</p></li>
<li><p>Once cwnd reaches ssthresh, TCP enters the Congestion Avoidance state and grows
 linearly. </p></li>
<li><p>Whenever three duplicate ACKs are received, ssthresh and cwnd are
 halved and all duplicate ACKs are picked up before going back to the Congestion Avoidance state.
 Should an RTO occur in any state, ssthresh is set to cwnd/2, cwnd is set to one MSS,
 and TCP switches to the Slow Start state.</p></li>
</ol>

<figure>
<img src="images/tcp_states.png" alt="TCP states" id="tcpstates" title="TCP states" style="width:694px;" />
<figcaption>TCP states</figcaption></figure>



<div class="footnotes">
<hr />
<ol>


<li id="fn:1">
<p><a href="http://en.wikipedia.org/wiki/Internet_Control_Message_Protocol" title="ICMP" class="external">ICMP</a> is the <em>Internet Control Message Protocol</em>, a IP network-layer
protocol designed for sending various status and error messages to hosts and routers on the Internet. <a href="#fnref:1" title="return to article" class="reversefootnote">&#160;&#8617;</a></p>
</li>

</ol>
</div>


<h1 id="networklayer">Network Layer</h1>

<p>We looked at the transport layer (layer 3), which allowed
applications to communicate with other applications over
logical communication channels. The transport layer sits
on top of the <strong>network layer</strong> (layer 4). The network
layer provides host-to-host communication and is responsible
for routing packets (called <strong>datagrams</strong> at the network
layer) from a source host to a destination host.</p>

<p>A <strong>route</strong> is the path that a packet takes through the network.
<strong>Routing</strong> is the process of moving the packet along the route.
<strong>Routing algorithms</strong> figure out the route that
the packet will take.
A <strong>router</strong> is a host that forwards packets from an incoming
link to a specific outgoing link as determined by the route.
<strong>Forwarding</strong> is the process that a router uses to
transfer a packet from an incoming link to the specific outgoing link.
A router consults a <strong>forwarding table</strong> (also known as
a <strong>routing table</strong>) that uses information
in the packet headers to determine the outgoing link.
The forwarding table is configured by routing algorithms.</p>

<p>Ideally, we might expect a variety of guarantees from the network. These include:</p>

<ul>
<li>Guaranteed (lossless) datagram delivery</li>
<li>Time limits on datagram delivery</li>
<li>In-order datagram delivery</li>
<li>Guaranteed constant end-to-end bandwidth or the ability to offer a specific minimum bandwidth</li>
<li>The ability to specify maximum jitter. Jitter is the variation in the latency of packet delivery.</li>
<li>Security services, such as authenticating the source and destination as well as encrypting contents through the network.</li>
</ul>

<p>The Internet Protocol, IP, gives us none of these. It provides
<strong>best effort</strong> packet delivery but makes no guarantees on
the reliability of delivery, bounds on delay, jitter, or packet order.
Other network technologies, such as <strong>ATM</strong> (<strong>Asynchronous
Transfer Mode</strong>), provide some of these capabilities. ATM is a
<strong>virtual circuit (VC) network</strong> that provides logical connections
at the network layer. All routers in the path are involved
in setting up and maintaining the connection.
For example, ATM&#8217;s CBR (Constant Bit Rate) service allows the
connection to request a specific constant bandwidth and
specify constraints on jitter and packet loss. The network will
also guarantee in-order delivery.</p>

<p>A <strong>datagram network</strong>, such as IP, provides connectionless
service at the network layer and relies on the transport
layer to provide connection-oriented service. Only end
hosts are involved in providing transport-layer service;
the network layer itself is oblivious.</p>

<h3 id="virtualcircuitnetworks">Virtual Circuit Networks</h3>

<p>Before examining datagram networks, we&#8217;ll take a quick look at
virtual circuit networks. Unlike datagram networks,
virtual circuit networks require a connection setup phase
where an end-to-end route is established and each router
along the path agrees to participating in the path and commits
necessary resources (e.g., buffers for queues) to ensure
it can deliver the desired level of service being
requested.</p>

<p>A host that initiates a connection request for a virtual
circuit (a communication channel) identifies the virtual
circuit with a number.
As the path for a virtual circuit is set up,
each router enters the input port/output port
mapping for that path in its forwarding table
and designates a virtual circuit number for the outgoing
link (easier than allocating a
virtual circuit number that may need to be unique globally).
Unlike datagram routers, virtual circuit routers
need to maintain connection state information.
For communication, each packet only needs to contain
a virtual circuit number. There is no need to specify
the source or destination addresses since each forwarding
table can look up the incoming interface and virtual circuit
number, find the outgoing interface and change the virtual
circuit number for the next link.</p>

<h3 id="datagramnetworks">Datagram networks</h3>

<p>With routers on datagram networks, there is no a priori
setup of the route from source to destination. Indeed,
the route may change during a communication session.
Each datagram must be identified with the destination
address of the endpoint. A router uses this destination address
to forward the packet to the next network link.
A <strong>forwarding table</strong> on a router allows it to determine
the outgoing interface for a given datagram.
IP addresses are 32 bits long (for IPv4; IPv6 addresses are
128 bits long). That gives 2<sup>32</sup> (or 2<sup>128</sup> possible addresses. It
is not feasible to have a table of over four billion entries.
Instead, a forwarding table is based based on matching
a <strong>prefix</strong> of a number of most significant (leftmost)
bits in the address. The fewer bits in the prefix, the
more addresses are matched for that prefix. Since the forwarding
table may have a mix of longer and shorter prefixes,
it uses a <strong>longest prefix matching rule</strong>
so that longer, more specific, prefixes are tested prior to
shorter, more general, prefixes.</p>

<h3 id="routerarchitecture">Router Architecture</h3>

<p>An IP router comprises two part: a control plane and a data plane.
The <strong>control plane</strong> is responsible for the high-level software
of the router. It runs a <strong>routing processor</strong> that
implements the user interface, runs routing
protocols, populates forwarding tables, implements the ICMP protocol,
and controls queue behavior.</p>

<p>The <strong>data plane</strong> is responsible for
packet forwarding. Its purpose is to move packets from the <strong>input
port</strong> to the <strong>output port</strong> on a router as quickly as possible.
Because of the need to move as manay as tens of millions of packets
per second per port, the data plane is generally implemented in
hardware. Note that on a router, a <strong>port</strong> refers to the input and output
interfaces and has <em>nothing</em> to do with the use of the term <em>port</em>
at the transport layer. A <strong>line card</strong> is the hardware that is
responsible for implementing the input and output ports for a
specific interface (e.g., an Ethernet interface). Because the
router operates at layer 3 (the network layer), the data plane must
process layers 1, 2, and 3 of the protocol stack: </p>

<ul>
<li>At layer 1, it has to retime and regenerate the signal at the output port.</li>
<li>At layer 2, it has to create the new datalink headers and
checksums for transmission onto the selected output port</li>
<li>At layer 3, it has to extract the destination address, search
the forwarding table to determine the output port, decrement
a time-to-live count, regenerate a checksum, and forward the
datagram to the output port.</li>
</ul>

<p>The <strong>input port</strong> of a router
implements the link-layer protocol to accept incoming packets (frames) on the physical interface
of the line card. It decapsulates (extracts the data encapsulated in the frame) the layer 3 datagram
to get the IP packet, validates the protocol version number, and updates the packet&#8217;s time-to-live (TTL) field.
If the TTL field reaches 0, the packet is dropped and a message is sent to the routing processor in
the control plane to send back an error packet.
The input port then performs a lookup in the forwarding table (using a longest prefix match) to determine the required output port
to which the packet needs to be delivered.</p>

<p>The <strong>output port</strong> of a router accepts
outbound datagrams and encapsulates them with the appropriate link-layer headers (e.g., Ethernet).
Like the input port, it implements the link-layer protocol to transmit these outgoing packets (frames) on the physical interface
of the line card. </p>

<p>A packet is delivered from the input port to the output port via the router&#8217;s <strong>switch fabric</strong>. This is a
general term for the architecture that allows the movement of packets between the line cards.
This packet delivery may need to be delayed if the switch fabric cannot currently accept the packet or if another
input port is currently moving data to that same output port. In that case, the packet will
need to wait in a queue at the input port.</p>

<p>A router will have queues at both input and output ports.
The output port maintains a queue of packets received from the switch fabric
and transmits them using the link-layer protocol for the outbound interface.</p>

<p>Queues, of course, are finite in size and have the
risk of overflowing and therefore causing
packet loss.
If the queue at the output port is full, there is no
room for the packet and it will have to be dropped
or some other packet in that queue will have to be deleted.
The simplest
algorithm is first come, first served (FCFS) queuing.
A more sophisticated one may place a priority on
the source, destination, protocol, or even a service
level that may be embedded in the packet. <strong>Active
Queue Management</strong> (<strong>AQM</strong>) refers to the algorithm in
place to make the decision of which packet gets sent next
and which packet gets dropped if the queue is full.</p>

<p>At the input port, packets may be queued if they cannot
be forwarded to the output port quickly enough.
Queuing is susceptible to <strong>head-of-the-line blocking</strong>.
If a packet cannot be immediately forwarded to an
output port (typically because that port or switching
fabric is in use by another line card), not only is
the packet delayed but all the packets queued behind
it are blocked.</p>

<p>There are several router architectures and the choice of design largely depends
on cost and the performance needs of packet forwarding. Every one of these
architectures is in use.</p>
<dl>
<dt>Conventional shared memory</dt>
<dd>A conventional shared memory design is not different from that of a PC with each input/output pair of ports functioning as an input/output device. An incoming packet at a port generates a system interrupt. The operating system copies the packet from the transceiver to the system&#8217;s memory. The processor runs a network stack whose network layer searches the routing table to determine where the packet needs to be forwarded. The packet then travels back down the stack onto the desired output port. The limitation of this design is that only one memory operation can take place at a time. Moreover, the single CPU and system bus can become bottlenecks.</dd>

<dt>Shared memory with distributed CPUs</dt>
<dd>To alleviate the CPU bottleneck, this design incorporates a CPU on each line card. This gives each line card the intelligence to process the three layers of the stack, and determine the destination port of the packet without having to make use of the central CPU, shared bus, or main (shared) memory. The central CPU is responsible for the control plane: providing the administrative interface and creating the forwarding table. It pushes a copy of the forwarding table onto each line card. The shared memory and shared bus are used to allow the processor on one line card to copy the packet to another line card. The fact that the bus is shared and only one memory operation can take place at a time in shared memory can still result in a bottleneck for moving packets between line cards.</dd>

<dt>Shared bus, no shared memory</dt>
<dd>To alleviate having to use shared memory as an intermediate buffer in copying packets, this design allows one line card to send a packet directly to the memory of another line card using a common shared bus. The shared bus is now the performance bottleneck.</dd>

<dt>Switched (crossbar) data path</dt>
<dd>The final variation of architectures removes the shared bus and replaces it with a crossbar switch. This is a considerably more expensive option but allows one line card interface to switch directly to another line card interface without affecting the ability of other line cards to communicate.</dd>
</dl>


<h1 id="networklayer:internetprotocol">Network Layer: Internet Protocol</h1>

<p>The <strong>Internet Protocol</strong> (<strong>IP</strong>) has three components:</p>

<ol>
<li><p>The <strong>IP protocol</strong> itself, which deals with addressing
hosts, formatting datagrams, fragmenting and reassembling
datagrams, and forwarding datagrams through routers.</p></li>
<li><p><strong>Routing protocols</strong>, which determine network
connectivity and how forwarding
tables are configured at routers</p></li>
<li><p>The <strong>Internet Control Message Protocol</strong> (<strong>ICMP</strong>),
which is a network-layer protocol for error and status reporting.</p></li>
</ol>

<h2 id="theipdatagram">The IP datagram</h2>

<p>The IP datagram comprises a 20-byte header, a variable-size
options field after the header, and the payload, which will
typically be the TCP or UDP segment. It contains a 32-bit
source IP address, which identifies the sender, and a 32-bit
destination IP address, which identifies the recipient.</p>

<p>A <strong>time-to-live</strong> (<strong>TTL</strong>) field is a counter that is
designed to keep packets from circulating indefinitely in
the network case forwarding tables accidentally create cycles.
An IP datagram is typically initialized with a TTL of 60 or 64
and the TTL is decremented by one each time it enters a router.
If the TTL reaches zero, the router will discard the packet.</p>

<p>A <strong>protocol</strong> field in the datagram identifies the higher-layer
protocol that is contained within the data. Common values are
6 to identify the data as a TCP segment and 17 to identify the
data as a UDP segment. </p>

<p>A <strong>header checksum</strong> field contains a 16-bit header checksum.
This is calculated with the same formula as UDP and TCP checksums.
Only the IP header is checksummed. A router has to recompute
the checksum since the TTL field (and possibly the options field)
will change with each network hop.</p>

<h2 id="fragmentationandreassembly">Fragmentation and reassembly</h2>

<p>If a router needs to
forward a packet to a link that has a smaller MTU (maximum
transmission unit) than the incoming link, it is possible that
the IP packet may be too large to be transmitted as a single
frame (packet) on the outgoing link. To handle this situation,
IP supports <strong>fragmentation</strong>. If a packet is bigger than
the MTU of the outgoing link, a router can split the datagram
into two or more fragments. Each fragment is a separate IP
datagram with its own IP header. When the fragments reach
their ultimate destination, the receiving host must
<strong>reassemble</strong> them into a complete packet before passing them
to the transport layer.</p>

<p>Fragmentation is controlled by two data fields and two one-bit flags
in the IP header. A <strong>don&#8217;t fragment</strong> (<strong>DF</strong>) bit tells a
router that fragmentation is not permitted on a datagram. This
may result in the inability to route the datagram. If a
router makes the decision to fragment the datagram, the datagram
is split into two or more fragments. Each transmitted IP datagram
contains an identification number in the <strong>identification</strong> field
of the IP header. This is set when the original datagram is created
and is typically an incrementing counter for each successive datagram.
When a datagram is fragmented, the IP header of each datagram holding
a fragment contains the same ID number. This tells the receiver
that those datagrams are part of the same original datagram.
Each fragment also contains a 13-bit fragment offset. This is a
number that is multiplied by eight to indicate where the
data in this fragment belongs in the reassembled datagram. The
first datagram contains an offset of zero. Each datagram fragment
except for the last one has a <strong>more fragments</strong> (<strong>MF</strong>)
bit set to one. The last fragment will have MF=0 and
the fragment offset along with the IP length field will indicate
the length of the final reassembled datagram.</p>

<h2 id="ipaddressing">IP addressing</h2>

<p>Our discussion focuses on IP version 4, which is the most
widely deployed version of IP. IPv4 addresses are 32-bits
long. Every <em>interface</em> on an IP network must have a unique
IP address. If a host has two interfaces (e.g., Ethernet and
802.11 links), it will have one IP address for each link.
If a router has 128 ports, it will have 128 IP addresses.</p>

<p>We earlier discussed that it would be impractical for
addresses to be randomly assigned as each router would have
to have to be able to look up an individual address in
a forwarding table of over four billion addresses. Moreover,
routing algorithms would need to manage information about
the route of every single address on the Internet.
Instead, groups of adjacent addresses are assigned to
an organization. Rutgers, for example, has been assigned
all addresses with the top 16 bits of 128.6. A router
would need to know where to forward anything that starts
with 128.6 rather than maintain a table of all the
2<sup>16</sup> (65,536) possible addresses that may start with 128.6.
This ability to use one prefix to refer to a route that
may span multiple sub-networks or hosts is called
<strong>route aggregation</strong>.</p>

<p>A <strong>subnet</strong> (also called a
<strong>subnetwork</strong> or a <strong>network</strong>) is a group of adjacent
IP addresses that share a common prefix and are assigned
to an organization. A subnet makes up a logical
network that is connected to a router. For example,
routers on the Internet needs to know how to route an
address starting 128.6 to Rutgers.
Subnets are expressed in <strong>CIDR</strong> (<strong>Classless Inter-Domain
Routing</strong>) notation, whose format is a 32-bit IP address that
comprises the identifying bits of the subnetwork followed by
a slash an the number of bits that identify the subnetwork.
For example, 128.6.0.0/16 means that the top (leftmost) 16 bits of the
address 128.6.0.0 identify the subnetwork. The subnetwork
logically divides an IP address into a <strong>network</strong> part (the
bits that make up the subnet) and the <strong>host</strong> part (the
bits that identify the host within the subnet).</p>

<p>A <strong>subnet mask</strong> (also called a <strong>netmask</strong>) is a bit mask that contains ones in the
positions of the network bits of the address. For Rutgers,
this means the top 16 bits will be one, resulting in a
subnet mask of 255.255.0.0. A subnet mask is used to
strip the host bits from the address to match prefixes in
a forwarding table.</p>

<p>Subnetworks are hierarchical. An Internet service provider (ISP)
will often be assigned large blocks of IP addresses by a
Regional Internet Registry (RIR). Routers between ISPs will
need to know which block of addresses is handled by
which ISP. A specific ISP will allocate smaller blocks of IP
addresses
to organizations or lower-tiered ISPs. This is not relevant
information outside of the ISP since outside routers only
need to know how to reach one of the ISP&#8217;s routers. Routers
within the ISP need to route to the organizations that
were allocated those addresses. This process can continue
iteratively. Within Rutgers, for example, are multiple
networks that use blocks within the 128.6.0.0/16 allocation.
For instance, the host aramis.rutgers.edu has an address of
128.6.4.2 and a netmask of 0xffffff00. This indicates that
it is in a subnetwork that is defined by the prefix
128.6.4.0/24.</p>

<p>IP supports several special addresses: bit patterns that
cannot be used as generic host addresses.
An address of 255.255.255.255 represents a <strong>limited broadcast address</strong>.
This is a broadcast address for the host&#8217;s network.
Datagrams directed to this address will be delivered to all
hosts on the directly-connected network but routers will not
forward them to other networks (they are <em>limited</em> to the same local
area network as
the sender).
An address with only the host bits set to one (e.g., 128.6.255.255)
represents a <strong>directed broadcast address</strong>.
Datagrams directed to this address will be routed to the
specified subnet (if the router permits it) and delivered to all
hosts on that subnet (they are <em>directed</em> to a specific subnet).
Routers may be configured to forward
these datagrams to ensure that they are delivered to subnets
outside the directly-connected local area network. </p>

<h2 id="hostconfiguration">Host configuration</h2>

<p>A regular host on the Internet needs to know a few key parameters:</p>

<ul>
<li>Its <strong>IP address</strong>, so it can identify itself in the source address field of an IP header.</li>
<li>Its <strong>subnet mask</strong>. Using the subnetmask along with the IP address, it can identify its own subnet and hence identify which addresses are on the local subnet and which ones need to be directed to a router.</li>
<li>Its <strong>gateway</strong>. This is a router on the LAN and the default
address for non-local addresses that are not in a host&#8217;s local
routing table. A gateway is a simple router that routes datagrams
between the LAN and another network.</li>
<li>One or more <strong>domain name servers</strong>. It needs to know the address
of at least one name server so that it can look up Internet
domain names and find corresponding addresses.</li>
</ul>

<p>These four parameters can be configured manually. Alternatively,
the <strong>Dynamic Host Configuration Protocol</strong> (<strong>DHCP</strong>) can
be used to do this automatically.</p>

<p>DHCP is a protocol to allow a client to get an IP address for
itself as well as essential network configuration parameters.
The challenge with developing such a protocol is that it has
to work before the client has a valid address on the network.
Hence, a conventional request-response protocol with source
and destination addresses will not work. A requirement for
DHCP is that the DHCP server has to be running on the same
local area network as the host. If not, a DHCP Relay Agent must
run that serves as a proxy and forwards requests and responses
to the remote DHCP server. </p>

<p>DHCP uses limited broadcast messages (255.255.255.255). A
client is allowed to send a limited broadcast and is capable
of receiving one even if does not have an address assigned.
DHCP works in four steps, with an acronym of <strong>D-O-R-A</strong> to
describe them.</p>

<ol>
<li><p><strong>Discover</strong>. The client sends a limited broadcast
<strong>DHCP Discover</strong> UDP message to port 67. This contains a
random transaction identifier.</p></li>
<li><p><strong>Offer</strong>. The server listens to broadcasts coming in on
port 67. It gets the <em>Discover</em> message and responds back
by sending a limited broadcast <strong>DHCP Offer</strong> UDP message
to port 68. The response contains the following parameters:</p>

<ul>
<li>Matching transaction identifier</li>
<li>Proposed IP address</li>
<li>Subnet mask</li>
<li>Lease time</li>
</ul></li>
<li><p><strong>Request</strong>. The client picks up the server&#8217;s <em>Offer</em>
message. It compares the transaction identifier to ensure
that the offer is not directed to another client. If there
have been multiple DHCP servers and it received multiple
offers, it selects the one it wants to accept and ignores
the others. The client responds with a <strong>Request</strong> message
that contains a copy of the parameters in the Offer.</p></li>
<li><p><strong>ACK</strong>. The server associates the offered parameters
with the host and sends back a <strong>DHCP ACK</strong> message
acknowledging the association. The client can now configure
its network with those parameters.</p></li>
</ol>

<p>DHCP can be used in several scenarios:</p>

<ol>
<li><p>Automatic allocation. DHCP can be used to assign
a permanent IP address to a host. </p></li>
<li><p>Dynamic allocation. DHCP can be used to <strong>lease</strong>
an address to a host. The host may use the address for
a specified period of time. This allows the reuse of an
address after it is no longer needed by the host.
A Wi-Fi hotspot is a common example of this use of DHCP.</p></li>
<li><p>Manual allocation. An administrator can configure
the DHCP server to assign a specific address in response
to a DHCP <em>Discover</em> message. This is done by associating
the host&#8217;s link layer address (e.g., Ethernet MAC address)
with a specific IP address.</p></li>
</ol>

<h2 id="networkaddresstranslationnat">Network Address Translation (NAT)</h2>

<p>In order to move datagrams between hosts on the Internet,
each host interface needs to have a globally unique IP
address. If this is not the case, routers will not be
able to route the packet to that interface. The need for
this, of course, creates a huge need for IP addresses.
An organization with 10,000 hosts would need 10,000 IP
addresses.</p>

<p><strong>Network Address Translation</strong> (<strong>NAT</strong>) addresses this
problem by allowing an organization to create a
<strong>private IP address space</strong> within the organization
while presenting one, or a small set of IP addresses
to the outside Internet. As a packet flows through
a <strong>NAT-enabled router</strong>, the router uses a
<strong>NAT Translation Table</strong> to map a source
{private-address, port<sub>1</sub>} to a {public-address, port<sub>2</sub>}.
When packets flow back to the router from the outside,
the router uses the NAT Translation Table to perform
the inverse mapping of
{public-address, port<sub>2</sub>} to {private-address, port<sub>1</sub>}.</p>

<p>To enable NAT, the gateway router has to look at, and possibly
modify, the transport layer header since since a source
port number may need to be changed to one that is not used
by any other internal-external mapping at the router.</p>

<p>The private address space within the organization
must contain a range of addresses that are not used
by any hosts on the public Internet. Otherwise, there would be
ambiguity as to which host is being addressed and where it is located. Hence
private addresses are <strong>non-routable</strong> on the Internet and can only
be used in internal networks. RFC 1918 defines three
address blocks that can be used for these addresses.</p>

<p>Hosts in a NAT environment cannot accept incoming packets
unless a host/port mapping has been established by an outgoing
packet. As such, NAT is not particularly useful for servers
but is incredibly useful for client machines.</p>

<h2 id="icmp">ICMP</h2>

<p>The <strong>Internet Control Message Protocol</strong> (<strong>ICMP</strong>) is
a simple network-layer protocol that was designed to allow
hosts and routers to communicate network-related information.
ICMP is an eight byte or greater segment that sits in the payload
(data section) of an IP datagram.
It contains a <strong>checksum</strong> over the ICMP header and associated
data as well as
<strong>type</strong> and <strong>code</strong> fields,
which define the purpose of the message.
Depending on the message, four additional bytes may specify
parameters to the message and optional data may contain
the IP header and first eight bytes of the original datagram
for which ICMP is generating a report.</p>

<p>The most common ICMP message types include an <em>echo request</em> (ping),
<em>echo response</em> (ping), a <em>destination unreachable</em> status,
a <em>TTL exceeded</em> warning, and a <em>bad IP header</em> error.</p>

<p>The <strong>ping</strong> program is an example of a service that uses ICMP.
It creates a raw socket and generates an ICMP message of the
type <strong>echo request</strong> (type 8). When the message is routed
to the destination host, the ICMP protocol sends back an
ICMP <strong>echo reply</strong> (type 0) datagram. </p>

<p>The <strong>traceroute</strong> program traces a route to a specific host.
It also uses ICMP by sending a series of UDP segments to a
bogus destination port on the desired host. Each UDP segment
has a progressively longer time-to-live (TTL) value in the
IP header. The first router will not route the datagram with
a TTL of 1 since it decremented to 0 and hence expired.
Instead, the router sends back an ICMP <strong>TTL exceeded</strong>
warning message that contains the name and address of the router in
the body of the ICMP message. The datagram with a TTL=2 will be routed
by the first router but will be rejected by the second one, and so on.</p>

<h2 id="ipv6">IPv6</h2>

<p>We have thus far discussed IP version 4, the most widely deployed
version of IP. As IP was rapidly using up allocatable subnetworks
due to its 32-bit address size, design on a successor protocol,
called IPv6, began in the mid 1990s.</p>

<p>IPv6 uses a huge address space: 128-bit addresses compared
with IPv4&#8217;s 32-bit addresses. A 128-bit address allows for
3.4&times;10<sup>38</sup> addresses, which is 8.9&times;10<sup>28</sup>
times more than IPv4. Even though its addresses are longer,
IPv6 uses a simplified header compared to its predecessor.
It is a fixed-length headers with fewer fields. An optional
extension to the the header supports less-frequently used options
and additional capabilities.
Under IPv6, routers will never fragment IPv6 datagrams. This differs
from IPv4, where a router may do so if the outbound link has
a smaller MTU. With IPv6, the sender is expected to perform a
path MTU discovery ahead of time to determine the minimum transmission unit
for the entire route. To handle cases where higher levels of
software might create larger datagrams without checking
the path MTU, IPv6 does support fragmentation by the sender.
Since fragmentation is often not used, however, the fields related to managing
it are relegated to this optional header extension.
There is also no header
checksum. The designers reasoned that the link layer has a
checksum and TCP as well as UDP include critical IP fields in
their checksum computation. </p>

<p>Transitioning to IPv6 has been a challenge in a world
with widespread IPv4 deployment. IPv6 systems can bridge to
IPv4 systems since the IPv4 address space is mapped onto a
subset of the IPv6 space. The problem is that IPv4 systems
cannot effectively communicate with IPv6 systems due to its
larger address space. A system using IPv6 may not be visible
to a system on an IPv4 network. Most systems today are
<strong>dual-stack</strong> systems, with both network stacks implemented
and capable of using either protocol. In areas with widespread
IPv4 deployments, such as the U.S., IPv6 is finding most of its
initial deployment in less visible areas, such as cable modems,
set-top boxes, and VoIP (voice over IP) MTAs (multimedia terminal adapter).</p>

<h1 id="routing">Routing</h1>

<h2 id="routingalgorithmgoals">Routing algorithm goals</h2>

<p>Routers connect networks together at the network layer and are responsible
for moving datagrams (routing) from one link to another. In many cases,
a datagram will have to flow through multiple routers and there are multiple
possible paths that the datagram can take to reach its destination.
The goal of a <strong>routing algorithm</strong> is to figure out a good path, or <strong>route</strong>,
for a datagram to take to get to its destination. By <strong>good</strong>, we mean
an algorithm that will minimize the cost of the overall route. That <strong>cost</strong> may be either
time (quickest route) or money (if there are financial costs that differ between
different routes).</p>

<p>For purposes of analysis, a route may be represented as a <strong>connected graph</strong>,
<strong>G = (N, E)</strong>, where <em>N</em> is the set of nodes (vertices) (routers, in real life) and
<em>E</em> is the set of edges (links between the routers in real life).
A connected graph is one where, given two nodes <em>a</em> and <em>b</em>, there is some path
from <em>a</em> to <em>b</em>.
Each edge is identified by a pair of nodes. A node <em>y</em> is considered to be
a <strong>neighbor</strong> of node <em>x</em> if the edge (x, y) exists in the graph. That is,
<em>(x, y) &isin; E</em>.</p>

<p>Each edge has associated with it a value that represents the cost of the link.
We represent the cost of an edge between nodes <em>x</em> and <em>y</em> as <em>c(x, y)</em>. If
there is no edge <em>(x, y)</em> in the graph then the cost <em>c(x, y)</em> is infinite (&#8734;).
If we need to route from node <em>x</em> to node <em>y</em> in this case, we will need
to establish a path through some other nodes.
For the purposes of our analysis, we will assume that a link has the same
cost in each direction. That is, <em>c(x, y) = c(y, x)</em>. </p>

<p>A <strong>path</strong> in a graph <em>G = (N, E)</em> is a sequence of nodes
(<em>x<sub>1</sub>, </em>x<sub>2</sub>, &#8230; <em>x<sub>p</sub></em>) such that each of the pairs
(<em>x<sub>1</sub>, </em>x<sub>2</sub><em>), (</em>x<sub>2</sub>, <em>x<sub>3</sub></em>), etc. are edges in <em>E</em>: a path
is a sequence of edges. The <strong>cost of a path</strong> is the sum of the
edge costs. Since there may be multiple paths from one node
to another, one or more of
these will be a <strong>least-cost path</strong>.
If all edges have the same cost, that least-cost path will also be the <strong>shortest path</strong>.</p>

<h2 id="categoriesofroutingalgorithms">Categories of routing algorithms</h2>

<p>There are two categories of routing algorithms. A <strong>global routing algorithm</strong>
relies on complete knowledge of the network graph. The algorithm, as input, knows
the connectivity graph: all the nodes and edges. Algorithms in this category are known as <strong>link-state</strong> (<strong>LS</strong>)
algorithms. <strong>Dijkstra&#8217;s shortest path algorithm</strong> is an example of an LS algorithm.</p>

<p>In a <strong>decentralized routing algorithm</strong> , no node has complete knowledge of all
links in the graph. A node initially knows only about its direct links.
Through an iterative process of exchanging lists of nodes and costs with its
neighbors, a node will eventually compute the least-cost path to any destination in the graph.
The <strong>distance-vector</strong> (<strong>DV</strong>) algorithm is an example of a decentralized routing algorithm.</p>

<h2 id="link-state:dijkstrasshortestpathalgorithm">Link-State: Dijkstra&#8217;s shortest path algorithm</h2>

<p>Dijkstra&#8217;s algorithm is a global algorithm that assumes that the entire network topology
(graph nodes, edges, and edge costs) is known to the algorithm. In an implementation, each
node will need to broadcast any link change information to every other node so that
all nodes will have an identical and complete view of the network.</p>

<p>Dijkstra&#8217;s algorithm is an iterative algorithm that, after <em>k</em> iterations, will
compute the least-cost paths to <em>k</em> nodes from some given initial node.
For each iteration, the algorithm keeps a list of nodes, <em>N&#8217;</em> for which the lowest
cost path has already been found (the current node requires no edge and is
the initial element on this list). For each node in the graph, the algorithm
stores:</p>

<ul>
<li><strong>D(v)</strong>: the <strong>distance</strong>, our current knowledge of the least cost path to get to node <em>v</em>.</li>
<li><strong>p(v)</strong>: the <strong>previous node</strong> before node <em>v</em> along the least cost path that we have so far.</li>
</ul>

<h3 id="initialization">Initialization</h3>

<p>Let us assume that we need to find the least-cost routes from some node <em>u</em> to all other nodes.
The list of nodes with a known least-cost path, <em>N&#8217;</em> is initialized to <em>u</em>, our starting node.
The distance for each node <em>v</em>, <em>D(v)</em> is set to the cost of the
edge from <em>u</em> to <em>v</em>, <em>c(u, v)</em>. If there is no edge between the
nodes, the cost is set to infinity. For each node <em>v</em> with a non-infinite cost, the previous node, <em>p(v)</em>,
is set to <em>u</em> since the entire path at this time is simply a single edge from <em>u</em> to <em>v</em>.</p>

<h3 id="foreachiteration">For each iteration</h3>

<p>Each iteration picks a new node, <em>n</em>, and examines the total distance to each of <em>n</em>&#8217;s
neighbor nodes from <em>u</em> through <em>n</em>. Here are the steps.</p>

<ol>
<li><p>Pick a node <em>n</em> that is not in <em>N&#8217;</em> and has the smallest distance <em>D(n)</em>. This will be the
node that we will examine this iteration and for which we will find the definitive least-cost path.</p></li>
<li><p>Add node <em>n</em> to the least-cost list <em>N&#8217;</em>.</p></li>
<li><p>For each neighbor <em>m</em> of node <em>n</em> that is not in <em>N&#8217;</em>, compute the cost of the route through <em>n</em>.
This is <em>D(n)</em> + <em>c(n, m)</em>; that is, the cost to <em>n</em> plus the cost of the edge from <em>n</em> to <em>m</em>.</p></li>
<li><p>If this computed cost to <em>m</em> through <em>n</em> is lower than the value we currently have for <em>D(m)</em>,
update <em>D(m)</em> with the new cost and set the previous node of <em>m</em>, <em>p(m)</em>, to <em>n</em> since the path through <em>n</em>
resulted in a lower cost.</p></li>
</ol>

<p>Eventually, all nodes will be in the list <em>N&#8217;</em> and there will be no more nodes left to process.
At this time we have computed the least-cost paths from <em>u</em> to all nodes in the graph.</p>

<h3 id="findingtheleast-costroute">Finding the least-cost route</h3>

<p>After running Dijkstra&#8217;s algorithm for a starting node <em>u</em>, we know the least cost to each node <em>v</em> and the
node that is encountered right before <em>v</em> on that least-cost path: <em>p(v)</em>. We can work backwards from this
to compute the full route. For example, if the previous node for some node <em>z</em> is <em>w</em>, we can look up <em>p(w)</em>
to find the node before <em>w</em> along the least cost path to <em>u</em>. Suppose that is <em>r</em>. We then look up
<em>p(r)</em> to to find the node before <em>r</em> along the least cost path to <em>u</em>. Suppose that is <em>u</em>, our starting
node. We now reconstructed the least-cost path: <em>u &rarr; r &rarr; w &rarr; z</em>.</p>

<p>A routing table at <em>u</em>
is interested not in the last hop or the entire path, but only in the first hop along
the least-cost path so it can forward its
datagram to that next router. In the routing table, we would need an entry that states that datagrams
for the range of addresses handled by <em>z</em> need to be forwarded to router <em>r</em>.</p>

<h3 id="oscillations">Oscillations</h3>

<p>If we have an environment where link costs vary to reflect the current traffic volume on the link,
the lowest-cost paths will favor uncongested links. This will cause routers to send more data
over these low-traffic links, which will, in turn, increase the level of traffic on these links and
take traffic away from what used to be high-traffic links.
When the algorithm is re-run, lowest-cost routes will now be recomputed to be the formerly high-cost
routes since we channeled traffic away from them. As the new routing table is used, we will see
the same phenomenon happen again as traffic is shifted to what used to be low-volume links.
This results in <strong>oscillations</strong> between network routes each time the LS algorithm is re-run.
The best approach to avoiding these oscillations is to have routers run their LS algorithm at
random times rather than all at the same time.</p>

<h3 id="bellman-fordequation">Bellman-Ford equation</h3>

<p>The distance-vector algorithm is based on a simple principle that is embodied in the <strong>Bellman-Ford</strong>
equation. This equation states that the least cost to get from node <em>x</em> to node <em>y</em> is to go through
a neighbor node <em>v</em> where the cost form <em>x</em> to <em>v</em> plus the cost from <em>v</em> to <em>y</em> is the smallest. </p>

<p>Let&#8217;s look at a two-node example. Suppose that you are in Geneva, want to get to Munich, but must
travel through either Zurich or Turin. Your cost is travel time.
The paths from Zurich or Turin to Geneva may involve several more
stops but you don&#8217;t care about that. You just know the cost to your neighbors, Zurich (3:00 hours)
and Turin (2:40) and you know the cost from Zurich to Munich (3:15) and the cost from Turin to
Munich (6:00). To determine the best first leg of the route, you want to minimize the overall cost.
The cost (time) via Zurich is 3:00 + 3:15, or 6:15. The cost via Turin is 2:40 + 6:00, or 8:40.
Hence, you choose to use Zurich as your first hop. Pretty simple, eh?</p>

<h2 id="distance-vectoralgorithm">Distance-Vector Algorithm</h2>

<p>The <strong>distance-vector</strong> (<strong>DV</strong>) algorithm is a distributed algorithm where a node (router) communicates
only with its
neighboring nodes.
The DV algorithm is <strong>iterative</strong>. A node will send messages to its neighbors only when its local link costs
change or it receives a distance vector update message that results in the node changing its least-cost route
estimate to some destination.</p>

<p>A node <em>n</em>&#8217;s <strong>distance vector</strong> is a list of costs from <em>n</em> that each other node in the graph.
Unknown costs are infinity. Each cost is considered to be an <em>estimate</em> as it may be based on
incomplete data. Eventually, the algorithm converges and the costs become true least-cost values.
In the DV algorithm, a node sends its distance vector to its neighboring nodes.</p>

<p>A node also keeps a <strong>distance vector table</strong>. This is a set of distance vectors that it has received
from its neighbors. These distance vectors are used to allow a node to check whether it is more
efficient to have a route that goes through a specific neighbor, <em>m</em>, than the node it currently
thinks is the next hop on the shortest path. </p>

<h3 id="initialization">Initialization</h3>

<p>Initially, a node knows only of its neighbors and the cost to each neighbor. The distance vector is
a set of <em>(node, cost)</em> tuples, with the cost being the cost of the direct link to the neighbor.
Send a copy of the distance vector to each neighbor.</p>

<h3 id="operation">Operation</h3>

<p>Suppose that a node <em>n</em> receives and saves a distance vector from another node, <em>m</em>.
It then does a node-by-node comparison of all the nodes in both vectors. For a given node <em>x</em>:</p>

<ul>
<li><p>Is the cost of routing to <em>x</em> through node <em>m</em> lower than the current cost estimate?
That is, is the cost to <em>x</em> supplied in node <em>m</em>&#8217;s distance vector plus the cost of the link from <em>x</em> to <em>m</em>
result in a smaller value than <em>n</em> currently has?</p></li>
<li><p>If yes, update <em>n</em>&#8217;s distance vector for the destination <em>m</em> to the value computed by going through <em>x</em>. If no,
leave the distance to <em>m</em> unchanged.
To build a routing table, <em>n</em> would also record that the currently-known lowest-cost to <em>m</em> is via <em>x</em>.
Unlike the link-state algorithm, the distance-vector algorithm keeps track of first hops rather than last hops.</p></li>
<li><p>Anytime a node initializes or updates its distance vector, it sends a copy to each of its neighbors.</p></li>
</ul>

<p>Eventually, no node will experience any changes to its distance vector and therefore will
not send any updates to its neighbors. The algorithm has converged.</p>

<h3 id="poisonreverse">Poison reverse</h3>

<p>If a link to a node fails at some point, a neighbor will mark the cost of that link as infinite.
As long as alternate paths exist, the algorithm will converge and use alternate paths.
However, consider (for example) a case where there is only a single link to node <em>C</em> from <em>B</em>.
Node <em>A</em> tells its neighbors that it can route to <em>C</em> (by going through B). If the link between
<em>C</em> and <em>B</em> fails, node <em>B</em>., using <em>A</em>&#8217;s information about the route, will attempt to use <em>A</em> as an alternate route, not realizing that the attempted route will be
<em>B &rarr; A &rarr; B &rarr; C</em>. This will result in an infinite sequence of distance
vector updates between <em>B</em> and <em>A</em>, each advertising a progressively higher cost as they
factor an additional link cost between the <em>AB</em> link each time. This is known as the
<strong>count-to-infinity</strong> problem.</p>

<p>A technique called <strong>poison reverse</strong> tries to mitigate this problem. Since <em>A</em> knows that
it needs to route through <em>B</em> to get to <em>C</em>, whenever <em>A</em> sends its distance vector to
<em>B</em>, it will set any costs whose first hop is <em>B</em> to infinity. This will avoid creating
a <strong>routing loop</strong>.</p>

<h1 id="internetrouting">Internet Routing</h1>

<h2 id="autonomoussystems">Autonomous Systems</h2>

<p>An <strong>autonomous system</strong> (<strong>AS</strong>) is a collection of routers and hosts that are administered together
and expose a single routing policy to other systems on the Internet.
ASes provide a two-level routing <strong>hierarchy</strong> in the Internet. Organizations manage their own
infrastructure and expose only limited connectivity to others.
Each AS comprises one or more subnetworks and
serves one or more ranges of IP addresses. It advertises the set of IP prefixes that
it can route (via CIDR and route aggregation to minimize the length of the list). The AS is responsible
for the routing of traffic within its AS, whether it is routing it to another AS or to a machine within its own AS.
The Internet can be viewed as a set of connected ASes, with packets being sent from one AS, possibly
routed through other ASes, and delivered to a target AS.
Routing on the Internet takes place between ASes. Using ASes simplifies the problem of dealing with
the billion hosts on the Internet. </p>

<p>An <strong>Intra-AS routing protocol</strong>, called an <strong>Interior Gateway Protocol</strong> (<strong>IGP</strong>) runs
within an AS. It is up to the system administrator to pick a protocol (e.g., an LS or DV algorithm)
and manage the
routes between machines within the AS. The outside world does not see the routes within an AS.
Some Intra-AS routing protocols are RIP and OSPF.</p>

<p><strong>Gateway routers</strong> are routers within an AS that connect to other ASes and forward packets between them.
In addition to running an intra-AS routing protocol with other routers inside the AS, they
are also responsible for routing to destinations outside the AS by sending datagrams to
other gateway routers.
An <strong>Inter-AS routing protocol</strong>, called an <strong>Exterior Gateway Protocol</strong> (<strong>EGP</strong>) runs on
<strong>gateway routers</strong> and enables
them to learn of routes to addresses served by other ASes or routed by the gateway routers within that AS.</p>

<p>Logically, an external AS looks like a router. An AS will be
connected to some other ASes and needs to learn
which destinations are reachable via those ASes.
Some of those ASes may need to relay the datagram in order to send it onto other ASes.
If a desired subnet can be accessed through either of several ASes (that is, either of those ASes can
route to that subnet, not that the subnet belongs to multiple ASes), then a common approach is to
have the AS send the packet out onto another AS in the least-cost manner. This is called <strong>hot-potato
routing</strong>. The goal is to find the lowest cost path to any gateway router that can then route to some
AS that can deliver the packet.
Since ASes are owned by different organizations, everyone on the Internet must agree to use the same
<strong>inter-AS</strong> routing protocol. Currently, this protocol is BGP version 4. </p>

<h2 id="autonomoussystemtaxonomy">Autonomous System Taxonomy</h2>

<p>Each Autonomous System is assigned a unique ID by a Regional Internet Registry (the same
organization that assigns blocks of IP addresses). Policies defined by the owner of the AS
determine if the AS will route traffic to other ASes and, if it will, whose traffic it will route.</p>

<p>A <strong>Transit Autonomous System</strong> is an AS that provides the ability to route traffic from
one AS to another AS.
A <strong>Tier 1 ISP</strong> represents a transit autonomous AS (or set of ASes) that does not pay any other network for
transit. It peers (and connects directly) with every other Tier 1 network so it can route an
IP address directly to the Tier 1 network that oversees it.</p>

<p>A <strong>Transit</strong> relationship on the Internet is considered to be one where an AS sells access to the Internet.
That is, it agrees to act as a router between ASes but traffic is metered and charged.
A <strong>peering</strong> relationship is one where a pair of ASes agrees to exchange traffic with each other
for no cost.
A <strong>Tier 2 ISP</strong> is an AS (or set of ASes) that needs to purchase Transit to connect to some
parts of the Internet. Establishing a peering relationship avoids the need to purchase Transit.</p>

<p>A <strong>stub Autonomous System</strong> is an AS that is connected only to one other AS (run by an ISP).
Because it is connected to just one AS, it cannot provide transit. A <strong>multi-homed stub
Autonomous System</strong> is an AS that is connected to multiple ASes (e.g., multiple ISPs) but will
<em>not</em> offer routing services between them.</p>

<h2 id="routinginformationprotocolrip">Routing Information Protocol (RIP)</h2>

<p>The <strong>Routing Information Protocol</strong> (<strong>RIP</strong>) is an Intra-AS IP routing protocol
(<strong>interior gateway protocol</strong>) that uses
a form of the <strong>Distance-Vector algorithm</strong>. It counts only hops, so the cost of each link is one.
RIP creates and manages a <strong>routing table</strong> on a router. For each destination (a subnet: a group of IP
addresses), the table contains the number of hops to that destination and the address of the next router (the first hop).</p>

<p>As with the DV algorithm, each router sends periodic <strong>RIP advertisements</strong> to its
neighbors. A RIP
advertisement is just the routing table with hop counts. When another router receives such an
advertisement, it compares the routes in the received table to see if routing that
that node will result in fewer hops. If so, it will update its own routing table. It will
also add any new subnets that it obtains from received tables.</p>

<h2 id="openshortestpathfirstospf">Open Shortest Path First (OSPF)</h2>

<p><strong>Open Shortest Path First</strong> (<strong>OSPF</strong>) is another <strong>interior gateway protocol</strong> that was designed
as a successor to RIP. It is a <strong>link-state algorithm</strong> based on Dijkstra&#8217;s shortest path algorithm.
Because of this, every router constructs a complete graph of the systems in the AS.
Any link changes are broadcast to all routers, not just a node&#8217;s neighbors.</p>

<p>To support larger networks, OSPF allows the network in an AS to be partitioned into multiple
<strong>OSPF Areas</strong>. Each area runs its own OSPF link-state algorithm and routes any packets that
are destined to go out-of-area to an <strong>area border router</strong> (<strong>ABR</strong>). The collection of these
area border routers belong to a common <strong>backbone area</strong>. They summarize (aggregate)
routes to the subnetworks in their own area and advertise them to other area border routers.
Area border routers also run the OSPF algorithm not just to learn routes within its
area but also to ensure that each ABR
can route to the proper ABR in another area based on a prefix match of the destination IP address.
OSPF Areas make a single AS look like a mini Internet.</p>

<h2 id="bordergatewayprotocolbgp">Border Gateway Protocol (BGP)</h2>

<p>Unlike RIP and OSPF, the <strong>Border Gateway Protocol</strong> (<strong>BGP</strong>) is an <strong>exterior gateway protocol</strong>:
an <strong>inter-AS routing protocol</strong>.
Gateway routers in an AS establish a TCP connection with gateway routers in other ASes.
A pair of such routers are known as <strong>BGP peers</strong> and the TCP connection between them is known
as an <strong>external BGP session</strong> (<strong>eBGP</strong>).
In cases where an AS has more than one gateway router, other routers need to know which
IP address prefixes are served by which gateway.
<strong>Internal BGP sessions</strong> (<strong>iBGP</strong>) between a gateway router
and other routers within the AS allow the gateway router to propagate information about
external IP prefixes that it can reach. Typically, iBGP will run between the gateway
router and the area border routers (ABRs) in an OSPF backbone area.</p>

<p>BGP peers exchange CIDR route prefixes and use a distance vector (DV) algorithm to establish least-cost paths.
A gateway router advertises its <strong>prefix reachability</strong>, which means it
tells its peers in neighboring ASes the routes that it is capable of handling (as CIDR prefixes).
In this manner, each AS finds out which neighboring AS yields the lowest-cost path to a
destination. Each BGP advertisement identifies a <strong>route</strong>, which consists of:</p>

<ul>
<li>a CIDR prefix (e.g., 128.6.0.0/16)</li>
<li>a path, which is a list of ASes through which the advertisement passed. This allows an AS to detect
and reject routing loops. The sending of a full path makes BGB a <strong>path vector</strong> protocol: a
variation of the distance-vector protocol that sends entire paths.</li>
<li>a next-hop router, which identifies the address of the remote router that sent the
advertisement and allows the gateway router in an AS to address the router in a remote AS.
The next-hop value also allows an AS to choose to choose from several possible links to
another AS.</li>
</ul>

<p>An important facet of BGP is its use of policies. An <strong>import policy</strong> defines
what routes an gateway router will reject. Policies are designed by an administrator and
can set local preferences and policies such as not offering transit to certain ASes.</p>


<h1 id="multicast">Multicast</h1>

<p>A <strong>unicast</strong> is a point-to-point transmission. The communications we looked at to date
were unicast messages: a node sends a message to a unique destination.
A <strong>broadcast</strong> is a message that is received by everyone on a network.
We encountered this when we examined IP addresses. An IP address with
all bits set to 1 is a <strong>limited broadcast address</strong> where the datagram is
delivered to all nodes on the local area network but not forwarded by routers.
A <strong>directed broadcast address</strong> where only host bits are set to 1
(the low-order bits that make up the host, as opposed to the network bits)
delivers a datagram to all hosts on the specified subnet. Routers must
may forward the datagram to that subnetwork.</p>

<p>A <strong>multicast</strong> is a message that is delivered to all members of a group.
That is, it is delivered to all hosts that have <strong>subscribed</strong> to receive
the multicast.
These group members do not all have to reside on the same local area network.
A simple way of implementing a multicast is by an <strong>n-way
unicast</strong>. The sending host simply iterates over the list of destinations
and sends a datagram to each destination. This is inefficient since the sender must
transmit
<em>N</em> times the traffic. Moreover, the sender needs to know all of the
recipients.</p>

<p>It is more efficient &#8211; and desirable &#8211; to have routers replicate packets.
With <strong>uncontrolled flooding</strong>, a host sends
a single datagram that is then replicated by each router and sent over every
other interface in that router. The problem with this approach is that
any cycles in the graph formed by router connections will create a <strong>broadcast storm</strong>: a router
will have receive a duplicate datagram, send it
out on each interface, receive one or more of those duplicates at a later time,
duplicate and send that, and so on.</p>

<p>A <strong>controlled flooding</strong> approach ensures that these cycles do
not occur. <strong>Sequence number controlled flooding</strong> requires each
multicast packet to have a sequence number affixed to it.
Each router keeps track of the sequence numbers of datagrams that it has
recently routed. If it receives a datagram with a new sequence number,
the router sends a copy over each link except the one it came in on.
If the router
gets a packet with a sequence number that it has already seen,
it discards the packet. <strong>Reverse path forwarding</strong> (<strong>RPF</strong>) is a
technique that does not require adding new data to a
packet. When a router receives a datagram, it looks at the
datagram&#8217;s source address and then consults its routing table
to see whether the datagram arrived on the link that
corresponds to the route to the source (which will be the
shortest path to the source). If it does, then the router
forwards a copy of the packet onto every other link. If it does not,
that indicates that the packet is a duplicate that arrived through
a cycle. That packet is discarded. A multicast based on controlled flooding
using RPF is called a <strong>source-based tree</strong>: the flow of multicast
packets forms a tree that is rooted at the initial sender.
<strong>With a source-based
tree, each sender establishes a separate shortest-path tree for a multicast group.</strong>
Since RPF is used, routers have to keep track of the sender of the multicast packet.</p>

<p>Another approach to controlled flooding is not to send a copy
of the datagram onto every link in the router. Instead, we create
an <strong>overlay network</strong> that is a subset of the full network.
This overlay network is a <strong>spanning tree</strong>, which
is a graph that contains all the nodes of the network but
only a subset of the edges such that the graph is connected
(all nodes are reachable) but there are no cycles.</p>

<p>One way of constructing a spanning tree is to pick a random node
to serve as a <strong>rendezvous point</strong> (also known as a center node).
Every node that wants to receive multicasts will send a <strong>join</strong>
message to that rendezvous point. As each join message propagates
through routers, each router records the incoming and outgoing
interfaces as being associated with that multicast address. The
incoming and outgoing links become part of the spanning tree.
When all interested nodes have sent <strong>join</strong> messages, the
spanning tree is complete. Any multicast messages will be forwarded
only along the links that make up the spanning tree. This multicasting
technique is called a <strong>group-shared tree</strong> since only interested
edge routers beome part of the tree.
<strong>With a group-shared tree, a single tree is shared among all senders
for a multicast group; each sender has to send its multicast packets to a common
rendezvous point in the exact same manner as if it was sending unicast messages.</strong>
Unlike a source-based tree, routers do not need to make routing decisions based
on the sender&#8217;s address for a multicast destination.</p>

<h2 id="ipmulticastrouting">IP multicast routing</h2>

<p><strong>IP multicast</strong> is designed, like IP, to be decentralized and span multiple physical networks.
Membership is dynamic: a machine can join or leave a multicast group at any time.
There is no central coordinator and no restriction on the number
of hosts that can be in a group. Multicasting provides network efficiency.
Datagrams in a multicast stream only need to be replicated when a router needs
to send them to multiple network links. A sender transmits only a single
stream of datagrams and only one stream of datagrams is ever needed
on any network segment regardless of the number of receivers.</p>

<p>An <strong>IP multicast address</strong> (also known as a <strong>class D address</strong>) is
an IP address that starts with the bit pattern 1110 and
contains a 28-bit multicast group ID.
With IPv6, a multicast group address starts with the bit pattern of eight 1s (1111 1111)
and contains a 112-bit multicast group ID (the other buts are used for flags and to indicate scope;
we will not cover that here).
A host may join any IP multicast address and receive messages addressed to that multicast ID.
The collection of systems that are interested in receiving multicast messages from a specific multicast group is called a <strong>host group</strong>.</p>

<p>Routers have to get involved to support multicasting beyond the local area network.
Two protocols are used to implement multicasting.
The <strong>Internet Group Management Protocol</strong>
(<strong>IGMP</strong>) is designed for hosts to inform routers that
they are interested in joining a multicast group.
The <strong>Protocol Independent Multicast</strong> (<strong>PIM</strong>)
protocol enables routers to tell their neighboring routers that they
are, or no longer are, interested in receiving packets for a particular
multicast group.</p>

<p>A host uses IGMP to send a
<strong>multicast report</strong> message to join a specific multicast group. A multicast-aware
router will get this message and now know that the link on which
the message arrived needs to receive any packets addressed to that multicast
group. Periodically, a router will send a <strong>membership query</strong>, asking hosts
what multicast groups they want to receive. If no host on a network link
responds, the router will assume that nobody on that LAN is interested in
receiving multicast packets. If a host is interested, it has to re-send
a <strong>multicast report</strong>. This technique of requiring renewals and deleting
a subscription if no renewal is received is called <strong>soft state</strong>. A
host may send an explicit <strong>leave group</strong> message to state that it is
no longer interested but the soft state mechanism ensures that multicasts
will not be sent onto the network even if that does not take place (if the
machine dies, for example).</p>

<p>IGMP allows routers to know what multicast groups the nodes on its connected LANs are interested in.
PIM, Protocol Independent Multicast, is responsible for conveying membership information
among routers. It assumes the presence of other protocols to know the network topology and which
routers are connected together. There are two approaches to multicasting on the WAN (wide-area
network): <strong>flooding</strong> and <strong>sparse-mode multicast</strong>.</p>

<p><strong>Flooding</strong>, also known as <strong>Dense Mode Multicast</strong>, uses a source-based tree. That is, all multicast traffic
originates from the source and the message is duplicated at a router and sent to all of its connected routers.
Each of those routers,
in turn, duplicates and sends the message to all of its connected routers, and so on. Reverse
path forwarding (RPF) is used to ensure that no forwarding cycles arise.
PIM Dense Mode floods the entire network of connected multicast-aware routers. A router
stops forwarding traffic only when it receives a <em>Prune</em> message from a router telling it
that the router does not want the multicast traffic for that address.
It does this if its downstream routers are not interested in that multicast stream.
If a node on a LAN joins a multicast group at a later time and sends an IGMP message to a
router, that router would then send a PIM <em>Graft</em> message to its connected routers
to state interest in the stream.
Dense mode only makes sense when there are interested receivers spread through most locations covered
my multicast-aware routers. It is rarely used.</p>

<p>In contradistinction to Dense Mode, PIM <strong>Sparse Mode Multicast</strong> starts with requests
from multicast receivers rather than flooding the network with traffic from the sender.
Each multicast group must be associated with a router known as a <strong>rendezvous point</strong>.
Edge routers that are interested in receiving a multicast group send <em>join</em> messages
to that rendezvous point. This builds a spanning tree between the rendezvous point
and the subscribers.
This rendezvous point acts as a
central point that senders route to when they are transmitting
multicast streams.
The advantage of PIM sparse mode multicast is that packets go only where needed.</p>

<h1 id="datalinklayer">Data Link Layer</h1>

<p>The data link layer is concerned with sending and receiving data on the actual communication
links that connect machines. These are the links that constitute a local area network (<strong>LAN</strong>).
Packets at the data link layer are called <strong>frames</strong>. <strong>Medium Access Control</strong> (<strong>MAC</strong>) is
the general term for the protocol that is used for transmitting and receiving link-layer frames.
Interfaces at the link layer use link-layer addressing. A <strong>MAC address</strong> (for example, an
Ethernet address) is different from, and unrelated to, an IP address. An Ethernet MAC address
is globally unique to a device and there is no expected grouping of such addresses within
a local area network. IP addresses on a LAN, on the other hand, will share a common network prefix.</p>

<h2 id="errordetectioncorrection">Error Detection &amp; Correction</h2>

<p>MAC protocols usually employ <strong>error detection codes</strong> and sometimes employ <strong>error detection and
correction codes</strong>. We&#8217;ve seen that IP used error detection codes to validate an IP header
and both TCP and UDP used them to validate their headers and data. Why do we need this at the link layer?
The basic advantage of detecting errors at the link layer is to catch bad frames early and avoid
the overhead of propagating a useless frame up the network stack. If the firmware on a network
card can detect an error, it can drop the packet and not even waste time interrupting the processor.
If error correcting codes are used, the link layer now has the opportunity to correct a limited
amount of bit errors in the received frame. This can avoid the costly delay of having to retransmit
a packet.</p>

<h3 id="parity">Parity</h3>

<p>The simplest form of error detection is <strong>parity</strong>. With parity, we add one bit to a block of data, called
a <strong>parity bit</strong>.
With <strong>even parity</strong> the parity bit is set such that the total number of one bits in the
block of data (including parity) is an even number.
With <strong>odd parity</strong> the parity bit is set such that the total number of ones is an odd number.
Parity is simple to compute and requires a low overhead both in terms of storage and computation.
However, it cannot detect an even number of bit errors (one error cancels the other). Moreover,
in networks, bit errors typically occur in bursts, not single bits, so parity is a poor choice
for this kind of application. Parity is popular in applications where errors rarely happen and,
when they do, are usually single-bit errors. Memory chips are an example where parity codes are
useful.</p>

<figure>
<img src="images/2d-parity.png" alt="Two-dimensional parity" id="d-parity" title="Two-dimensional parity" style="width:150px;" />
<figcaption>Two-dimensional parity</figcaption></figure>



<h3 id="two-dimensionalparity">Two-dimensional parity</h3>

<p>The use of parity is based on the assumption that errors are rare and multi-bit errors are even more rare.
We can arrange a sequence of bits into a two-dimensional M &times; N array. Then we generate a parity bit for each row
and another one for each column. Finally, we generate a parity bit for the intersection of the row and column
parity bits. To validate data, we check the parity along each row and each column. If there is a single bit error in
the data, the intersection of the row and column with parity errors will identify the bad bit. </p>

<p>Two-dimensional parity, which can be easily extended to <em>n</em> dimensions, is a simple example of
an <strong>error correcting code</strong> (<strong>ECC</strong>). Error correcting codes were pioneered by Richard Hamming
in 1950 and there are numerous such codes, multi-dimensional parity being one of the simplest.
A data transmission that uses error correcting codes is said to use <strong>forward error correction</strong> (<strong>FEC</strong>).</p>

<h3 id="checksums">Checksums</h3>

<p>A <strong>checksum</strong> is any form of code that is uses the data to compute a small, fixed-size value that
is sent along with the data and is used for error checking. The receiver can apply the same computation
to validate the checksum. Any bit errors in the data will yield a high probability that the computed
checksum will be a different value.
We already examined a simple checksum earlier. Protocols such as IP, UDP, TCP, ICMP, OSPF,
and IGMP all use the <strong>Internet checksum</strong>. In this checksum, we summed up the data 16 bits
at a time, adding one whenever the sum of two values resulted in a carry (overflow), and inverting the
bits of the result. The Internet checksum is extremely easy to compute put provides
poor detection of errors in the data. </p>

<h3 id="cyclicredundancycheck">Cyclic redundancy check</h3>

<p>A <strong>cyclic redundancy check</strong> (<strong>CRC</strong>) is a much more robust checksum that is particularly good
at detecting multiple consecutive bad bits. An <em>n</em> bit CRC code will generally detect a burst of
up to <em>n</em> bad bits.
CRC codes are a type of code known as a linear block code. A block code treats the data
as a sequence of fixed-length integers. The Internet checksum is also a form of a block code.
A CRC is a bit more computationally intensive to compute than
an Internet checksum but, when done at the MAC layer, is generally done by the hardware of the
adapter, and therefore does not take time from the processor.</p>

<figure>
<img src="images/crc.png" alt="CRC computation. G=10111, CRC=1011" id="crc" title="CRC computation" style="width:200px;" />
<figcaption>CRC computation. G=10111, CRC=1011</figcaption></figure>



<p>A CRC computation is similar to long division but with the use of exclusive-ors instead of
subtraction.
The entire data to be checksummed is treated as a large binary number that is
left-shifted by the desired length of the CRC code (e.g., <em>r</em> bits).
This number is &#8220;divided&#8221; by a value called the <strong>generator</strong> (abbreviated by <strong>G</strong>).
The generator is pre-defined and agreed to by both sides.
For an r-bit CRC code, the generator must be <strong>r + 1</strong> bits long,
start with a 1 and be an odd number (end with a 1).
The &#8220;division&#8221; is a series of exclusive-ors on the data, aligning G
with the leftmost 1 in the remaining data, exclusive-oring the result,
and repeating until there is no more room to position <em>r</em> bits under
the data. What&#8217;s left is the <strong>remainder</strong>, which is the <strong>CRC</strong> value.</p>

<p>The figure on the right shows a CRC computation with a Generator of 23
(10111) yielding a remainder of 11 (1011), which becomes the checksum.
The data is sent with the CRC number in place of the 0 bits that filled
the data on the right when we left shifted the data by <em>r</em> bits.
The recipient performs the same division without left shifting the value it receives.
If the remainder is 0, then this indicates that there is no data
corruption.</p>

<h2 id="multipleaccessprotocols">Multiple access protocols</h2>

<p>A <strong>multiple access protocol</strong> is MAC protocol that is used on a network
where multiple hosts need to send messages on the same physical link (e.g.,
the same wire or range of radio frequencies). These types of links are
<strong>broadcast links</strong> since multiple hosts are connected to the same medium.
The <strong>multiple access problem</strong> is that of coordinating transmissions
from multiple hosts to avoid collisions. A <strong>collision</strong> is when
two or more hosts transmit at the same time on the same frequency,
causing one transmission to interfere with the other, and damaging both
signals. There are three strategies for handling this.</p>

<h3 id="channelpartitioning">1. Channel partitioning</h3>

<p>Channel partitioning means dividing a communication channel either
into fixed time slots or fixed frequency bands. <strong>Time division multiplexing</strong>
(<strong>TDM</strong>) divides a channel into time slots. For <em>n</em> hosts, each host
gets 1/ <em>n</em> of the time slots. A host can transmit only during its
defined time slot. <strong>Frequency division multiplexing</strong> (<strong>FDM</strong>) divides
a channel into <em>n</em> frequency bands. A host can transmit at any time but
can only transmit on its allotted frequency band. As with TDM, this
channel partitioning scheme does not make efficient use of the bandwidth.
Frequency bands or time slots go wasted if the nodes to which they are
allocated have nothing to send.</p>

<h3 id="takingturns">2. Taking turns</h3>

<p>MAC protocols based on taking turns allow each node to have full use
of the network and coordinate access to make sure that no other
node will be using the network. This ensures that collisions cannot
occur.</p>

<p>A <strong>polling protocol</strong> uses a master that polls each of the nodes on
the network, each in turn, to see if one of them wants to transmit
data. This ensures that there are no collisions as the master will
not poll another node until transmission is complete. However,
a node incurs the delay of waiting to be polled (think about a LAN
with thousands of nodes on it). Also, if the master dies, the network
becomes inoperable.</p>

<p>A <strong>token passing protocol</strong> uses a special frame, called a <strong>token</strong>,
that is passed around the nodes on the network in sequence.
If a node gets a token, then it can transmit data. Once it is done
transmitting, it passes the token to its neighbor. If it has nothing
to transmit, it passes the token to its neighbor immediately.
Unlike a polling protocol, there is no need for a master. However,
it suffers from some of the same problems. A node has to wait for
the token and, should a node fail to pass the token, no other node
will be able to transmit.</p>

<h3 id="randomaccess">3. Random access</h3>

<p>With random access protocols, any node has full use of the channel
but only one node at a time may use it. There are no scheduled time slots
as in TDM. Because there is no schedule on who can transmit when,
random access protocols have a chance of collision when multiple nodes
decide to transmit at the same time.</p>

<p>In the <strong>Slotted ALOHA</strong> protocol, access to the network is divided into
time slots. Time slots are not assigned to any node.
A node can transmit at the start of any time slot and must finish
transmission at the end of that slot. If two nodes happen to transmit
during the same time slot, a collision results and both nodes will
have to retransmit the frame. Each of them retransmits it in the
next time slot with some predefined probability <em>p</em>. Imagine the decision
is made by a
node flipping a weighted coin to decide whether to retransmit in the
next time slot. If the flip tells it not to retransmit, it makes the
same decision for transmitting on the time slot after that (and so on).
Slotted ALOHA introduced the concept of a randomized wait after
a collision but it does not use the network very efficiently. For
a large number of transmitting nodes, only about 37% of time slots
will have useful data. The rest will be empty or have collisions.</p>

<p><strong>Carrier Sense Multiple Access with Collision Detection</strong> (<strong>CSMA/CD</strong>)
is a successor to Slotted ALOHA and is the protocol used on Ethernet on a
shared link (which was the design of the original Ethernet).
CSMA/CD has two parts to it: <strong>carrier sensing</strong> means that a node
will not just transmit when it is ready to do so but will first listen
to the communication channel and wait until it is clear (i.e., nobody
else is transmitting); <strong>collision detection</strong> means that a
node is listening to the channel at the same time that it is
transmitting. If it detects interference (a collision), it immediately
stops transmitting.</p>

<p>After a node detects a collision and stops transmitting, it will
have to retransmit that frame. Before doing so, it will wait a
random interval. The desired heuristic is to pick a random wait time
from a long time interval if there is a lot of traffic on the network
and to pick a random wait time from a short time interval if there is
little traffic on the network. Of course, a network adapter has no
idea what is going on in the rest of the network. Instead, each
time the transmission of a frame results in a collision,
a random backoff value will be chosen from a time interval that is
double the previous time interval. This technique is called
<strong>binary exponential backoff</strong>.</p>

<h2 id="ethernet">Ethernet</h2>

<p>Ethernet was designed as a random access packet switched network
that uses CSMA/CD over a shared wire. It has been evolving since
the the mid&#8211;1970 and continues to evolve. Not only did it get
faster (from 2.74 Mbps to 10 Gbps and beyond) but it moved from
a shared bus topology to a switched star topology, where every
node is connected by a dedicated cable to a central switch.</p>

<p>There are slight variations on the format of an Ethernet frame
depending on which version of the protocol is used but the
key parts of an <strong>Ethernet frame</strong> are:</p>

<ul>
<li>MAC destination address</li>
<li>MAC source address</li>
<li>type, indicating the higher level protocol encapsulated within the data field (e.g., an IP datagram). It
also identifies the version of the Ethernet protocol used in the frame.</li>
<li>payload: the data</li>
<li>CRC checksum</li>
</ul>

<p>Surprisingly, the frame length is missing in the most common version (Ethernet II)
since the end of the frame is recognized by the hardware when the carrier signal drops.
The packet driver on the adapter counts bytes as they arrive and stops when the
receiver hardware has no more bytes to receive.</p>

<h3 id="addressresolutionprotocolandneighbordiscoveryprotocol">Address Resolution Protocol and Neighbor Discovery Protocol</h3>

<p>Ethernet addresses are 48-bit addresses that are globally unique. They
are generally assigned by the manufacturer of the adapter.
These MAC addresses are unrelated to IP addresses. Any IP datagram
needs to be encapsulated in an Ethernet frame before it can be transmitted
on an Ethernet network. The MAC address in this frame is the address of the
<strong>next hop</strong> for that datagram.
The <strong>Address Resolution Protocol</strong> (<strong>ARP</strong>)
allows a host to find a MAC address that corresponds to a desired IP address.</p>

<p>If a host needs to send a datagram to a system on its
subnetwork, it must encapsulate the datagram in an Ethernet
frame whose destination address is the MAC address of the recipient.
If a host needs to send a datagram to a system that is not on its
subnetwork, it needs to look up the destination IP address in its routing
table and encapsulate the datagram in an Ethernet
frame whose destination address is the MAC address of the first-hop router that
is responsible for that IP destination.</p>

<p>ARP maintains a cache of recently-used IP-MAC address mappings in an
<strong>ARP table</strong>. If the IP address it needs is not in the table, then
it broadcasts an <strong>ARP query</strong> that contains the desired IP address.
Every adapter on the LAN will receive it (it&#8217;s an Ethernet broadcast).
If one of the adapters owns that IP address, it responds directly to
the sender with an <strong>ARP response</strong> containing the corresponding MAC address.</p>

<p>IPv6 does not support ARP and uses a similar but more efficient mechanism
called the <strong>Neighbor Discovery Protocol</strong> (<strong>NDP</strong>). Every IPv6 host must listen to a special
multicast IP address that is derived from its own IP address. The
multicast Ethernet MAC address is, in turn, derived from that IP
multicast address. Hence, a query (called a <strong>neighbor solicitation</strong>)
will be sent as a multicast message and delivered only to those
hosts whose IP addresses result to the same Ethernet MAC address.
Most likely, there will only be one such address in a LAN. The benefit of
this method over ARP is that
every host on the LAN is not interrupted with a query.</p>

<h3 id="multicastaddressing">Multicast addressing</h3>

<p>As with IP, Ethernet defines a range of MAC addresses that are reserved for use as multicast addresses.
An Ethernet controller may, depending on the hardware, be configured to receive multicast frames in
several ways:</p>

<ul>
<li>If a hash of a MAC address matches a certain value, accept the frame.</li>
<li>If a MAC address matches one of several MAC addresses, accept the frame. This is usually limited to a small number of addresses.</li>
<li>Accept all multicast frames in <strong>multicast promiscuous mode</strong>.</li>
</ul>

<p>The link-layer software driver needs to be prepared to receive extraneous frames and discard them
if they contain destination addresses that the host is not interested in receiving.</p>

<p>While IP and Ethernet MAC addresses are completely unrelated on a host,
multicast addresses <em>are</em> related since there would otherwise be no way of identifying
the set of MAC addresses that need to receive a specific IP multicast datagram.
Each host needs to define a multicast MAC address that can be derived from the IP address
so that each receiver can listen on that address and each transmitter can transmit to that address.
Both IPv4 and IPv6 use a similar approach. A number of least-significant bits of the
IP multicast address (23 out of 28 bits for IPv4; 32 out of 112 bits for IPv6)
are copied onto a defined MAC multicast address. The IP driver will need to discard
any extraneous multicast packets that arrive because the lower bits of the multicast
address happened to be the same.</p>

<h3 id="link-layerswitches:switchedethernet">Link-layer switches: Switched Ethernet</h3>

<p>Ethernet began as a shared network using a <strong>bus topology</strong>.
As coaxial cables gave way to twisted pair wiring (phone wires), adapters on
different hosts could no longer tap into the same cable. Ethernet moved
to a <strong>star topology</strong> where a separate cable connected each adapter to a
central point: an Ethernet hub.
An <strong>Ethernet hub</strong> simulated a shared Ethernet network by taking every bit
that was received on one interface and transmitting it onto every other interface.</p>

<p>Hubs evolved into <strong>switches</strong>. While switches look similar to hubs, they
are more intelligent and improve the performance of Ethernet networks
considerably. A switch essentially acts like a link-layer router.
It <strong>forwards</strong> frames received on one interface (connector) to another.
Unlike a router, a switch is <strong>self-learning</strong>. It learns which MAC addresses
correspond to which interfaces on the switch by looking at the
<em>source address</em> of frames that arrive on each interface. This mapping of
MAC addresses to interfaces is stored in a <strong>forwarding table</strong> (also called
a <strong>switch table</strong> or a <strong>MAC table</strong>).
Because switches may be connected to each other, multiple MAC addresses
may map to a single interface on a switch (that interface happens to connect
to another switch). If a switch receives a frame with a destination
address that is missing from its table, it simply sends a copy of that frame out onto all
interfaces.</p>

<p>The biggest benefit from switches is that collisions can no longer occur.
A link between a switch and a host is <strong>full duplex</strong>: frames can be
sent concurrently with frames being received. The switch itself will
queue and forward frames (this leads to the possibility of buffer overflow, but not collision).
Adapters operating in full duplex mode have no need to do collision detection.</p>

<h2 id="virtuallocalareanetworksvlans">Virtual Local Area Networks (VLANs)</h2>

<p>Some Ethernet switches can be configured to act as multiple logical, or virtual, switches,
thereby creating several distinct local area networks.
These networks are called <strong>Virtual Local Area Networks</strong> (<strong>VLANs</strong>) and they look and feel like
distinct LANs.
Each interface on the switch can be assigned, or reassigned, to one of these VLANs.
Operations such as broadcasts and link-layer multicasts will remain within
a single VLAN.</p>

<p>VLANs can be extended geographically or in capacity by connecting multiple
switches together. Instead of running a cable for each VLAN between the switches, a single
cable may be used to connect the switches to handle all the traffic that flows on all of
the VLANs. This is called <strong>VLAN trunking</strong>. To identify which VLAN an Ethernet frame
belongs to, the Ethernet frame is augmented with a VLAN tag. This tag is removed
when the frame is switched to a non-trunk interface.</p>

<p>With VLAN switches, an administrator can define which VLAN any device resides on, regardless of
which switch it is connected to. This allows the administrator to segment groups of computers
(e.g., accounting vs. development vs. test) without having to connect each group to its own
dedicated switch.</p>

<h1 id="wirelessnetworks">Wireless Networks</h1>

<p>A <strong>base station</strong> is a device that sends and receives data to and from wireless hosts.
It may also coordinate transmission among hosts, with directives on who can transmit,
when, and what signal strength. The base station generally interfaces to other, usually wired,
networks thereby connecting the wireless devices with the wider Internet. Examples
of base stations are cell towers and wireless access points.</p>

<p>A wireless device, called a <strong>station</strong> may operate in <strong>infrastructure mode</strong>, in which case traditional
network services such as DHCP, DNS, and routing as well as connectivity to the Internet
are expected to be provided through the base station and the wired network to which it connects.
Alternatively, hosts may operate in <strong>ad hoc mode</strong>, also known as <strong>peer-to-peer mode</strong>.
In this case, there is no base station or back-end infrastructure available.
Hosts are self-configuring and need to figure out how to communicate among themselves, which includes
assigning unique addresses, discovering names, and deducing routes to each other.</p>

<h2 id="">802.11</h2>

<p><strong>802.11</strong>, also known as <strong>Wi-Fi</strong>, is a set of standards for wireless local area networking.
Standards such as 802.11a, 802.11b, 802.11g, 802.11n, and 802.11ac define operating frequencies and
data encoding techniques used for sending and receiving frames on a wireless LAN. Most
802.11 systems operate in either the 2.4 or 5 GHz frequency bands. </p>

<p>An 802.11 base station is known as an <strong>access point</strong> (<strong>AP</strong>). The collection of an access point
and one or more mobile wireless devices (stations) is called a <strong>basic service set</strong> (<strong>BSS</strong>).
Each BSS has a unique ID, called the <strong>BSSID</strong>, which happens to be the MAC address of the
BSS&#8217;s access point. Devices that interact with an access point operate in <strong>infrastructure mode</strong>.
The access point connects to a wired Ethernet infrastructure. 802.11 devices can also operate
in <strong>ad hoc mode</strong>, where devices communicate directly with each other with no need of an access point.</p>

<p>Each access point, in addition to having a BSSID, has a <strong>Service Set Identifier</strong> (<strong>SSID</strong>), which is
a human-friendly text name that is assigned by the administrator who configures the AP. Each BSS operates
over a range of frequencies identified as a <strong>channel</strong>. Adjacent channels partly overlap with each other.
For example, in the U.S., the allowable non-overlapping channels in the 2.4 GHz band
are 1, 6, and 11.
If adjacent BSSes use these
distinct channels, then frames sent by a device in one BSS will never interfere with those in another BSS.</p>

<p>A station (a wireless endpoint device)
needs to find and associate itself with an AP. Two techniques are available for this.
With <strong>passive scanning</strong>, the AP periodically sends <strong>beacon frames</strong>, each containing the AP&#8217;s
SSID and MAC address (BSSID). The beacon is typically sent every 100 ms.
The station scans all channels, searching for beacon frames
from any APs in the area.</p>

<p>With <strong>active scanning</strong>, a station may broadcast a <strong>probe request frame</strong>.
It sends this to a broadcast address (ff:ff:ff:ff:ff:ff) and sets a timer to wait for
any answers. If no answer has been received at the end of the time period, the
station moves to the next channel and repeats the process.
APs that receive the probe will respond with a <strong>probe response frame</strong>, that contains their identity,
supported data rates, and other information. A station then selects an access point with
which to associate. This may be done by the user or programmatically (e.g., the AP with the strongest
signal or one that has been seen in the past). The station sends an <strong>association request</strong> frame,
receives an <strong>association response</strong> from the AP and is now part of that AP&#8217;s BSS. It can
then send a DHCP discovery message to configure itself on the IP network.</p>

<h2 id="mac">802.11 MAC</h2>

<p>There are a couple of fundamental differences between 802.11 and Ethernet, apart from one
being wireless and the other being wired. Operating in a wired medium results in
considerably higher bit-error rates. Also, while an Ethernet transceiver, when operating on a shared
medium, can listen while transmitting, an 802.11 transceiver cannot. Because of this,
Ethernet was able to perform collision detection and stop transmitting the instant that
it detected a collision. 802.11 cannot do so and will transmit a complete frame, even
if it gets garbled in the air.</p>

<h3 id="csmaca">CSMA/CA</h3>

<p>To deal with the inability to detect collisions, 802.11 employs a random access MAC protocol called
<strong>carrier sense multiple access with collision avoidance</strong> (<strong>CSMA/CA</strong>). The key
part of CSMA/CA is that, if stations sense a busy channel, they all perform a
random backoff and then try again.
Collisions are likely to occur if multiple stations wait for a busy channel to become clear
and then start to transmit at the same time. To avoid this situation, CSMA/CA tells the stations to wait a random time
after they sensed a busy channel became clear, and then transmit the frame.
The random time counter counts down <em>only</em> when the channel is sensed to be clear;
it pauses whenever a signal is detected on the channel.
The idea is that stations pick different random values and
when they are ready to try transmitting again, they will not transmit at the same time:
somebody will end up going first, causing the channel
to be busy for others.</p>

<h3 id="arq">802.11 ARQ</h3>

<p>Since 802.11 cannot listen while transmitting to detect a collision and because
there is a higher likelihood of data corruption on a wireless network,
802.11 adds an ARQ protocol (acknowledgements and retransmissions).
After a node sends a frame, the receiver (AP or station) validates the CRC in the
frame and sends back an acknowledgement frame. If the sender does not receive the
acknowledgement, it increases its backoff value, counts down a random interval
for the channel to be free to transmit, and retransmits the frame.
Like CSMA/CD, 802.11&#8217;s CSMA/CA uses <strong>binary exponential backoff</strong>.
Because 802.11 uses an ARQ protocol, the 802.11 frame contains a sequence
number to allow a receiver to detect duplicate frames.
After a certain number of retransmissions, the sender gives up, so reliable delivery
is not assured.</p>

<p>Because of the higher likelihood of errors and the time expense of retransmission, 802.11n
and 802.11ac systems also support the optional use of an error correcting code
(low-density parity check code, <a href="https://en.wikipedia.org/wiki/Low-density_parity-check_code">LDPC</a>).
This is in addition to the CRC error-detection code.</p>

<h3 id="rtscts">802.11 RTS/CTS</h3>

<p>A unique problem in wireless networks is that a station does not necessarily
know if a channel is busy or not because it can be out of radio range of
another station that is transmitting while both stations are in range of
an access point. This is called the <strong>hidden node problem</strong>. In this case,
a station may transmit when it thinks the channel is clear, not realizing
that a transmission was already taking place between the AP and another
station. This is unavoidable but is ameliorated via the optional use
of <strong>RTS/CTS</strong> (<strong>Request to Send / Clear to Send</strong>).
Before transmitting a frame, a station sends a <strong>Request to Send</strong> (<strong>RTS</strong>)
message to the AP, asking to reserve the communication channel for
a message of a specific size. The AP then responds with a
<strong>Clear to Send</strong> (<strong>CTS</strong>) message, giving it permission to send
the frame. The CTS message is <strong>broadcast</strong> so that all stations will
get the message and know that they should not transmit anything during
that interval. RTS and CTS frames are short and sending them reduces
the chance of collision compared to sending a long data frame. The
RTS/CTS mechanism serves as an extension of carrier sensing. It allows
a station to be informed that a channel will be busy even if it may not
have the ability to pick up the signal from the station that will be
transmitting the data.</p>

<h3 id="addressesandhostmigration">802.11 addresses and host migration</h3>

<p>The 802.11 frame is conceptually similar to an Ethernet frame. In fact,
they share the same MAC addressing scheme so that Ethernet addresses interoperate
with 802.11 addresses. The access point serves as a bridge between the two
protocols and converts 802.11 frames into Ethernet frames when they need
to go on an Ethernet link and vice versa.
One distinction between Ethernet and 802.11 frames is that an 802.11 frame
has <strong>four address fields</strong>, three of which are used in infrastructure mode.
One address identifies the wireless destination. Another identifies the
wireless source. In cases where the frame is routed between the wireless
and wired network, the third address identifies the MAC address of the
wired device (regardless of whether it is a sender or a receiver).
The reason for three addresses is that, unlike an Ethernet switch,
an AP has a distinct MAC address and is addressed as an intermediate destination
both for frames between wireless stations and for frames that go between
and devices on the wired network. If a frame needs to go to the wired network,
a station will address it to the AP, which will then create an Ethernet MAC
frame that is addressed to the targeted wired destination.</p>

<p>A station is part of a BSS. That means it is associated with a single access point.
To extend the range of a wireless LAN, multiple access points may be deployed
that share the same SSID. A device can dissociate itself from one AP and
associate itself with another one when it detects a stronger signal from the
new one. This is called <strong>host migration</strong>.
Since the APs are connected to the same LAN via the same switch (or
a set of connected switches), there is no problem with the station keeping
the same IP address and any state of TCP session. The challenge is to
get the Ethernet switch to immediately route frames to the latest access point
that the station has joined. To support host migration at the switch,
an access point broadcasts an Ethernet frame that contains the migrated
host&#8217;s source MAC address. The switch, being self-learning will immediately
update its forwarding table, associating the device&#8217;s MAC address with
that of the interface on which it arrived.</p>

<h3 id="powermanagement">Power management</h3>

<p>Most wireless devices are battery powered and power consumption is a
key design factor. A transceiver on a wireless device can quickly
go to sleep and wake up at a preset interval. Before a device
puts its 802.11 transceiver to sleep, it sends a message to the AP
stating that it is going to sleep (this is a bit in the MAC header).
Upon receiving this message, the AP will queue but not send any
frames targeted for that device. The device&#8217;s transceiver is programmed
to wake up before the AP is scheduled to send its next beacon frame
(which the AP does typically every 100ms). When the AP sends a
beacon frame, it sends a list of MAC addresses of stations that
have <strong>buffered frames</strong> (queued frames). The station will then
wake up and request the frames via a polling message. Otherwise,
it can go to sleep until the next beacon.</p>

<h1 id="qualityofserviceinnetworks">Quality of Service in Networks</h1>

<p>IP was designed as
a system that would provide best-effort packet delivery but with no guarantees
on the path a packet will take, whether it gets dropped, or what order it
arrives in. Hence, there is no concept of delivering any specific grade of service.
As IP networks began to be used for carrying continuous
media, such as voice and data,
several approaches were taken to attempt to provide better controls
for scheduling the delivery of IP packets.</p>

<p><strong>Quality of service</strong> (<strong>QoS</strong>) on a network is characterized by four factors:</p>

<ol>
<li><strong>Bandwidth</strong> (or <strong>bit rate</strong>): the average number of bits per second over the network</li>
<li><strong>Delay</strong> (or <strong>latency</strong>): the average time for data to get from one endpoint to another</li>
<li><strong>Jitter</strong>: the variation in the delay</li>
<li><strong>Errors</strong> (or <strong>packet loss</strong>): the percentage of packets that do not reach their destination or reach it with errors</li>
</ol>

<p>Quality of service problems arise because we do not have unlimited resources.</p>
<dl>
<dt><strong>Congestion</strong></dt>
<dd>Congestion arises when more packets are flowing into a router than can flow out.
It is largely due to <strong>bandwidth mismatch</strong>, such as having a 10 Gbps network routing traffic
to a 1 Gbps link, or to <strong>aggregation</strong>, such as having four 1 Gbps links all sending traffic
that needs to be routed over a single 1 Gbps link. If a router gets packets at a faster rate
than it can route, the result will be <strong>packet loss</strong>.
The router may also choose to block the arrival of new packets.</dd>

<dt><strong>Latency</strong></dt>
<dd>Latency is due to not only the <strong>propagation delay</strong> of transmitting data
on a network but to various delays along the route. The sending host spends time in <strong>packetization</strong>, the
creation of packets that need to be transmitted. <strong>Serialization</strong> is the queuing of packets are for transmission onto the
network since they can only go out one at a time. Each router incurs a processing delay due to
inspecting the packet and moving it onto the appropriate output queue. Then there is the <strong>queuing delay</strong>
of waiting for the router to schedule the packet for transmission. Once it reaches the destination, there is the
processing delay of depacketizing the packet and moving it through the network stack, delivering the data to the application,
and scheduling the process to consume it.</dd>

<dt><strong>Jitter</strong></dt>
<dd>If multiple packets arrive to a router or switch at approximately the same time
but need to be transmitted over a single link, they need to be queued since only
one can be sent at a time. Some packet will be first and some packet will be last.
This variation in dequeuing time creates jitter. Jitter is also caused by
having some packets take a different route than others and, most significantly, by the retransmission of
lost packets.</dd>

<dt><strong>Packet loss</strong></dt>
<dd>Packets can be corrupt or lost due to collision, interference, and signal degradation in the network.
The most likely cause, however, is that of a router simply dropping packets because a
queue is full. This condition is known as <strong>buffer overrun</strong>.</dd>
</dl>


<h2 id="achievingqualityofservice">Achieving quality of service</h2>

<p>Quality of service on a network is achieved via <strong>resource allocation</strong> and <strong>packet prioritization</strong>
Network service quality can be strictly enforced or may be attempted via a best-effort approach.
<strong>Hard QoS</strong> refers to a system that is designed to provide a guaranteed level of
service via reservations while <strong>soft QoS</strong> refers to a system that uses a best-effort approach
to try to deliver, but not guarantee, the desired level of service.</p>

<p>Two broad approaches to providing data channels with managed quality of service on a
network are <strong>admission control</strong> and <strong>traffic control</strong>.
Admission control is generally designed to enforce hard QoS.
With <strong>admission control</strong>, we ask that applications first request a particular quality of
service from the network. The network (i.e., all the routers in the path from the source
to the destination) will reserve resources for switching and routing the packets to
conform to that grade of service and grant the request to the application. If any router cannot
commit to the needed resources, the request will be denied. </p>

<p><strong>Traffic control</strong> provides soft QoS.
Soft QoS on a network refers to prioritization of packets
without any reservation of resources from routers or endpoints or
any <em>a priori</em> negotiation for a level of service.
With <strong>traffic control</strong>, applications may send data onto the network freely
but the network elements (routers) will classify, queue, schedule, and sometimes drop
packets based on various criteria.</p>

<p>A <strong>link scheduling discipline</strong> defines how packets are scheduled at the output queue.
When we looked at router architecture, we considered only
a single queue per output
interface with packets transmitted in a <strong>first-in-first-out</strong> (<strong>FIFO</strong>) manner.
This is the simplest approach but does not offer any opportunity for differentiating
one datagram from another based on its service class.</p>

<p>A router can set up multiple queues for an output link and place different
classes
(e.g., based on addresses, ports, or other fields in the IP header)
of packets onto different queues. A packet scheduler then picks
the next packet from a specific queue for transmission. A <strong>round-robin
scheduler</strong> will give each class of packet (queue) equal priority.
A <strong>priority scheduler</strong> will always service high-priority queues first but can lead to
<strong>starvation</strong> of low-priority queues: low-priority queues might never get serviced.
A desirable characteristic of queue management is <strong>traffic isolation</strong> - to ensure
that one class of service cannot adversely affect another class even if it has a higher
A <strong>weighted fair queuing</strong> (<strong>WFQ</strong>) approach gives each queue a priority level but also a
ensures a certain minimum percentage of the available link bandwidth so that
starvation does not occur.</p>

<h3 id="trafficshapingandtrafficpolicing">Traffic shaping and traffic policing</h3>

<p><strong>Traffic shaping</strong> is when a router queues packets in certain flows during
peak usage for later retransmission when there is available bandwidth. With
<strong>traffic policing</strong>, traffic that exceeds the allocated bandwidth for
a particular flow is discarded.</p>

<p>A <strong>leaky bucket</strong> is a traffic shaping algorithm that coverts a bursty flow into
a constant bitrate flow: it removes jitter. The bucket is represented by a queue that
receives data at an irregular rate (in bursts).
The queue is emptied at a constant rate (the hole at the bottom of the bucket),
resulting in an absence of jitter.
If there is nothing left to read in the queue, we have a <strong>buffer underrun</strong> and jitter occurs.
If the queue is already full when data arrives, we have a <strong>buffer overrun</strong> and packet loss occurs.</p>

<p>A <strong>token bucket</strong> is a &#8220;bucket&#8221; that holds tokens that are generated at
a constant rate. In order to transmit a packet, the bucket must be drained of
the number of tokens that is proportionate to the packet&#8217;s size. The token bucket algorithm
does not smooth out traffic like the leaky bucket does but ensures that an average bitrate
is enforced for a specific flow. For example, a buildup of tokens can result in
a burst of data.</p>

<h3 id="diffserv">DiffServ</h3>

<p><strong>Differentiated services</strong> (<strong>DiffServ</strong>) is a way for programmers to provide advisory information inside
an IP header on how a packet should be processed by routers. A packet can be
assigned a specific service code by setting a value in the 6-bit <strong>Differentiated Services Codepoint</strong> (<strong>DSCP</strong>)
field of the IP header.
DiffServ is an example of <strong>traffic classification</strong>.
It is entirely up to the routers to decide how to process this
information (e.g., via WFQ), what the service levels mean,
or even whether to process it at all. Differentiated services are an example
of <strong>soft QoS</strong>: there is no guarantee on the actual quality of service
that will be delivered. A common use for DiffServ is to try to provide a better quality
of service for voice over IP (VoIP) phone calls by tagging those packets for
Expedited Forwarding (EF) service, which
is defined to have the characteristics of low delay, low loss, and low jitter.
Whether DiffServ values are used at all and how they are interpreted is
strictly up to the ISP.</p>

<h3 id="intserv">IntServ</h3>

<p><strong>Integrated Services</strong> (<strong>IntServ</strong>) is
an approach that relies on end-to-end reservation of services.
A transmitting host specifies its traffic (as a token bucket with a given rate and size)
and requested level of service guarantee.
Integrated Services system relies on
the <strong>Reservation protocol</strong>, <strong>RSVP</strong>, which has been
developed to allow a flow of packets to be routed with bitrate
guarantees. RSVP is an example of a <strong>soft state protocol</strong>, meaning that
state is not maintained perpetually but reservations expire unless they
are refreshed periodically.</p>

<p>The problem with guaranteeing this is that all routers in the path
from the source to the destination must be configured to support RSVP: each
intermediate router must commit to reserving the needed amount of routing
resources to guarantee the desired level of service. Integrated Services is an example of
admission control while differentiated services is an example of traffic control.
If one ISP in the path does not support this then all bets are off.</p>

<h2 id="rtpandrtcp">RTP and RTCP</h2>

<p>The <strong>Real-Time Protocol</strong> (<strong>RTP</strong>) is an application-level protocol on top
of UDP. It does not define any mechanisms for data delivery or QoS control. As with
any UDP datagrams, neither in-order delivery nor delivery in general is assured.
What RTP does is allow a sender to attach timestamps to packets so that a
receiver can play them back at the same rate at which they were sent. </p>

<p>An RTP header contains:
- <strong>payload type</strong>: identifies type of video or audio encoding. An application can change the encoding type mid-stream (e.g., to switch to a lower bandwidth codec, for example)
- <strong>sequence number</strong>: app can detect missing packets &amp; conceal data loss
- <strong>timestamp</strong>: app can play back data at appropriate intervals
- <strong>source ID</strong> of stream: uniquely identifies stream to allow demultiplexing multiple streams that may received at the same port.</p>

<p>RTP is widely used for voice and video, particularly for media transport in SIP (Session Initiation Protocol) systems.</p>

<p>The <strong>RTP Control Protocol</strong> (<strong>RTCP</strong>) is a companion protocol to RTP and is used to provide feedback
from the receiver to the sender. By comparing the arrival time between packets with the difference in timestamps in
the RTP header, a receiver can compute jitter. By checking sequence numbers, a receiver can compute packet loss.
The receiver can periodically send this feedback through RTCP so that the sender can make adjustments
to the data stream (for example, change to a lower bandwidth codec).</p>


<h1 id="firewalls">Firewalls</h1>

<p>A <strong>firewall</strong> protects the junction between an untrusted
network (e.g., external Internet) and a trusted network (e.g., internal network).
Two approaches
to firewalling are <strong>packet filtering</strong> and <strong>proxies</strong>.
A <strong>packet filter</strong>, or <strong>screening router</strong>,
determines not only the route of a packet but whether the packet
should be dropped based on contents in the IP header, TCP/UDP header,
and the interface on which the packet arrived. It is usually implemented
inside a <strong>border router</strong>, also known as the <strong>gateway router</strong> that
manages the flow of traffic between the ISP and internal network.</p>

<p>The packet filter evaluates a set of rules to determine whether to <strong>drop</strong>
or <strong>accept</strong> a packet. This set of rules forms an <strong>access control list</strong>,
often called a <strong>chain</strong>. Strong security follows a <strong>default deny</strong> model,
where packets are dropped unless some rule in the chain specifically permits them.
With <strong>stateless
inspection</strong>, a packet is examined on its own with no context based
on previously-seen packets.
<strong>Stateful inspection</strong> allows the router to keep track of TCP connections
and understand the relationship between packets. For example, a port that
needs to be enabled for the FTP data channel once an FTP connection
has been established or that return packets should be allowed into the
network in response to outbound requests.</p>

<p>Packet filters traditionally do no look above the transport layer.
<strong>Deep packet inspection</strong>
(<strong>DPI</strong>) allows a firewall to examine application data
as well and make decisions based on its contents. Deep packet inspection
can validate the protocol of an application as well as check for malicious
content such as malformed URLs or other security attacks.</p>

<p>An <strong>application proxy</strong> is software that presents the same protocol to
the outside network as the application for which it is a proxy.
For example, a mail server proxy will listen on port 25 and understand
SMTP, the <em>Simple Mail Transfer Protocol</em>. The primary job of the proxy is
to <strong>validate</strong> the application protocol and thus guard against protocol
attacks (extra commands, bad arguments) that may exploit bugs in the service. Valid requests are then regenerated
by the proxy to the real application that is running on another server and is
not accessible from the outside network.
The proxy is the only one that can communicate with the internal network.
Unlike DPI, a proxy may modify the data stream,
such as stripping headers or modifying machine names. It may also restructure
the commands in the protocol used to communicate with the actual servers (that is,
it does not have to relay everything that it receives).</p>

<p>A typical firewalled environment is a <strong>screened subnet</strong>
architecture, with a separate subnet for systems that run externally-accessible
services (such as web servers and mail servers) and another one for internal
systems that do not offer services and should not be accessed from the outside.
The subnet that contains externally-accessible services is called the <strong>DMZ</strong> (<strong>demilitarized zone</strong>).
The DMZ contains all the hosts that may be
offering services to the external network (usually the Internet).
Machines on the internal network are not accessible from the Internet.
All machines within an organization will be either in the DMZ or
in the internal network.</p>

<p>Both subnets will be protected by screening routers.
They will ensure that no packet
from the outside network is permitted into the inside network.
Logically, we can
view our setup as containing two screening routers:</p>

<ol>
<li><p>The <strong>exterior router</strong> allows IP packets
 only to the machines/ports in the DMZ that are offering valid
 services. It would also reject any packets that are masqueraded to
 appear to come from the internal network.</p></li>
<li><p>The <strong>interior router</strong>
 allows packets to only come from designated machines in the DMZ that
 need to access services in the internal network. Any packets
 not targeting the appropriate services in the internal network will
 be rejected. Both routers will generally allow traffic to flow from
 the internal network to the Internet, although an organization may
 block certain services (ports) or force users to use a proxy (for
 web access, for example).</p></li>
</ol>

<p>Note that the two screening routers may be easily replaced with
a single router since filtering rules can specify interfaces. Each rule can thus
state whether an interface is the DMZ, internal network,
or Internet (ISP).</p>

<p>A variation on screening routers is the use of <strong>intrusion detection systems</strong> (<strong>IDS</strong>).
A screening router simply makes decisions based on packet headers. Intrusion
detection systems try to identify malicious behavior. There are three forms of IDS:</p>

<ol>
<li><p>A <strong>protocol-based IDS</strong> validates specific network protocols for
conformance. For example, it can implement a state machine to ensure that messages
are sent in the proper sequence, that only valid commands are sent, and that replies match
requests.</p></li>
<li><p>A <strong>signature-based IDS</strong> is similar to a PC-based virus checker. It
scans the bits of application data in incoming packets to try to discern if there
is evidence of &#8220;bad data&#8221;, which may include malformed URLs, extra-long strings
that may trigger buffer overflows, or bit patterns that match known viruses.</p></li>
<li><p>An <strong>anomaly-based IDS</strong> looks for statistical aberrations in network activity.
Instead of having predefined patterns, normal behavior is first measured and used
as a baseline. An unexpected use of certain protocols, ports, or even amount of
data sent to a specific service may trigger a warning.</p></li>
</ol>

<h1 id="securecommunicationandauthentication">Secure communication and authentication</h1>

<h2 id="cryptography">Cryptography</h2>

<p>Cryptography deals with encrypting <strong>plaintext</strong> using
a <strong>cipher</strong>, also known as an <strong>encryption algorithm</strong>,
to create <strong>ciphertext</strong>, which is unintelligible to anyone
unless they can <strong>decrypt</strong> the message.</p>

<p>A <strong>symmetric encryption algorithm</strong> uses the same secret key
for encryption and decryption.</p>

<p>A <strong>public key algorithm</strong> uses a pair of keys: data encrypted with
the first key can be decrypted only with the second key and vice versa.
One of these keys is kept private (known only to the creator) and is known as
the <strong>private key</strong>. The corresponding key is generally made
visible to others and is known as the <strong>public key</strong>. Anything
encrypted with the private key can only be decrypted with the public
key. This is the basis for <strong>digital signatures</strong>
because the encryption can only be performed by the key&#8217;s owner.
Anything that is
encrypted with a public key can be encrypted only with the corresponding
private key. This is the basis for authentication and covert
communication because decryption can only be performed by the owner, who
is the only one who has the private key.</p>

<p>When data is transmitted, it is broken into blocks and each block is encrypted
separately. This leads to two problems. If different communication sessions
contain the same messages and use the same key, an intruder can see that the
same data is being sent. Secondly, a malicious party can add or delete, add, or replace
blocks (perhaps with random junk or perhaps with blocks that were captured
from previous communication sessions). <strong>Cipher block chaining</strong> (<strong>CBC</strong>) addresses
these problems. Every block of data is still encrypted with the same key. However,
prior to being encrypted, the data block is exclusive-ored with the previous
encrypted block. The receiver does the process in reverse: a block of received
data is decrypted and then exclusive-ored with the previously-received block to
obtain the original data. The very first block is exclusive-ored with a random
<strong>initialization vector</strong>, which must be transmitted to the remote side. Note
that CBC does not make the encryption more secure; it simply makes the result of
each block of data dependent on the previous block so that data cannot be inserted or deleted in
the message stream.</p>

<p>A <strong>cryptographic hash function</strong> is a <strong>one-way function</strong>
whose output is always a fixed number of bits for
any input. By one-way, we mean that there is no way to compute
the input when given the output.
For good cryptographic hash functions
(e.g., SHA&#8211;1, SHA&#8211;2, SHA&#8211;3),
it is highly unlikely that two messages will ever
hash to the same value, it is extremely difficult to construct
text that hashes to a specific value, and it is extremely difficult
to modify the plaintext without changing its resultant hash.
The hash function is the basis for message authentication
codes and digital signatures. Note that when we talk about
cryptography and mention phrases such as &#8220;<em>extremely difficult</em>&#8221;,
we mean &#8220;<em>impossible for all practical purposes</em>,&#8221; not that
&#8220;you can do it if you spend an entire week working on the problem.&#8221;</p>

<h2 id="securecommunication">Secure communication</h2>

<p>To communicate securely using a symmetric cipher, both parties need
to have a shared secret key. Alice will encode a message to Bob
using the key and Bob will use the same key to decode the message. If
Alice wants to communicate with Charles, she and Charles will also
need a secret key. The fact that every pair of entities will need
a secret key leads to a phenomenon known as <strong>key explosion</strong>.
Overall, in a system with <em>n</em> users, there will be
<em>O(n<sup>2</sup>)</em> keys.</p>

<p>The biggest problem with symmetric cryptography is dealing with key
distribution: how can Alice and Bob establish a key so they can
communicate securely? The <strong>Diffie-Hellman key
exchange</strong> algorithm allows one to do this. Each party will
generate a private key and a public key (these are <em>not</em>
encryption keys; they are just numbers &#8212; Diffie-Hellman does
not implement public key cryptography &#8212; it is unfortunate
that the term was used to describe these numbers). Alice can use
her private key and Bob&#8217;s public key to compute a
<strong>common key</strong>.
Bob can compute the same common key by using his private key and Alice&#8217;s public key.
They can then communicate securely by using the common key with a symmetric cipher.</p>

<p>Using true public key cryptography, such as <strong>RSA</strong>, if Alice encrypts a
message with Bob&#8217;s public key, Bob will be the only one who can
decrypt it since doing so will require Bob&#8217;s private key. Likewise,
Bob can encrypt messages with Alice&#8217;s public key, knowing that only
Alice will be able to decrypt them with her private key.</p>

<h2 id="sessionkeysandhybridcryptosystems">Session keys and hybrid cryptosystems</h2>

<p>A <strong>session key</strong> is a random key that is generated for encrypting data for
one communication session. It is useful because if the key
is ever compromised, no lasting information is obtained: future
communication sessions will use different keys. A <strong>hybrid
cryptosystem</strong> uses public key cryptography to send a session key
securely. The originator generates a random session key and encrypts
it with the recipient&#8217;s public key. The recipient decrypts the
message with the corresponding private key to extract the session
key. After that, symmetric cryptography is used for communication,
with messages encrypted with the session key. This has the advantages
of higher performance (public key cryptography is <em>much, much</em> slower than
symmetric cryptography), ease of communicating with multiple parties
(just encrypt the session key with the public keys of each of the
recipients), and allows the bulk of data to be encrypted with
session keys instead of the hardly-ever-changing
public keys.</p>

<h2 id="messageauthentication">Message authentication</h2>

<p><strong>Message Authentication Code</strong> (<strong>MAC</strong>) is a cryptographic hash of a message that
is encrypted with a shared symmetric key. This MAC is sent along with the message.
If an intruder modifies the message, the receiver will be able to validate that
the message no longer matches the MAC: simply hash the message and compare it with
the value of the decrypted MAC. An intruder cannot generate a new MAC because
you need the secret key to do so.</p>

<p>A <strong>digital signature</strong> is similar to a MAC but uses public key cryptography.
It is simply the hash of a message encrypted with the creator&#8217;s private key.
Anyone who has the message signer&#8217;s public key can decrypt the hash and thus validate
it against the message. Other parties, however, cannot recreate the signature.
Even though they can generate the same hash for the message, they do not have the
signer&#8217;s private key to encrypt that hash.</p>

<h2 id="publickeyauthentication">Public key authentication</h2>

<p>As we saw with hybrid cryptosystems, public key cryptography makes key exchange
simple. It also simplifies authentication. If Alice wants to authenticate herself
to Bob (prove that she&#8217;s really Alice), Bob will generate a random bunch of bits,
called a <strong>nonce</strong>, and ask Alice to encrypt the nonce with her private key
(something only she can do). She will send the result back to Bob who will decrypt
the data with Alice&#8217;s public key. If the result matches Bob&#8217;s original nonce, he
is convinced that Alice has Alice&#8217;s private key and therefore is really Alice.</p>

<h2 id="digitalcertificates">Digital certificates</h2>

<p>For Bob to validate Alice in the above example, Bob must be confident that he
really has Alice&#8217;s public key rather than someone else&#8217;s.
<strong>X.509 digital certificates</strong> provide a way to do associate an identity
with a public key and have some entity vouch for that association.
A certificate
is a data structure that contains user information and the user‚Äôs
public key. This data structure also contains a <strong>signature</strong>
of the <strong>certification authority</strong> (<strong>CA</strong>).
The signature is created by taking a hash of the rest of the data in the structure
and encrypting it with the private key of the CA.
The CA is responsible for setting policies of how
they validate the
identity of the person who presents the public key for encapsulation
in a certificate.</p>

<h2 id="transportlayersecuritysecuresocketslayer">Transport Layer Security (Secure Sockets Layer)</h2>

<p><strong>Secure Sockets Layer</strong>
(<strong>SSL</strong>, also known as <strong>TLS &#8212; Transport Layer
Security</strong>) is a layer of software above TCP at the application layer
designed to provide authentication
and secure communication while giving the programmer the feel of
a normal sockets interface.
It makes it easy to add a secure transport onto insecure TCP socket-based
protocols (e.g., HTTP and FTP). SSL uses a <strong>hybrid cryptosystem</strong> and
relies on public keys for authentication. If both the sender and
receiver have X.509 digital certificates, SSL can validate them and
use nonce-based public key authentication to validate that each party has the
corresponding private key. In some cases, it may validate the server
only. If the server does not have a certificate, SSL will then use
a public key simply to allow a symmetric session key to be passed
securely from client to server.
The client generates a session key and encrypts it with the server&#8217;s public key.
This ensures that only the server will be able to decode the message and
get the session key.
After that, communication takes
place using a symmetric algorithm and the client-generated session
key. Each encrypted message that is sent contains a MAC (message authentication code)
to allow the receiver to ensure that the message has not bee accidentally
or maliciously modified.</p>

<h1 id="vpns">VPNs</h1>

<p><strong>Virtual private networks</strong> (<strong>VPNs</strong>)
allow disconnected local area networks to communicate
securely over the public Internet, saving money by using a shared public network (Internet)
instead of leased lines. This is achieved by <strong>tunneling</strong>, the
encapsulation of an IP datagram within another datagram.
In this case, a
datagram that is destined for a remote subnet, which will often have local
source and destination IP addresses that may not be routable over the
public Internet, will be treated as payload and be placed inside a datagram
that is routed over the public Internet. The source and destination
addresses of this outer datagram are the VPN endpoints at both sides, usually the VPN-aware routers.</p>

<p>When the VPN endpoint (router) receives this encapsulated datagram, it extracts the
data, which is a full IP datagram, and routes it on the local area network.
This tunneling behavior gives us the <em>virtual network</em> part of the
VPN.</p>

<p>To achieve security (the &#8220;private&#8221; part of VPN), an administrator setting up a VPN will
usually be concerned that the data contents are not readable and
the data has not been modified.
To ensure this, the encapsulated packets can be encrypted and signed. Signing
a packet enables the receiver to validate that the
data has not been modified in transit. Encrypting ensures
that intruders would not be able to make sense of the data, which is the
encapsulated datagram.</p>

<p>VPNs usually provide several options for key management: shared
private keys (AES or 3DES), Diffie-Hellman key exchange, or RSA public keys.
IPsec is one of the most popular types of VPNs and has two variations.
<strong>Authentication Header</strong> (<strong>AH</strong>) mode adds a signature to the encapsulated datagram
but does not encrypt any data. Data is readable but cannot be modified.
Authentication Header mode is rarely used since the overhead of encrypting
data is quite low. The other variant is the <strong>Encapsulating Security Payload</strong> (<strong>ESP</strong>)
mode, which adds a signature as with AH but also encrypts the entire datagram.</p>

<p>In an environment where an individual computer outside a trusted network
needs to connect to another node securely, tunneling will not work since there
is no gateway router that will extract an embedded datagram and no trusted local area network
on which to route it.
To support this environment, VPNs can operate in <strong>transport mode</strong> instead
of <strong>tunnel mode</strong>. No tunneling takes place and hence there is no encapsulation of the full IP
datagram. The IP payload (which will include TCP or UDP headers) is encrypted and signed but the original
IP header is left unchanged. </p>


</div>

<div id="footer">
<hr/>
<style type="text/css">  
span.codedirection { unicode-bidi:bidi-override; direction: rtl; }  
</style>  

<p> &copy; 2003-2016 Paul Krzyzanowski. All rights reserved.</p>
<p>For questions or comments about this site, contact Paul Krzyzanowski, 
<span class="codedirection">gro.kp@ofnibew</span>
</p>
<p>
The entire contents of this site are protected by copyright under national and international law.
No part of this site may be copied, reproduced, stored in a retrieval system, or transmitted, in any form,
or by any means whether electronic, mechanical or otherwise without the prior written
consent of the copyright holder.
If there is something on this page that you want to use, please let me know.
</p>
<p>
Any opinions expressed on this page do not necessarily reflect the opinions of my employers and may not
even reflect my own.
</p>
<p> Last updated: May  6, 2016
</p>
<img class="stamp" src="../..//css/images/recycled_pixels_logo.png" alt="recycled pixels" height="80" width="80" />
</div> <!-- footer -->
<div id="tear">
</div>


<div id="sidebar1">
<h1 class="first">Contents </h1>
	<h2> CS 352 </h2>
	<ul>
	<li> <a href="../index.html"> Main course page </a> </li>
	<li> <a href="../news.html"> News </a> </li>
	<li> <a href="../syllabus.html"> Syllabus </a> </li>
	<li> <a href="../hw/index.html"> Homework </a> </li>
	<li> <a href="../notes/index.html"> Documents </a> </li>
	<li> <a href="../exam/index.html"> Exam info </a> </li>
	<li> <a href="../grades/index.html"> Check your grades </a> </li>
	<li> <a href="https://sakai.rutgers.edu/portal"> Sakai </a> </li>
	</ul>

	<h2> CS 352 background </h2>
	<ul>
	<li> <a href="../about.html"> About the course </a> </li>
	<li> <a href="../prereq.html"> Prerequisites </a> </li>
	<li> <a href="../things.html"> Things you need </a> </li>
	<li> <a href="../policy.html"> Policy  </a> </li>
	</ul>

		<h2> Study guides </h2>
	<ul>
	<li> <a href="../exam/study-guide-1.html"> Study Guide 1 </a> </li>
	<li> <a href="../exam/study-guide-2.html"> Study Guide 2 </a> </li>
	<li> <a href="../exam/study-guide-3.html"> Study Guide 3 </a> </li>
	</ul>


</div>

<div id="sidebar2">
<!--
<h1 class="first"> Free junk </h1>
<p>
This is some stuff I'm throwing away. Please send me mail if you want any of it:
</p>
<hr/>
<ul>
<li> 
</ul>
-->
</div>

</div>
</div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-8293152-1");
pageTracker._trackPageview();
} catch(err) {}</script>

</body>
</html>
