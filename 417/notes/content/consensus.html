<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title> Consensus </title>
<link href="../../../css/layout.css" rel="stylesheet" type="text/css" />
<link href="../../../css/main.css" rel="stylesheet" type="text/css" />
<link href="../../../css/print.css" rel="stylesheet" type="text/css" media="print" />
<link href="../../../css/main-print.css" rel="stylesheet" type="text/css" media="print" />
<style type="text/css">

#main table.doclist {
	width: 80%;
}
#main .doclist .date, #main .doclist .item {
        vertical-align: baseline; /* for opera */
}
#main .doclist tr {
        vertical-align: baseline;
}
#main .doclist th.item {
        text-align: left;
}
#main .doclist td.item {
        text-align: left;
}
#main a.linksign:link, #main a.linksign:visited, #main a.linksign a:hover {
        text-decoration: none;
}

</style>
</head>
<body id="s_ru417">
<div id="wrapper">
<!-- _______________________________________ BANNER _______________________________________ -->
<div id="banner">
  <div id="logo">
  <img src="../../../css/images/pk-org-pencil.png" alt="pk.org" name="logo" width="122" height="45"/>
  </div>
  <div id="title"> Distributed Systems </div>
  <div id="search">
  <form method="get" action="http://www.google.com/search">
	<div style="border:none ;padding:2px;width:25em;">
	<input type="text" name="q" size="25" maxlength="255" value="" />
	<input type="submit" value="Search" />
	<input type="hidden"  name="sitesearch" value="www.pk.org" checked />
	</div>
  </form>
  </div>
  <ul>
    <li class="separator"><a href="../../../about/index.html">About</a></li>
    <li class="separator"><a href="../../../about/contact.html">Contact</a></li>
    <li><a href="../../../sitemap.html">Site map</a></li>
  </ul>
</div>

<!-- _______________________________________ MAIN NAV _______________________________________ -->
<div id="navbar">
	<ul>
	<li class="homelink"><a href="../../../index.html">Home</a></li>
<!--
	<li class="aboutlink"><a href="../../../about/index.html">About</a></li>
-->
	<li class="ru"><a href="../../../rutgers/index.html">Rutgers</a></li>
	<li class="ru352"><a href="../../../352/index.html">Internet Technology [352]</a></li>
	<li class="ru416"><a href="../../../416/index.html">Operating Systems [416]</a></li>
	<li class="ru417"><a href="../../../417/index.html">Distributed Systems [417]</a></li>
	<li class="ru419"><a href="../../../419/index.html">Computer Security [419]</a></li>
	<li class="cslink"><a href="../../../cs/index.html">Computing</a></li>
	<li class="photolink"><a href="../../../photo/index.html">Photography</a></li>
<!--
	<li class="funlink"><a href="#">Coming</a></li>
	<li class="funlink"><a href="#">Soon</a></li>
-->
	</ul>
</div>

<div id="subnav">
<p>
You are in: 
<ul>
	<li class="first"> <a href="index.html"> Home </a> 
 	<li> <a href="../../index.html"> Rutgers CS 417 </a> 
 	<li> <a href="../../notes/index.html"> Documents </a> 
 	<li> <a href="../../notes/consensus.html"> Consensus </a> 
</ul>
</p>
</div>
<div id="content-wrapper">
<div id="main">
<div id="headline">
<h1> Consensus </h1>
<h2> Reaching agreement </h2>
<p class="author"> Paul Krzyzanowski </p>
<p class="date"> October 2011 </p>
</div>

<h1> Introduction </h1>

<p class="first">
<em>Consensus</em> is the task of getting all processes in a group to agree on some specific value based on the votes of each processes.
All processes must agree upon the same value and it must be a value that was submitted by at least one of the processes
(i.e., the consensus algorithm cannot just invent a value).

In the most basic case, the value may be binary (0 or 1), which will allow all processes to use it to make a decision
on whether to do something or not.
</p>

<p>
With election algorithms, our goal was to pick a leader. With distributed transactions, 
we needed to get unanimous agreement on whether to commit.
These are forms of consensus. 
With a consensus algorithm, we need to get unanimous agreement
on some value. 

This is a simple-sounding problem but finds a surprisingly large amount of use in distributed systems. Any algorithm that
relies on multiple processes maintaining common state relies on solving the consensus problem.

Some examples of places where consensus has come in useful are:
</p>
<ul>
<li> synchronizing replicated state machines and making sure all replicas have the same (consistent) view of system state. </li>
<li> electing a leader (e.g., for mutual exclusion) </li>
<li> distributed, fault-tolerant logging with globally consistent sequencing </li>
<li> managing group membership </li>
<li> deciding to commit or abort for distributed transactions </li>
</ul>

<p>
Consensus among processes is easy to achieve in a perfect world.  
For example, when we examined distributed mutual exclusion algorithms earlier, we visited a form of consensus
where everybody reaches the same decision on who can access a resource.
The simplest implementation was to assign a system-wide coordinator who is in charge of determining
the outcome. The two-phase commit protocol is also an example of a system where we assume that
the coordinator and cohorts are alive and communicating &mdash; or we can afford to wait for them to
restart, indefinitely if necessary.
The catch to those algorithms
was that all processes had to be functioning and able to communicate with each other.
Faults make it difficult. Faults include process failures and communication failures.
</p>

<h1> Dealing with failure </h1>
<p>
We cannot provably achieve consensus with completely asynchronous faulty processes.
This means making no assumption on the speeds of the processes or network communication.
The core problem is that there is no way to check whether a process has failed or whether the process is alive but
the communication to the process is intolerably slow. This impossibility is proved by Fischer, Lynch, Patterson (FLP[85]).
Also, in the presence of unreliable communication, consensus is impossible to achieve since we may never be
able to communicate with a process. 
</p>
<p>
We will examine two fault tolerance scenarios that illustrate some basic constraints
that are imposed on us. The two army problem is particularly relevant.
</p>

<h2>Two Army Problem</h3>
<p>
Let's examine the case of good processors but faulty communication lines.
This is known as the <strong>two army problem</strong> and can be summarized as follows:
</p>
<p>
Two divisions of an army, <em>A</em> and <em>B</em>, coordinate an attack on enemy army, <em>C</em>.
<em>A</em> and <em>B</em> are physically separated and use a messenger to communicate.
<em>A</em> sends a messenger to <em>B</em> with a message of "<em>let's attack at dawn</em>".
<em>B</em> receives the message and agrees, sending back the messenger with an
"<em>OK</em>" message. The messenger arrives at <em>A</em>, but <em>A</em> realizes that <em>B</em> did
not know whether the messenger made it back safely.
If <em>B</em> is not convinced that <em>A</em> received the acknowledgement, then it will not be
confident that the attack should take place since the army will not win on its own.
<em>A</em> may choose to send the messenger back to <em>B</em> with a message of
"<em>A received the OK</em>" but <em>A</em> will then be unsure as to
whether <em>B</em> received this message. 
The two army problem demonstrates that even with non-faulty processors,
provable agreement between two processes is <em>not</em>
possible with unreliable communication channels.
</p>

<p>
In the real world, we will need to place upper bounds on communication and computing speeds and consider
a process to be faulty if it does not respond within that bounded time. 
</p>

<p>
<strong>Fail-stop</strong>, also known as <strong>fail-silent</strong>, is the condition when a failed process
does not communicate.
A <strong>byzantine fault</strong> is where the faulty 
process continues to communicate but may produce faulty information.
We can create a consensus algorithm that is resilient to fail-stop.
If there are <em>n</em> processes, of which <em>t</em> may be faulty,
then a process can never expect to receive more than <em>(n-t)</em> acknowledgements.
The consensus problem now is to make sure that the same decision is made by all processes,
even if each process receives up to <em>(n-t)</em> answers from a different set of processes
(perhaps due to partial network segmentation or routing problems).
</p>
<p>
A fail-stop resilient algorithm can be demonstrated as follows. It is an iterative algorithm. Each phase consists of:
</p>
<ol class="separated">
	<li> A process broadcasts its preferred value and the number of processes that it has seen that also have that preferred
	value (this count is called the cardinality and is 1 initially). </li>
	<li> The process receives <em>(n-t)</em> answers, each containing a preferred value and cardinality.</li>
	<li> The process may then change its preferred value according to which value was preferred most by other processes.
	It updates the corresponding cardinality to the number of responses it has received with that value plus itself.</li>
</ol>

<p>
Continue this process until a process receives <em>t</em> messages of a single value with cardinality at least <em>n/2</em>.
This means that at least half of the systems have agreed on the same value.</li>
At this point, run two more phases, broadcasting this value.</li>
</p>
<p>
As the number of phases goes to infinity,
the probability that consensus not reached approaches 0.
</p>

<p>
To make it easier to develop algorithms in the real world, we can relax the definition of <em>asynchronous</em> and allow some synchrony.
Several types of asynchrony may exist in a system:
</p>
<ol>
<li> <strong>Process asynchrony</strong>:
a process may go to sleep or be suspended for an arbitrary amount of time.
</li>

<li> <strong>Communication asynchrony</strong>:
there is no upper bound on the time a message may take to reach its destination.
</li>

<li> <strong>Message order asynchrony</strong>:
messages may be delivered in a different order than sent.
</li>
</ol>
<p>
It has been shown [Dolev, D., Dwork, C., and Stockmeyer] that it is not sufficient to make processes synchronous but that any of the following 
cases is sufficient to make a consensus protocol possible:
</p>
<ol class="separated">

<li> Process and communication synchrony: place an upper bound on
process sleep time and message transmission).
</li>

<li> Process and message order synchrony: place an upper bound on
process sleep time and message order).
</li>

<li> Message order synchrony and broadcast capability. </li>

<li> Communication synchrony, broadcast capability, and send/receive
atomicity.
<em>Send/receive atomicity</em> means that a processor can carry
through the operations of receiving a message, performing computation,
and sending messages to other processes. </li>
</ol>

<h2> Byzantine failures in synchronous systems </h2>

<div class="right-sidetext">
<p>
Solutions to the Byzantine Generals problem are <em>not</em> obvious, intuitive,
or simple. They are not presented in these notes.
You can read Lamport's paper on the
problem
<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/byz.pdf"
target="_blank">
here</a>. You can also check out the brief summary
and various solutions &ndash; which go beyond the Lamport paper &ndash;
<a href="http://pages.cs.wisc.edu/~sschang/OS-Qual/reliability/byzantine.htm"
target="_blank">here</a>.
</p>
</div>
<p>
<a name="byzantine"> </a>
We looked at the case of unreliable communication lines with reliable or fail-stop communication. 
The other case to consider is that of reliable communication lines but faulty (not fail-stop)
processors. Byzantine failures are failed processors that, instead of staying quiet (fail-stop), instead
communicate with erroneous data.
</p>

<h3>Byzantine Generals Problem</h3>
<p>
Consensus with reliable communication lines and byzantine failures is illustrated
by the <strong>Byzantine Generals Problem</strong>.

In this problem, there are <em>n</em> army generals who head different divisions. Communication is reliable (radio or telephone) but
<em>m</em> of the generals are traitors (faulty) and are trying to prevent others
from reaching agreement by feeding them incorrect information. The question is:
can the loyal generals still reach agreement? Specifically, each general knows
the size of his division. At the end of the algorithm can each general know the troop strength of every other loyal division?
</p>
<p>
Lamport demonstrated a solution that works for certain cases. His answer to this
problem is that any solution to the problem of overcoming <em>m</em> traitors
requires a minimum of 3<em>m</em>+1 participants (2<em>m</em>+1 loyal generals).
This means that more than 2/3 of the generals must be loyal. Moreover, it was demonstrated that no protocol can overcome <em>m</em> faults with
fewer than <em>m</em>+1 rounds of message exchanges and O(<em>mn</em><sup>2</sup>) messages.
If <em>n</em> &lt; <em>3m + 1</em> then the problem has no solution.
</p>
<p>
Clearly, this is a rather costly solution. While the Byzantine model may be applicable to certain types of special-purpose hardware, it will rarely be useful in general purpose distributed computing environments.
</p>

<p class="nospace">
There is a variation on the Byzantine Generals Problem that uses signed messages. What this means is that
messages from loyal generals cannot be forged or modified. In this case, there are algorithms that
can achieve consensus for values of <em>n</em> &ge; <em>m + 2</em>, where
</p>
<blockquote>
 n = total number of processors
<br/>
 m = total number of faulty processors
</blockquote>

<h1> Replicated state machines </h1>
<p>
An important motivation for building distributed systems is to achieve high scalability and high availability. 
High availability can be achieved via redundancy: replicated functioning components will take the place of those
that ceased to function. To achieve redundancy with multiple active components, we want all working replicas to 
do the same thing: produce the same outputs given the same inputs.
</p>
<p>
A state machine approach to systems design models each replica (each component of the system) as a deterministic
state machine. For some given input to a specific state of the system, a deterministic output and 
transition to a new state will be produced. We refer to each replica (component) as a <em>process</em>.
For correct execution and high availability, it is important that each process sees the same inputs. To do
this, we rely on a consensus algorithm. This ensures that multiple processes will do the same thing since
they will each be provided with the same set of inputs.
</p>
<p>
An example of an input may be a request from a 
client to read data from a specific location from a file or write data to a specific location of a file.
We want the replicated files to contain the exact same data and yield the same results. To achieve
this, we need agreement among all processes on what the client requests are and the
requests must be totally ordered: each server must see file read/write requests in the exact same
order as everyone else. The total ordering part is most easily achieved by electing one process to 
serve sequence numbers (although there are other more complex but more distributed implementations.
</p>

<h1> Paxos </h1>

<p>
Paxos is a popular fault-tolerant distributed consensus algorithm. It allows a globally consistant (total) order to be assigned to 
client messages (actions). 
</p>

<p>
Much of what is summarized here is from Lamport's <em>Paxos Made Simple</em> but I tried to simplify it
substantially. Please refer to that paper for more detail and definitive explanations.
</p>

<p>
The goal of a distributed consensus algorithm is to allow a set of computers to all agree on a single value 
that one of the nodes in the system proposed (as opposed to making up a random value). The
challenge in doing this in a distributed system is that messages can be lost or machines cn fail. Paxos guarantees
that a set of machines will chose a single proposed value as long as a majority of systems that participate
in the algorithm are available.
<p>
The setting for the algorithm is that of a collection of processes that can propose values.
The algorithm has to ensure that a single one of those proposed values is chosen and all processes should learn that value.
</p><p>
There are three classes of agents:
</p>
<ol>
	<li> Proposers</li>
	<li> Acceptors</li>
	<li> Learners</li>
</ol>
<p>
A machine can take on any or all of these roles.

<strong>Proposers</strong> put forth proposed values.
<strong>Acceptors</strong> drive the algorithm's goal to reach agreement on
a single value and let the <strong>learners</strong> are informed of the outcome. Acceptors either reject a proposal or
agree to it and make promises on what proposals they will accept in the future. This ensures
that only the latest set of propsals will be accepted.
A process can act as more than one agent in an implementation. Indeed, many implementations have collections of processes where each
process takes on all three roles.
</p>
<p>
Agents communicate with each other asynchronously. They may also fail to communicate and may restart. 
Messages can take arbitrarily long to deliver. They can can be duplicated or lost but are not corrupted.
A corrupted message should be detectable as such and can be counted as a lost one (this is what UDP does, for example).
</p><p>
The absolutely simplest implementation contains a single acceptor. 
A proposer sends a proposal value to the acceptor. The acceptor processes one request at a time, chooses the first
proposed value that it receives, and lets everyone (learners) know. Other proposers must agree to that value.
</p><p>
This works as long as the acceptor doesn't fail. Unfortunately, acceptors are subject to failure.
To guard against the failure of an acceptor, we turn to replication and use multiple acceptor processes.
A proposer now sends a <em>proposal</em> containing a value to a set of acceptors.
The value is chosen when a majority of the acceptors <em>accept</em> that proposal (agree to it).
</p><p>
Different proposers, however, could independently initiate proposals at approximately the same
time and those proposals could contain different values. They each will communicate with a different subset of acceptors.
Now different acceptors will each have different values but none will have a majority.
We need to allow an acceptor to be able to accept more than one proposal.
We will keep track of proposals by assigning a unique proposal number to each proposal.
Each proposal will contain a proposal number and a value.
Different proposals must have different proposal numbers.
Our goal is to agree on one of those proposed values from the pool of proposals sent to different subsets of acceptors.
</p><p>
A value is chosen when a single proposal with that value has been accepted by a majority of the acceptors.
That means it has been <em>chosen</em>.
Multiple proposals can be chosen but all of them bust have the same value:
if a proposal with a value <em>v</em> is chosen, then every higher-numbered proposal that is chosen must also have value <em>v</em>.
</p><p class="nospace">
If a proposal with proposal number <em>n</em> and value <em>v</em> is issued,
then there is a set <em>S</em> consisting of a majority of acceptors such that either:
<ol class="alphalist">
<li> no acceptor in S has accepted any proposal numbered less than n, or</li>
<li> v is the value of the highest-numbered proposal among all proposals numbered < n accepted by the acceptors in S.</li>
</ol>
<p>
A proposer that wants to issue a proposal numbered <em>n</em> must learn the highest numbered proposal
with number less than <em>n</em>, if any, that has been or will be accepted by each acceptor in a majority of acceptors.

To do this, the proposer gets a <em>promise</em> from an acceptor that there will be no future acceptance
of proposals numbered less than <em>n</em>.
</p><p>

<h2> The Paxos algorithm </h2>

<p>
The Paxos algorithm operates in two phases:
</p>
<dl>
<dt> Phase 1: Prepare: send a proposal request </dt>
<dd> Proposer:
	<br/>
	A proposer chooses a proposal number <em>n</em> and sends a <em>prepare</em> request to a majority of acceptors.
	The number <em>n</em> is stored in the proposer's stable storage so that
	the proposer can ensure that a higher number is used for the next proposal
	(even if the proposer process restarts).
</dd>
<dd> Acceptor:
	<br/>
	<ul>
	<li> If an acceptor has received a proposal greater than <em>n</em>
	in the past, then it ignores this <em>prepare(n)</em> request. 
	</li>
	<li> The acceptor promises never to accept a proposal numbered less than <em>n</em>.
	</li>
	<li> The acceptor replies to the proposer with a past proposal that it has
	accepted previously that had the highest number less than <em>n</em>: <em>reply(n',v')</em>.
	</li>
	</ul>
</dd>

<dd>
	If a proposer receives the requested responses to its <em>prepare</em> request from
	a majority of the acceptors, then it can issue a proposal with number <em>n</em>
	and value <em>v</em>, where <em>v</em> is the value of
	the highest-numbered proposal among the responses or any value selected by the proposer
	if the responding acceptors reported no proposals.
</dd>

<dt> Phase 2: Accept: send a proposal (and then propagate it to learners after acceptance) </dt>
<dd> Proposer:
	<br/>
	A proposer can now issue its proposal.
	It will send a message to a set of acceptors stating that its proposal should be accepted
	(an <em>accept(n,v)</em> message).

	If the proposer receives a response to its <em>prepare(n)</em> requests
	from a majority of acceptors, it then sends an <em>accept(n, v)</em> request to each of those
	acceptors for a proposal numbered <em>n</em> with a value <em>v</em>, where <em>v</em> is the
	highest-numbered proposal among the responses, or is any value if the responses reported no proposals.
	</dd>

<dd> Acceptor:
	<br/>
	If an acceptor receives an <em>accept(n, v)</em> request for a proposal numbered <em>n</em>,
	it accepts the proposal unless it has already responded to a <em>prepare</em> request
	having a number greater than <em>n</em>.
</dd>
</dl>
<p>
	The acceptor receives two types of requests from proposers: <em>prepare</em> and <em>accept</em> requests.
	Any request can be ignored.
An acceptor only needs to remember the highest-numbered proposal that it has ever
accepted and the number of the highest-numbered <em>prepare</em> request to which it has responded.
The acceptor must store these values in stable storage so they can be preserved in case the acceptor fails and has to restart.
</p>

</dl>

<p>
A proposer can make multiple proposals as long as it follows the algorithm for each one.
</p>

<h2> Consensus </h2>
<p>
Now that the acceptors have a proposed value, we need a 
way to learn that a proposal has been accepted by a majority of acceptors.
The <em>learner</em> is responsible for getting this information.
Each acceptor, upon accepting a proposal, forwards it to all
the learners. The problem with doing this is the potentially large 
number of duplicate messages: <em>(number of acceptors) * (number
of learners)</em>. If desired, this could be optimized. One or more
<em>"distinguished learners"</em> could be elected. Acceptors will communicate
to them and they, in turn, will inform the other learners.
</p><p>

<h2> Ensuring progress </h2>
<p>
One problem with the algorithm is that its possible for two proposers
to keep issuing sequences of proposals with increasing numbers,
none of which get chosen. An <em>accept</em> message from one proposer may
be ignored by an acceptor because a higher numbered <em>prepare</em> message
has been processed from the other proposer. To ensure that the
algorithm will make progress, a <em>"distinguished proposer"</em> is selected
as the only one to try issuing proposals.
</p><p>
In operation, 
clients send commands to the leader, an elected <em>"distinguished
proposer"</em>. This proposer sequences the commands (assigns a value)
and runs the Paxos algorithm to ensure that an agreed-upon sequence number
gets chosen. Since there might be conflicts due to failures or
another server thinking it is the leader, using Paxos ensures that only
one command (proposal) gets assigned that value.
</p>

<h1> Leasing versus Locking </h1>
<p>
Processes often rely on locks to ensure exclusive access to a resource. 
The difficulty with locks is that they are not fault-tolerant. If a process holding a lock dies or forgets
to release the lock, the lock exists unless additional software is in place to detect these actions and
break the lock. For this reason, it is more safer to add an expiration time to a lock. This turns a
<em>lock</em> into a <em>lease</em>. 
</p>
<p>
We saw an example of this approach with the two-phase and three-phase commit protocols. A two-phase commit
protocol uses locking while the three-phase commit uses leasing; if a lease expires, the transaction is
aborted. We also saw this approach with maintaining references to remote objects.
If the lease expires, the server considers
the object unreferenced and suitable for deletion. The client is responsible for 
renewing the lease periodically as long as it needs the object. 
</p>
<p>
The downside with a leasing approach is that the resource is unavailable to others until the lease expires.
Now we have a trade-off: have long leases with a possibly long wait after a failure or have short leases
that need to be renewed frequently.
</p>

<h1> Hierarchical leases versus consensus </h1>
<p>
In a fault tolerant system with replicated components, leases for resources should be granted 
by running a consensus algorithm. Looking at Paxos, it is clear that, while there is not
a huge amount of message passing taking place, there are number of players involved and
hence there is a certain efficiency cost in using the algorithm. A compromise approach
is to use the consensus algorithm as an election algorithm to elect a coordinator. This 
coordinator is granted a lease on a large set of resources or the state of the system. In turn,
the
coordinator is now responsible for handing out leases for all or a subset of the system state.  
When the coordinator's main lease expires, a consensus algorithm has to be run again to grant a 
new lease and possibly elect a new coordinator but it does not have
to be run for every client's lease request; that is simply handled by the coordinator.
</p>


<h1> References </h1>
<dl>
<dt>
Leslie Lamport,
<em><a href="pdos.csail.mit.edu/6.824/papers/paxos-simple.pdf">Paxos Made Simple</a></em>,
November 2001.
</dt>
<dd> One of the clearest papers out there detailing the Paxos algorithm </dd>

<dt>
Lampson, Butler.
<em><a href="http://research.microsoft.com/en-us/um/people/blampson/58-Consensus/Acrobat.pdf">How to Build a Highly Available System Using Consensus</a></em>,
Microsoft Research
</dt>
<dd> An updated version of
<em>Distributed Algorithms</em>, ed. Babaoglu and Marzullo, Lecture Notes in Computer Science 1151, Springer, 1996, pp 1-17.
<br/>
<dd> A great coverage of leases, the Paxos algorithm, and the need for consensus in achieving highly available computing using replicated state machines. </dd>

<dt>
Henry Robinson,
<em>
<a href="http://the-paper-trail.org/blog/?p=173">Consensus Protocols: Paxos</a></em>,
Paper Trail blog, February 2009.

</dt>
<dd>
</dd>

<dt>
Iair Amir, Jonathan Kirsch,
<em><a href="http://www.cnds.jhu.edu/pub/papers/psb_ladis_08.pdf">Paxos for System Builders: An Overview</a></em>, 
Johns Hopkins University.
</dt>
<dd> Written from a system-builder's perspective and covers some of the details of implementation. The paper is a really brief (5 page)
overview. 
</dd>

<dt>
Iair Amir, Jonathan Kirsch,
<em><a href="http://www.cnds.jhu.edu/pub/papers/cnds-2008-2.pdf">Paxos for System Builders</a></em>, 
Johns Hopkins University, Technical Report CNDS-2008-2, March 2008.
</dt>
<dd>
This is the 35-page full version of the above paper.
</dd>

<dt>
Michael J. Fischer, Nancy A. Lynch, Michael S. Paterson,
<em><a href="http://www.google.com/url?sa=t&source=web&cd=5&ved=0CEEQFjAE&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.122.9034%26rep%3Drep1%26type%3Dpdf&rct=j&q=Fischer%2C%20Lynch%2C%20Patterson%20consensus&ei=edmYTuG1IKT40gGzndzeBA&usg=AFQjCNG1-nNfVtFn0pYIJ-UE1xP7DFKwLw&sig2=QkEZ4juS7A_rWwZFd1BHGg&cad=rja">Impossibility of Distributed Consensus with One Faulty Process</a></em>,
Journal of the Association for Computing Machinery, Volume 32, No. 2, April 1984, pp. 374-382.
</dt>
<dd> This is the seminal paper that proves that one cannot achieve consensus with completely asynchronous faulty processes.</dd>
<dt>
Bracha, G. and Toueg, S.
<em>Asynchronous Consensus and Broadcast Protocols,</em> Journal of the ACM 32, 4 (October 1985), 824&ndash;840.
</dt>
<dd>
Describes a fail-stop consensus algorithm.
</dd>
<dt>
Dolev, D., Dwork, C., and Stockmeyer, L.
<em>On the Minimal Synchronism Needed for Distributed Consensus,</em> J. ACM 34, 1 (January 1987), 77&ndash;97.
</dt>
</dl>

</div>
<div id="footer">
<hr/>
<style type="text/css">  
span.codedirection { unicode-bidi:bidi-override; direction: rtl; }  
</style>  

<p> &copy; 2003-2019 Paul Krzyzanowski. All rights reserved.</p>
<p>For questions or comments about this site, contact Paul Krzyzanowski, 
<span class="codedirection">gro.kp@ofnibew</span>
</p>
<p>
The entire contents of this site are protected by copyright under national and international law.
No part of this site may be copied, reproduced, stored in a retrieval system, or transmitted, in any form,
or by any means whether electronic, mechanical or otherwise without the prior written
consent of the copyright holder.
If there is something on this page that you want to use, please let me know.
</p>
<p>
Any opinions expressed on this page do not necessarily reflect the opinions of my employers and may not
even reflect my own.
</p>
<p> Last updated: February 14, 2019
</p>
<img class="stamp" src="../../..//css/images/recycled_pixels_logo.png" alt="recycled pixels" height="80" width="80" />
</div> <!-- footer -->
<div id="tear">
</div>


<div id="sidebar1">
<h1 class="first">Contents </h1>
	<h2> CS 352 </h2>
	<ul>
	<li> <a href="../../index.html"> Main course page </a> </li>
	<li> <a href="../../news.html"> News </a> </li>
	<li> <a href="../../syllabus.html"> Syllabus </a> </li>
	<li> <a href="../../hw/index.html"> Homework </a> </li>
	<li> <a href="../../notes/index.html"> Documents </a> </li>
	<li> <a href="../../exam/index.html"> Exam info </a> </li>
	<li> <a href="../../grades/index.html"> Check your grades </a> </li>
	<li> <a href="https://sakai.rutgers.edu/portal/site/6f683a17-68f7-4cb4-ad35-6c245c3f4682"> Sakai </a> </li>
	<!-- <li> <a href="https://sakai.rutgers.edu/portal"> Sakai </a> </li> -->
	</ul>

	<h2> CS 352 background </h2>
	<ul>
	<li> <a href="../../about.html"> About the course </a> </li>
	<li> <a href="../../prereq.html"> Prerequisites </a> </li>
	<li> <a href="../../things.html"> Things you need </a> </li>
	<li> <a href="../../policy.html"> Policy  </a> </li>
	</ul>
</div>

<div id="sidebar2">
<!--
<h1 class="first"> Free junk </h1>
<p>
This is some stuff I'm throwing away. Please send me mail if you want any of it:
</p>
<hr/>
<ul>
<li> 
</ul>
-->
</div>

</div>
</div>
</body>
</html>
